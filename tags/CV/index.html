<!doctype html>
<html lang="ko"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>태그: CV - KyuBum&#039;s Dev Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="KyuBum Shin"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="KyuBum Shin"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="KyuBum&#039;s Dev Blog"><meta property="og:url" content="https://kyubumshin.github.io/"><meta property="og:site_name" content="KyuBum&#039;s Dev Blog"><meta property="og:locale" content="ko_KR"><meta property="og:image" content="https://kyubumshin.github.io/img/og_image.png"><meta property="article:author" content="KyuBum Shin"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://kyubumshin.github.io"},"headline":"KyuBum's Dev Blog","image":["https://kyubumshin.github.io/img/og_image.png"],"author":{"@type":"Person","name":"KyuBum Shin"},"publisher":{"@type":"Organization","name":"KyuBum's Dev Blog","logo":{"@type":"ImageObject","url":"https://kyubumshin.github.io/img/logo.svg"}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.0.2"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="KyuBum&#039;s Dev Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="검색" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-9-tablet is-9-desktop is-9-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">태그</a></li><li class="is-active"><a href="#" aria-current="page">CV</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-07-16T12:24:11.000Z" title="2022. 7. 16. 오후 9:24:11">2022-07-16</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-07-16T23:07:13.432Z" title="2022. 7. 17. 오전 8:07:13">2022-07-17</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Paper/">Paper</a><span> / </span><a class="link-muted" href="/categories/Paper/CV/">CV</a></span><span class="level-item">7분안에 읽기 (약 993 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/07/16/paper/Ghostnet/">GhostNet: More Features from Cheap Operations</a></h1><div class="content"><hr>
<p>본 글은 CVPR 2020에서 발표한 GhostNet에 대한 코드와 함께 보는 논문 리뷰입니다</p>
<h3 id="간단-요약"><a href="#간단-요약" class="headerlink" title="간단 요약"></a>간단 요약</h3><ul>
<li>Ghostnet에서는 전통적인 CNN에서 나타나는 특징인 중복된 Feature(중요한 Feature)에 중점을 두고 이를 효율적으로 만들어 내는 방법에 대하여 작성된 논문입니다.</li>
<li>기존에 뽑아둔 Feature를 일반적인 Convolution 연산보다 적은 resource를 가지는 Linear Transfer연산을 통하여 조금씩 변형하여 중복된 Feature를 만드는 것에 초점이 잡혀있습니다</li>
</ul>
<h3 id="GhostNet-Idea"><a href="#GhostNet-Idea" class="headerlink" title="GhostNet : Idea"></a>GhostNet : Idea</h3><p>아래의 그림은 Resnet 50의 일부 Feature 맵을 시각화 한 사진인데, 여기서 보면 일부 비슷한 모습을 가지는 Feature Map을 볼 수 있습니다. 이를 본 논문에서는 이러한 겹치는 Feature를 Ghost Feature라고 서술하였습니다.</p>
<center>

<img src="/img/ghost1.PNG" alt="" width="600px"/>

</center>

<p>Ghostnet에서는 이러한 Ghost Feature map을 convolution 연산을 통해 자연스럽게 나오는게 아닌 Linear Transfer를 이용하여 기존의 뽑아낸 Feature를 조금씩 변형하여 만든 Feature맵으로 유도하여 적은 Parameter와 Flops를 가진 모델을 만드는 것을 목표로 합니다.</p>
<h3 id="Ghost-Module"><a href="#Ghost-Module" class="headerlink" title="Ghost Module"></a>Ghost Module</h3><p>기존의 CNN Block Module은 아래의 첫번째 그림과 같이 연산이 됩니다. 이 경우 필요한 Parameter의 수는 다음과 같습니다.</p>
<blockquote>
<p>$c$ : channel수<br>$kernel$ : kernel size</p>
</blockquote>
<p>$$<br>parameters &#x3D; c_{in} \times c_{out} \times kernel<br>$$</p>
<p>두번째 그림에서 보여주는 Ghost Module은 다음과 같은 연산과정을 통하여 이루어집니다</p>
<ul>
<li>Input을 Convolution연산을 통하여 Feature를 추출</li>
<li>추출된 Feature에 Linear transformation을 통하여 기존의 Feature들과 유사한 Ghost Feature를 생성</li>
<li>Feature와 Ghost Feature를 Concat을 통하여 하나로 합침</li>
</ul>
<center>

<img src="/img/ghost2.PNG" alt="" width="600px"/>

</center>

<p>실제 구현에서는 Linear Transfer 연산으로 Mobilenet에서 사용되었던 Depthwise Convolution 연산을 사용하였습니다.<br>input이 80이고 output이 100인 연산을 convolution으로 만 진행할 경우 필요한 Parameters는 아래와 같지만<br>$$ 80 \times 100 \times kernel_{conv}$$<br>이를 Ghost Block으로 진행 할 경우 다음과 같이 Parameter가 줄게 됩니다.</p>
<ul>
<li>기존 Feature와 Ghost Feature의 1:1 인 경우<br>$$<br>80 \times 50 \times kernel_{conv}+ 50 \times kernel_{dwconv}<br>$$</li>
</ul>
<blockquote>
<ul>
<li>torch의 Conv2d Layer는 group을 이용하여 depthwise Convolution 연산을 진행 할 수 있습니다.</li>
<li>group이 output사이즈와 같을경우 depthwise 연산을 진행</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GhostModule</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, inp, oup, kernel_size=<span class="number">1</span>, ratio=<span class="number">2</span>, dw_size=<span class="number">3</span>, stride=<span class="number">1</span>, relu=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(GhostModule, self).__init__()</span><br><span class="line">        self.oup = oup</span><br><span class="line">        init_channels = math.ceil(oup / ratio)</span><br><span class="line">        new_channels = init_channels*(ratio-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.primary_conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(inp, init_channels, kernel_size, stride, kernel_size//<span class="number">2</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(init_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>) <span class="keyword">if</span> relu <span class="keyword">else</span> nn.Sequential(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.cheap_operation = nn.Sequential(</span><br><span class="line">            nn.Conv2d(init_channels, new_channels, dw_size, <span class="number">1</span>, dw_size//<span class="number">2</span>, groups=init_channels, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(new_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>) <span class="keyword">if</span> relu <span class="keyword">else</span> nn.Sequential(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x1 = self.primary_conv(x)</span><br><span class="line">        x2 = self.cheap_operation(x1) <span class="comment"># depthwise convolution 연산</span></span><br><span class="line">        out = torch.cat([x1,x2], dim=<span class="number">1</span>) <span class="comment"># identity 결합</span></span><br><span class="line">        <span class="keyword">return</span> out[:,:self.oup,:,:]</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://github.com/huawei-noah/Efficient-AI-Backbones/blob/master/ghostnet_pytorch/ghostnet.py">코드 출처 : huawei-noah&#x2F;Efficient-AI-Backbones</a></p>
<p>Ghostnet은 CNN 중복된 Feature 특징 통하여 딥러닝 모델에서의 Feature가 서로 상관관계가 있고, 이를 기존 Feature를 통해 생성하면서 좋은 결과를 보여주었습니다. </p>
<h3 id="데모"><a href="#데모" class="headerlink" title="데모"></a>데모</h3><p><a target="_blank" rel="noopener" href="https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/pytorch_vision_ghostnet.ipynb">Colab 데모 페이지</a></p>
<p>GhostNet은 pytorch hub를 통하여 손쉽게 Classification 모델로 사용할 수 있습니다.</p>
<ol>
<li>모델 Load</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">model = torch.hub.load(<span class="string">&#x27;huawei-noah/ghostnet&#x27;</span>, <span class="string">&#x27;ghostnet_1x&#x27;</span>, pretrained=<span class="literal">True</span>)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>Classification</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line">input_image = Image.<span class="built_in">open</span>(filename)</span><br><span class="line">preprocess = transforms.Compose([</span><br><span class="line">    transforms.Resize(<span class="number">256</span>),</span><br><span class="line">    transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]),</span><br><span class="line">])</span><br><span class="line">input_tensor = preprocess(input_image)</span><br><span class="line">input_batch = input_tensor.unsqueeze(<span class="number">0</span>) <span class="comment"># create a mini-batch as expected by the model</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># move the input and model to GPU for speed if available</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    input_batch = input_batch.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">    model.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    output = model(input_batch)</span><br><span class="line"><span class="comment"># Tensor of shape 1000, with confidence scores over Imagenet&#x27;s 1000 classes</span></span><br><span class="line"><span class="built_in">print</span>(output[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># The output has unnormalized scores. To get probabilities, you can run a softmax on it.</span></span><br><span class="line">probabilities = torch.nn.functional.softmax(output[<span class="number">0</span>], dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(probabilities)</span><br></pre></td></tr></table></figure></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-04-19T00:36:05.000Z" title="2022. 4. 19. 오전 9:36:05">2022-04-19</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-04-19T12:52:01.331Z" title="2022. 4. 19. 오후 9:52:01">2022-04-19</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">4분안에 읽기 (약 670 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/04/19/boostcamp/week/week13/OCR-1/">부스트 캠프 ai tech 13~14주 OCR - 1</a></h1><div class="content"><hr>
<h2 id="OCR"><a href="#OCR" class="headerlink" title="OCR"></a>OCR</h2><ul>
<li>Text라는 단일 Class에 대하여 예측을 하는 Task</li>
<li>위치를 검출하는 Text Detection과 내용을 인식하는 Text Recognization으로 나뉠 수 있다</li>
</ul>
<h2 id="일반적인-Object-Detection-Task와의-차이"><a href="#일반적인-Object-Detection-Task와의-차이" class="headerlink" title="일반적인 Object Detection Task와의 차이"></a>일반적인 Object Detection Task와의 차이</h2><p>Text Image Data의 특성상 일반적인 Object Detection과는 아래과 같은 차이를 가진다</p>
<ul>
<li>Object의 높은 밀도<ul>
<li>Data 특성 상 연속적인 Line에 여러가지 word가 존재하고 그 line이 붙어서 반복하기 때문에 일반적인 Object Detection에 비해 매우 높은 밀도를 가진다</li>
</ul>
</li>
<li>극단적인 Ratio<ul>
<li>언어에 따라서 띄어쓰기가 아에 존재하지 않거나 긴 단어들이 존재하기 때문에 Ratio가 극단적이다</li>
</ul>
</li>
<li>특이한 모양<ul>
<li>구겨짐<ul>
<li>글자들이 종이같은 평면에 존재하는 경우가 많고, 휘어지거나 구겨지는 경우도 다수 존재하기 때문에 object의 영역도 구겨지면서 특이한 모양을 가진다</li>
</ul>
</li>
<li>휘어짐<ul>
<li>간판등 디자인적 부분으로 인하여 휘어진 영역을 가지는 경우도 존재한다</li>
</ul>
</li>
<li>세로 쓰기</li>
</ul>
</li>
<li>객체의 특징<ul>
<li>같은 글자라도 큰 크기편차가 존재할 가능성이 높음</li>
<li>객체 영역이 모호함</li>
</ul>
</li>
</ul>
<p><strong>OCR 모델은 위의 특징들을 고려하여 만들어 져야 한다</strong></p>
<h2 id="OCR-Model"><a href="#OCR-Model" class="headerlink" title="OCR Model"></a>OCR Model</h2><h3 id="Base-Model"><a href="#Base-Model" class="headerlink" title="Base Model"></a><strong>Base Model</strong></h3><p>OCR 모델은 크게 2가지 방법으로 글자 영역을 예측한다</p>
<ol>
<li>Regression-based<ul>
<li>이미지를 입력받아서 글자영역 표현값을 출력하는 단순한 방식</li>
<li>사각형으로 출력하기 때문에 불필요한 영역을 포함하고, 정확도가 떨어지는 한계가 존재</li>
<li>Anchor Box의 범위를 벗어나는 영역에 경우 측정 정확도가 떨어진다<ul>
<li>글자 객체가 가지는 극단적인 Ratio를 제대로 반영하지 못하는 경우가 많음</li>
</ul>
</li>
</ul>
</li>
<li>Segmentation-based<ul>
<li>이미지를 입력받아서 화소단위로 정보를 뽑고 후처리를 통하여 글자 영역의 표현값을 확보</li>
<li>Post-processing과 후처리가 필요하기 때문에 Regression-based에 비하여 연산량이 많다</li>
<li>서로 간섭이 있거나 인접한 개체간의 구분이 어렵다</li>
</ul>
</li>
<li>Hybrid 방식<ul>
<li>위의 두가지 방법을 혼합하여 예측하는 방식</li>
<li>Regression을 통하여 대략적인 영역을 탐색하고, Segmentation-based로 화소정보를 추출하여 세부적인 영역을 결정하는 방식을 취한다</li>
<li>MaskTextSpotter</li>
</ul>
</li>
</ol>
<h3 id="Task-Based"><a href="#Task-Based" class="headerlink" title="Task Based"></a>Task Based</h3><ol>
<li>Character-Based<ul>
<li>Charater 단위로 검출</li>
</ul>
</li>
<li>Word-Based<ul>
<li>Word 단위로 검출</li>
</ul>
</li>
</ol>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a target="_blank" rel="noopener" href="https://boostcamp.connect.or.kr/program_ai.html">Naver Connect Boostcamp - ai tech</a></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-04-04T02:30:12.000Z" title="2022. 4. 4. 오전 11:30:12">2022-04-04</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-07-16T12:34:26.639Z" title="2022. 7. 16. 오후 9:34:26">2022-07-16</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Paper/">Paper</a><span> / </span><a class="link-muted" href="/categories/Paper/CV/">CV</a></span><span class="level-item">8분안에 읽기 (약 1177 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/04/04/paper/ViT/">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></h1><div class="content"><hr>
<p>논문 링크 : <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.11929.pdf">https://arxiv.org/pdf/2010.11929.pdf</a></p>
<blockquote>
<p>원래 이번 논문 리뷰는 Swin Transformer에 대해서 다루려고 했는데 ViT의 내용을 알지 못하면 풀어서 이야기 할수 없는 부분이 많아서 이번 논문은 CV분야를 CNN에서 transformer로 판도를 바꿔버린 ViT에 대해서 다룹니다</p>
</blockquote>
<h2 id="간단-요약"><a href="#간단-요약" class="headerlink" title="간단 요약"></a><strong>간단 요약</strong></h2><ul>
<li>Image Data + Multi head Self Attention</li>
<li>Position embedding을 통해 token에 위치 정보를 추가</li>
<li>Cls Token을 이용한 Classification</li>
<li>그 외의 부분은 NLP의 Transformer 구조를 그대로 가져옴</li>
<li>학습을 위해서는 매우 많은 사전 Data가 필요하다<ul>
<li>데이터가 많을 수록 더 robust한 모델이 만들어진다</li>
</ul>
</li>
</ul>
<br/>

<h1 id="들어가기-앞서"><a href="#들어가기-앞서" class="headerlink" title="들어가기 앞서"></a><strong>들어가기 앞서</strong></h1><h2 id="Inductive-bias-귀납-편향"><a href="#Inductive-bias-귀납-편향" class="headerlink" title="Inductive bias (귀납 편향)"></a><strong>Inductive bias (귀납 편향)</strong></h2><ul>
<li><p>Inductive bias은 Model이 접해보지 못한 input 데이터에 좋은 성능을 내기 위해 사전에 설정된 가정을 이야기한다</p>
</li>
<li><p>CNN 계열 : Locality</p>
<ul>
<li>하나의 pixel에 대하여 주변 pixel또한 비슷한 데이터를 가지고 있을것이라고 가정</li>
<li>부분적인 데이터를 모아서 보기 때문에 Global한 영역에 대해서는 처리가 어렵다<ul>
<li>이를 해결하기 위해 Receptive field를 넓히는 등 여러 연구가 이루어지고 있다</li>
</ul>
</li>
</ul>
</li>
<li><p>RNN 계열 : Sequentiality</p>
<ul>
<li>특정 정보는 비슷한 시간대에 모여있을 것이라고 가정하고 설계 된 모델</li>
<li>순차적이지 않고 먼 뒤쪽에 연관된 데이터가 나오는경우 (ex. 대명사) 예측이 제대로 이루어 지지않음</li>
</ul>
</li>
<li><p>이러한 Inductive bias가 강하게 설정 되어 있을 수록 특정 데이터에 대해서 적은 Data로도 좋은 performence를 보여줄 수 있다</p>
<ul>
<li>그 데이터에 특화된 모델이기 때문이다</li>
</ul>
</li>
</ul>
<center>

<img src="/img/ib.png" alt="" width="500px"/>

</center>

<h2 id="Transformer와-Inductive-bias"><a href="#Transformer와-Inductive-bias" class="headerlink" title="Transformer와 Inductive bias"></a><strong>Transformer와 Inductive bias</strong></h2><ul>
<li>Vision Transformer는 추후에 설명할 Positional Embedding과 Self Attention을 활용하여 이미지의 모든 정보(Global)를 활용하여 연산을 한다<ul>
<li>부분적으로 이미지를 취합하여 예측하는 CNN보다 약한 가정이 들어갔다 볼 수 있다</li>
</ul>
</li>
</ul>
<br/>

<h1 id="논문에서-제안한-Point"><a href="#논문에서-제안한-Point" class="headerlink" title="논문에서 제안한 Point"></a><strong>논문에서 제안한 Point</strong></h1><h2 id="1-Patch-분할을-통한-image-Token화"><a href="#1-Patch-분할을-통한-image-Token화" class="headerlink" title="1. Patch 분할을 통한 image Token화"></a><strong>1. Patch 분할을 통한 image Token화</strong></h2><p>ViT에서는 이미지를 Transformer에 넣기위해 다음과 같은 과정으로 Token화 시켜준다</p>
<ol>
<li>이미지를 Patch 사이즈로 분할한다 (2-d Matrix : $P \times P \times 3$)</li>
<li>$P \times P \times 3$의 길이의 1-d vector로 Patch를 변환시켜준다</li>
<li>Fully Connected Layer를 통하여 Linear Projection을 통해서 같은 차원을 가지는 Embedding Vector를 생성한다</li>
</ol>
<center>

<img src="/img/Vit1.png" alt="" width="700px"/>

</center>

<h2 id="2-Transformer-for-Classification"><a href="#2-Transformer-for-Classification" class="headerlink" title="2. Transformer for Classification"></a><strong>2. Transformer for Classification</strong></h2><ul>
<li>ViT는 Input에 Image로 부터 생성된 Token을 입력한다는 점을 제외하면 Transformer의 구조를 그대로 사용하였기 때문에 기존 NLP의 BERT와 매우 흡사한 구조를 가지고 있다</li>
<li>BERT의 Single text Classification과 같이 Transformer를 Classfication Model로 사용하기 위해 Classfication Token을 0번에 입력한다<ul>
<li>학습 가능한 Parameter로 생성을 하여 학습을 통해서 추후에 결정된다</li>
</ul>
</li>
<li>또한 Patch의 위치 정보를 가지고 있는 Position embedding을 각 token에 더해준다<ul>
<li>(Patch 개수 + 1) x (image token size) 의 크기를 가지는 Parameter</li>
<li>0번은 Classfication Token에 더해지고 나머지 번호는 각각의 순서대로 Patch에 더해지게 된다</li>
<li>Classfication Token과 마찬가지로 학습 가능한 Parameter로 생성을 하여 학습을 통해서 추후에 결정된다</li>
<li>2차원 구조의 Position embedding을 사용했었지만, 1차원 구조보다 더 좋지않은 성능을 보여주었기 때문에 사용하지 않는다</li>
</ul>
</li>
</ul>
<center>

<img src="/img/Vit2.png" alt="" width="700px"/>

</center>

<ul>
<li>마지막으로 Classfication Token 위치의 출력층에 FC Layer를 두어서 분류의 대한 예측을 진행한다</li>
</ul>
<center>

<img src="/img/Vit3.png" alt="" width="700px"/>

</center>

<br/>

<h2 id="3-ViT-Transformer-Encoder"><a href="#3-ViT-Transformer-Encoder" class="headerlink" title="3. ViT Transformer Encoder"></a><strong>3. ViT Transformer Encoder</strong></h2><ul>
<li>Multi Head Attention 전에 Layer Normalization을 진행한다</li>
<li>자세한 이유는 아래의 논문을 참고<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1906.01787.pdf">https://arxiv.org/pdf/1906.01787.pdf</a></li>
</ul>
</li>
</ul>
<center>

<img src="/img/Vit4.png" alt="" width="400px"/>

</center>


<h1 id="ViT의-한계"><a href="#ViT의-한계" class="headerlink" title="ViT의 한계"></a><strong>ViT의 한계</strong></h1><ul>
<li><del>Inductive Bias가 작기 때문에 충분한 데이터를 확보하지 못한 경우에는 성능이 떨어지게 된다</del><ul>
<li>Transformer의 고질적인 문제</li>
<li>이부분에 대해서 멘토링 시간에 새로운 논문을 소개해 주셔서 읽고 추후 리뷰를 통해 이야기 하겠습니다</li>
</ul>
</li>
<li>Image의 사이즈가 커질수록 만들어지는 Patch의 개수가 늘어나기 때문에 모델에 필요한 Parameter가 기하급수적으로 증가한다</li>
</ul>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a target="_blank" rel="noopener" href="https://sgfin.github.io/2020/06/22/Induction-Intro/">Induction-Intro</a></li>
<li><a target="_blank" rel="noopener" href="https://robot-vision-develop-story.tistory.com/29">Inductive Bias - BaeMI의 잡다한 개발 스토리</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=bgsYOGhpxDc&ab_channel=%E2%80%8D%EA%B9%80%EC%84%B1%EB%B2%94%5B%EC%86%8C%EC%9E%A5/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%EA%B3%B5%ED%95%99%EC%97%B0%EA%B5%AC%EC%86%8C%5D">[DMQA Open Seminar] Transformer in Computer Vision</a></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-03-20T03:45:11.000Z" title="2022. 3. 20. 오후 12:45:11">2022-03-20</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-04-04T04:09:51.988Z" title="2022. 4. 4. 오후 1:09:51">2022-04-04</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Paper/">Paper</a><span> / </span><a class="link-muted" href="/categories/Paper/CV/">CV</a><span> / </span><a class="link-muted" href="/categories/Paper/CV/OB/">OB</a></span><span class="level-item">14분안에 읽기 (약 2033 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/03/20/paper/FCOS/">FCOS: Fully Convolutional One-Stage Object Detection</a></h1><div class="content"><hr>
<p>논문 링크 : <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.01355.pdf">https://arxiv.org/pdf/1904.01355.pdf</a></p>
<blockquote>
<p>첫 논문 리뷰로 YoloX 등의 고성능 Anchor Free 모델에 응용되는 OB 모델인 FCOS에 대해서 리뷰를 진행하겠습니다.     </p>
</blockquote>
<h2 id="간단-요약"><a href="#간단-요약" class="headerlink" title="간단 요약"></a><strong>간단 요약</strong></h2><ul>
<li><code>Anchor Free</code> 기반의 Object Detection Model</li>
<li>Semantic Segmentation과 비슷하게 Pixel 단위의 예측을 통하여 Object Detection(OB)을 진행</li>
<li>FPN + Multi head Branch를 이용하여 성능 UP</li>
<li>Center-ness을 이용하여 좀 더 정확성을 끌어올림</li>
</ul>
<h2 id="Anchor-Based-Model의-한계"><a href="#Anchor-Based-Model의-한계" class="headerlink" title="Anchor Based Model의 한계"></a><strong>Anchor Based Model의 한계</strong></h2><p>본 논문에서 지금 까지의 OB모델들은 Anchor Box Base로 좋은 성능을 내어왔지만 다음과 같은 단점이 존재한다고 서술한다</p>
<ol>
<li>Anchor Box 또한 Hyper Parameter로써 성능에 매우 큰 영향을 미치기 때문에 조심스러운 튜닝이 필요하다</li>
<li>조심스럽게 튜닝을 마쳐도 고정된 Anchor Box의 크기와 차이가 많이 나면 효과적으로 학습하지 못한다</li>
<li>높은 Recall 성능을 얻기 위해서는 촘촘한 Anchor Box가 필요하다. 수많은 Anchor Box들은 많은 Negetive Sample을 생성하며 Positive와의 균형이 깨어져서  Imbalance를 야기한다</li>
<li>Anchor Box와 Ground Truth(GT)와의 IoU 계산에서 많은 코스트가 필요하다</li>
</ol>
<p>이를 통하여 본 논문에서는 이런 단점을 해결하기 위해 Anchor Free Object Detection Model인 FCOS를 제안하였다</p>
<h1 id="논문에서-제안한-Point"><a href="#논문에서-제안한-Point" class="headerlink" title="논문에서 제안한 Point"></a><strong>논문에서 제안한 Point</strong></h1><h2 id="0-사전-설정"><a href="#0-사전-설정" class="headerlink" title="0. 사전 설정"></a><strong>0. 사전 설정</strong></h2><ul>
<li>먼저 Layer $i$ 의 Feature Map을 $F_{i}$ 라고 하고, input 이미지의 GT를 $B_i &#x3D; (x^{i}_{0}, y^{i}_{0}, x^{i}_{1}, y^{i}_{1}, c^{i})$ 라고 설정한다.  <ul>
<li>$(x^{i}_{0}, y^{i}_{0})$ : left-top</li>
<li>$(x^{i}_{1}, y^{i}_{1})$ : right-bottom</li>
<li>$c^{i}$ : class</li>
</ul>
</li>
<li>Feature Map에서의 위치 좌표 $(x, y)$ 는 실제 이미지에서 다음의 좌표와 대응된다.<ul>
<li>(xs, ys)로만 표현할 경우 오차의 범위가 너무 커지기 때문에 stride의 절반을 더해주어서 보상한다</li>
<li>$s$ : size of stride<br>$$<br>([\frac{s}{2}] + xs, [\frac{s}{2}] + ys)<br>$$</li>
</ul>
</li>
</ul>
<h2 id="1-Fully-Convolutional-One-Stage-Object-Detector"><a href="#1-Fully-Convolutional-One-Stage-Object-Detector" class="headerlink" title="1. Fully Convolutional One-Stage Object Detector"></a><strong>1. Fully Convolutional One-Stage Object Detector</strong></h2><p>이 부분에서는 OB를 Pixel 단위로 예측하는 방식이 어떻게 진행되는지에 대해서 알아본다</p>
<ul>
<li>기존의 Anchor Based Model은 기준점 $x, y$를 Box의 중심으로 가정하고 그 위치로 부터 Anchor Box를 생성하는 방식으로 물체를 Detection 한다. 하지만  FCOS에서는 $x, y$ 좌표의 픽셀말다 해당하는 Class와 GT Box의 Border를 추측한다.</li>
</ul>
<h3 id="FCOS-상세-계산-방법"><a href="#FCOS-상세-계산-방법" class="headerlink" title="FCOS 상세 계산 방법"></a><strong>FCOS 상세 계산 방법</strong></h3><ol>
<li>$x, y$ 좌표의 픽셀의 분류된 class가 GT Box 안에 속하면서 class값과 같을 경우 Positive Sample로 생각한다<ul>
<li>해당되지 않을 경우에는 negative Sample 간주하고, Background(class &#x3D; 0)로 계산된다</li>
</ul>
</li>
<li>$x, y$ 좌표의 class를 분류함과 동시에 4D vector $\mathbb{t}^* &#x3D; (l^*, t^*, r^*, b^*)$ 에 대하여 Regresstion을 진행한다</li>
</ol>
<center>

<img src="/img/FCOS2.PNG" alt="" width="600px"/>

</center>

<ul>
<li>여기서 $(l^*, t^*, r^*, b^*)$ 는 각각 $x, y$ 좌표에서부터 추측한 Bbox의 경계선 까지의 거리를 말하며 다음과 같이 나타낼 수 있다.</li>
</ul>
<p>$$<br>l^* &#x3D; x -x^{i}_{0} ,\quad t^* &#x3D; y - y^{i}_{0}\\<br>r^* &#x3D; x^{i}_{1} - x,\quad b^* &#x3D; y^{i}_{1} - y<br>$$</p>
<ol start="3">
<li>$x, y$에서 예측한 4차원 거리벡터와 GT box로 계산한 거리가 일치하도록 학습이 이루어진다</li>
</ol>
<h3 id="Network-Output"><a href="#Network-Output" class="headerlink" title="Network Output"></a><strong>Network Output</strong></h3><ul>
<li>FCOS 에서는 Output으로 80-D의 Classfication Vector와 4-D의 Bbox vector $(l^*, t^*, r^*, b^*)$, Center-ness를 추정하도록 구성된다</li>
</ul>
<center>

<img src="/img/FCOS1.PNG" alt="" width="600px"/>

</center>

<ul>
<li>실제로는 Network에서 출력되는 값 $(l, t, r, b)$ 들을 exp를 통하여 변환시킨 값이 $(l^*, t^*, r^*, b^*)$가 된다<ul>
<li>너무 큰 값을 출력으로 할 경우 학습에 문제가 생길 가능성이 존재하기 때문에 의도적으로 작은값을 출력하도록 설정하였다</li>
</ul>
</li>
<li>80-D의 Classfication Vector는 multi-class classifier 구별되는 것이 아닌 각 class에 대해서 binary classification으로 예측된다</li>
</ul>
<h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a><strong>Loss</strong></h3><ul>
<li>Classification Loss는 Focal Loss를 사용</li>
<li>Regression Loss는 IoUloss를 사용</li>
<li>그리고 각 Loss는 Positive sample 수만큼 나누어서 Normalization을 해주었다</li>
</ul>
<h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a><strong>Inference</strong></h3><ul>
<li>$p_{x, y}$ 가 threshold 이상일 경우에는 Positive Sample로 생각하여 Bbox를 Bbox vector를 통해서 예측하고 출력한다</li>
</ul>
<h2 id="2-Multi-level-Prediction-with-FPN-for-FCOS"><a href="#2-Multi-level-Prediction-with-FPN-for-FCOS" class="headerlink" title="2. Multi-level Prediction with FPN for FCOS"></a><strong>2. Multi-level Prediction with FPN for FCOS</strong></h2><ul>
<li><p>위의 항목에서 학습을 그대로 진행하면 Anchor Free Model의 고질적인 2가지의 문제점이 발생한다</p>
<ol>
<li>작은 물체들은 stride가 큰 Feature map에서 표현이 되지 않기 때문에 Recall이 낮아진다</li>
<li>GT Box가 겹쳐져 있을때 어느 GT에 맞춰서 Pixel이 학습을 해야하는지 모호함이 발생할 수 있다</li>
</ol>
</li>
<li><p>이러한 문제점을 해결하기 위해서 논문에서는 FPN을 통한 Multi level Prediection을 제안하였다</p>
</li>
</ul>
<h3 id="Stride-Problems"><a href="#Stride-Problems" class="headerlink" title="Stride Problems"></a><strong>Stride Problems</strong></h3><ul>
<li>FCOS에서는 FPN을 이용하여 다양한 stride를 누적을 통하여 표현</li>
<li>아래에 그림에서와 기본적인 FPN을 통해서 P3, P4, P5의 Layer를 생성한다<ul>
<li>P3, P4, P5는 각각 (8, 16, 32)의 누적된 stride를 가지고 있다</li>
</ul>
</li>
<li>P5에서 추가적으로 stride가 2로 설정된 CNN Layer를 2개 생성한다<ul>
<li>생성된 P6, P7는 각각 64, 128의 누적된 stride를 가지고 있다</li>
</ul>
</li>
<li>FPN을 통해서 다양한 stride를 가지고 탐색을 진행한다<ul>
<li>각 Level의 Layer에서 Box Regression을 진행할 때 $x, y$로 부터 예측되는 Bbox vector의 범위의 제한을 두고 제한을 넘어가면 Negative Sample로 취급한다</li>
<li>논문에서는 0, 64, 128, 256, 512, $\infty$ 로 설정하였다<br>ex) P3의 경우 0~64 제한</li>
</ul>
</li>
</ul>
<h3 id="GT-Box-Overlap"><a href="#GT-Box-Overlap" class="headerlink" title="GT Box Overlap"></a><strong>GT Box Overlap</strong></h3><ul>
<li>GT Box가 겹치는 경우 발생하는 모호함에 대해서 FPN구조로 어느정도 해결이 가능하다</li>
<li>Overlap된 구간에 존재하는 Pixel들에 대해서 겹쳐지는 GT Box들간에 크기 차이가 존재할 경우 다른 Level의 Layer에서 예측될 가능성이 높다</li>
<li>그럼에도 불구하고 한 위치에 Layer 이상의 Box들이 할당이 되면은 면적이 가장 작은 GT Box를 사용한다</li>
</ul>
<center>

<img src="/img/FCOS3.PNG" alt="" width="500px"/>

</center>

<h2 id="3-Center-ness"><a href="#3-Center-ness" class="headerlink" title="3. Center-ness"></a><strong>3. Center-ness</strong></h2><ul>
<li>물체의 중앙점에서 먼 Pixel에서 예측된 Box Vector의 스코어가 낮은 경향을 보이는 문제가 존재하였다. 이것을 해결하기 위해 Center-ness를 도입하였다<ul>
<li>Box 외각의 pixel에서의 예측값은 classification을 통한 확률은 높아서 Positive sample로 판단 되었지만, 실제 Box Vector값은 잘 예측하지 못하는 경우가 많이 발생하였다</li>
</ul>
</li>
</ul>
<h3 id="Center-ness"><a href="#Center-ness" class="headerlink" title="Center-ness"></a><strong>Center-ness</strong></h3><ul>
<li>예측한 Box vector로 부터 $x, y$ 좌표가 Box의 Center에 가까울 수록 높은 가중치를 가지게 된다</li>
</ul>
<p>$$<br>centerness &#x3D; \sqrt{\frac{min(l^*, r^*)}{max(l^*, r^*)} \times \frac{min(t^*, b^*)}{max(t^*, b^*)}}<br>$$</p>
<ul>
<li>classification score 출력에 center-ness를 곱해주면 마지막에 NMS를 진행할 때 낮은 score를 가지게 되기 때문에 걸러지게 만들 수 있다</li>
</ul>
<center>

<img src="/img/FCOS4.PNG" alt="" width="300px"/>

</center>

<ul>
<li>논문에서는 center-ness를 도입해서 classfication score는 높지만 IoU score는 낮은 Sample들을 걸러내는 효과를 얻었다고 한다</li>
</ul>
<center>

<img src="/img/FCOS5.PNG" alt="" width="700px"/>

</center>

<h2 id="결론"><a href="#결론" class="headerlink" title="결론"></a><strong>결론</strong></h2><ul>
<li>Anchor Free 모델로 Anchor와 관련된 HyperParameter를 제외했다</li>
<li>다른 One stage Anchor Based Model과 비교해서 tuning이 없이도 비슷한 성능을 보여주었다</li>
<li>Pixel Prediction(Semantic Segmentation) + Multi Label FPN + Center ness</li>
<li>전체 구조가 간단하면서도 좋은 성능을 보여주어서 응용성이 뛰어나다</li>
<li>Two Stage Detecto의 RPN으로도 응용이 가능하다</li>
</ul>
<h2 id="후기"><a href="#후기" class="headerlink" title="후기"></a><strong>후기</strong></h2><p>예전에 YoloX를 사용해 보면서 가볍게 보고 넘어갔던 논문이었는데 개념적으로만 일고 넘어가서 이번에 완전히 이해하는것을 목표로 리뷰를 해 보았다.<br>그동안 개념적으로 이런 논문이지를 알고 왜 그런것인지에 대해서 초점을 맞춰서 하나하나 읽고, 찾아가면서 공부를 했는데 퍼즐맞추는것처럼 나름 재미있었다!  그리고… 사실 다음주 P-Stage 대비용으로 하나두개 씩 읽어두는게 좋을거같아서 한것도 있다…ㅎ<br>마지막으로 영어공부를 하면서 해야겠다. 파파고가 너무 그동안 편했던것 같다</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-03-17T23:54:37.000Z" title="2022. 3. 18. 오전 8:54:37">2022-03-18</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-03-18T14:58:49.428Z" title="2022. 3. 18. 오후 11:58:49">2022-03-18</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">2분안에 읽기 (약 304 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/03/18/boostcamp/week/week9/CNN-8/">부스트 캠프 ai tech 9주 4일차 3D perspective</a></h1><div class="content"><hr>
<h2 id="3D"><a href="#3D" class="headerlink" title="3D"></a>3D</h2><ul>
<li>사람은 Projection된 2D 이미지로 부터 3D를 인식한다</li>
</ul>
<h2 id="3D의-표현방법"><a href="#3D의-표현방법" class="headerlink" title="3D의 표현방법"></a>3D의 표현방법</h2><ul>
<li>Multiview images<ul>
<li>여러 방향에서 찍은 사진데이터로 표현</li>
</ul>
</li>
<li>Volumetric (voxel)<ul>
<li>x, y, z의 3차원 pixel로 3D를 표현</li>
</ul>
</li>
<li>Part assembly<ul>
<li>단순한 여러개의 Polygon 덩어리로 3D를 표현</li>
</ul>
</li>
<li>Point cloud<ul>
<li>물체의 surface을 dot의 좌표로 표현</li>
</ul>
</li>
<li>Mesh<ul>
<li>point와 edge로 이루어진 map</li>
<li>3각형으로 이루어진 Polygon Data</li>
</ul>
</li>
<li>implicit shape<ul>
<li>고차원의 함수형태로 surface를 표현</li>
</ul>
</li>
</ul>
<center>

<img src="/img/3dvar.PNG" alt="" width="600px"/>

</center>

<h2 id="3D-Dataset"><a href="#3D-Dataset" class="headerlink" title="3D Dataset"></a>3D Dataset</h2><h3 id="ShapeNet"><a href="#ShapeNet" class="headerlink" title="ShapeNet"></a>ShapeNet</h3><ul>
<li>51300개, 55개의 Category를 가진 3D Dataset</li>
<li>전부 디자이너들이 제작함</li>
</ul>
<h3 id="PartNet"><a href="#PartNet" class="headerlink" title="PartNet"></a>PartNet</h3><ul>
<li>26671개의 3D 데이터가 573585개의 Part로 분리되어있는 3D Dataset</li>
</ul>
<h3 id="SceneNet"><a href="#SceneNet" class="headerlink" title="SceneNet"></a>SceneNet</h3><ul>
<li>5 Million개의 RGB-Depth Pair Dataset</li>
<li>Simulationed indoor image(생성 이미지)</li>
</ul>
<h3 id="ScanNet"><a href="#ScanNet" class="headerlink" title="ScanNet"></a>ScanNet</h3><ul>
<li>2.5 Million</li>
<li>실제 Indoor Scan Image</li>
</ul>
<h3 id="Outdoor-3D-Scene-Dataset"><a href="#Outdoor-3D-Scene-Dataset" class="headerlink" title="Outdoor 3D Scene Dataset"></a>Outdoor 3D Scene Dataset</h3><ul>
<li>KITTI<ul>
<li>LiDAR Data, 3D Bboxes</li>
</ul>
</li>
<li>Semantic KITTI<ul>
<li>LiDAR Data, point</li>
</ul>
</li>
<li>Waymo open Dataset<ul>
<li>LiDAR Data, 3D Bboxes</li>
</ul>
</li>
</ul>
<h2 id="3D-Task"><a href="#3D-Task" class="headerlink" title="3D Task"></a>3D Task</h2><ul>
<li>3D object recognition</li>
<li>3D object detection</li>
<li>3D semantic segmentation</li>
</ul>
<h3 id="Conditional-3D-generation"><a href="#Conditional-3D-generation" class="headerlink" title="Conditional 3D generation"></a>Conditional 3D generation</h3><ul>
<li>2D Image에서 3D Mesh를 구하는 Task</li>
<li>Mesh RCNN<ul>
<li>기존 Mask RCNN 에서 Mesh Branch를 추가한 형태</li>
</ul>
</li>
</ul>
<center>

<img src="/img/meshrcnn.PNG" alt="" width="400px"/>

</center>

<ul>
<li>Learning to Reconstruct Shapes from Unseen Classes<ul>
<li>CNN구조로 부터 Feature 추출</li>
<li>3개의 Branch로 Feature 재생성<ul>
<li>normal map</li>
<li>depth</li>
<li>silhuette</li>
</ul>
</li>
<li>재구성을 통한 3D shape 출력</li>
</ul>
</li>
</ul>
<center>

<img src="/img/recon.PNG" alt="" width="600px"/>

</center>


<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a target="_blank" rel="noopener" href="https://boostcamp.connect.or.kr/program_ai.html">Naver Connect Boostcamp - ai tech</a></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-03-16T01:11:47.000Z" title="2022. 3. 16. 오전 10:11:47">2022-03-16</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-03-18T12:09:54.684Z" title="2022. 3. 18. 오후 9:09:54">2022-03-18</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">6분안에 읽기 (약 909 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/03/16/boostcamp/week/week9/CNN-7/">부스트 캠프 ai tech 9주 3일차 Multi-modal Learning</a></h1><div class="content"><hr>
<h2 id="Multi-modal-Overview"><a href="#Multi-modal-Overview" class="headerlink" title="Multi-modal Overview"></a>Multi-modal Overview</h2><ul>
<li>Unimodal : 하나의 특징 Data로 학습시키는 기법</li>
<li>Multi-modal : 여러가지의 Data로 학습시키는 기법</li>
</ul>
<h3 id="Difficultiy-of-Multi-modal"><a href="#Difficultiy-of-Multi-modal" class="headerlink" title="Difficultiy of Multi-modal"></a>Difficultiy of Multi-modal</h3><ol>
<li>데이터마다 표현방법이 모두 다르다<ul>
<li>Audio : wave</li>
<li>Image : pixel</li>
<li>text : sequence? query?</li>
</ul>
</li>
<li>데이터간의 Unbalance한 Feature map<ul>
<li>데이터 자체의 종류가 다르기 때문에 생성하는 분포의 차이가 클 가능성이 높다</li>
<li>학습시키기 용이하지 않음</li>
</ul>
</li>
<li>항상 Multi-modal이 좋은방법은 아니다<ul>
<li>전체 모델에서 한 데이터에 집중하여 학습하면서 다른 데이터를 소홀히 하는 편향적인 학습이 이루어 질 가능성이 존재한다</li>
</ul>
</li>
</ol>
<h3 id="Multimodal-Learning"><a href="#Multimodal-Learning" class="headerlink" title="Multimodal Learning"></a>Multimodal Learning</h3><ul>
<li>Matching : 두 데이터에 대해서 같은 공간으로 매칭시키도록 학습  </li>
<li>Translating : 한 데이터를 다른 데이터로 변형 시키도록 학습  </li>
<li>Referencing : 참조를 통한 상호보안적으로 더 좋은 결과를 내게 학습</li>
</ul>
<center>

<img src="/img/multimodal1.PNG" alt="" width="600px"/>

</center>

<h1 id="Image-amp-Text"><a href="#Image-amp-Text" class="headerlink" title="Image &amp; Text"></a>Image &amp; Text</h1><h2 id="NLP-Preview"><a href="#NLP-Preview" class="headerlink" title="NLP Preview"></a>NLP Preview</h2><h3 id="Text-Embedding"><a href="#Text-Embedding" class="headerlink" title="Text Embedding"></a>Text Embedding</h3><ul>
<li>Text Map to vector</li>
<li>Text를 1차원 Vector로 변환해서 공간상에 표현하는 방법</li>
</ul>
<center>

<img src="/img/textembedding.PNG" alt="" width="500px"/>

</center>

<h3 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h3><p>* </p>
<h2 id="Joint-embedding"><a href="#Joint-embedding" class="headerlink" title="Joint embedding"></a>Joint embedding</h2><ul>
<li>Matching 기법을 사용해서 만들어진 Multi-modal</li>
<li>Pretrained unimodal Model을 합쳐서 사용하는 기법</li>
</ul>
<p>두 가지의 Data로 부터 같은 차원의 Feature를 추출하고, 이것을 Joint Embedding 시켜서 Matching 이 이루어지는 같은 데이터는 높은 Metric, Matching 되지않는 데이터는 낮은 Metric을 부여하도록 학습시키는 방법이다.  </p>
<ul>
<li>같은 차원의 Feature로 추출하는 이유<ul>
<li>하나의 Feature Dimension상에서 Metric을 계산해야한다</li>
</ul>
</li>
</ul>
<center>

<img src="/img/jointembedding.PNG" alt="" width="600px"/>

</center>

<center>

<img src="/img/jointembedding2.PNG" alt="" width="400px"/>

</center>

<h3 id="Image-Tagging"><a href="#Image-Tagging" class="headerlink" title="Image Tagging"></a>Image Tagging</h3><ul>
<li>주어진 이미지에 대해서 Tag를 붙여주거나, 여러가지 Tag에 맞는 이미지를 검색해 주는 모델</li>
<li>Distance를 Metric으로 사용한 모델</li>
<li>Tag를 추가하거나 빼면 기존의 이미지와 유사하면서 바뀐 Tag만 적용될만한 이미지를 우선적으로 뽑아주는 결과를 얻었다</li>
</ul>
<center>

<img src="/img/imagetag.PNG" alt="" width="500px"/>

</center>

<h3 id="Recipe-text-vs-food-image"><a href="#Recipe-text-vs-food-image" class="headerlink" title="Recipe text vs food image"></a>Recipe text vs food image</h3><ul>
<li>주어진 요리 사진에 레시피를 출력해주거나 레시피를 입력했을때 매칭된 이미지를 뽑아주는 모델</li>
<li>cosine similarity와 semantic regularization loss를 이용하였다<ul>
<li>cosine similarity : text data와 image로 뽑은 Feature가 얼마나 유사한지를 계산</li>
<li>semantic regularization loss : 공통된 요리 카테고리에 속해있는지를 계산해서 반영한다 (성능을 높이기 위해 보정)</li>
</ul>
</li>
</ul>
<center>

<img src="/img/recipe2image.PNG" alt="" width="600px"/>

</center>

<h2 id="Cross-modal-translation"><a href="#Cross-modal-translation" class="headerlink" title="Cross modal translation"></a>Cross modal translation</h2><ul>
<li>Translating 기법을 사용한 모델</li>
<li>모델에서 뽑은 Feature를 input으로 다른 모델에 넣고 변환시킨다.</li>
<li>Data를 Feature로 바꿔주는 첫 모델을 Encoder, Feature로 Output를 뽑아내는 모델을 Decoder라고 지칭한다</li>
</ul>
<center>

<img src="/img/translatingmodal.PNG" alt="" width="600px"/>

</center>

<h3 id="Show-attend-and-tell"><a href="#Show-attend-and-tell" class="headerlink" title="Show attend and tell"></a>Show attend and tell</h3><ul>
<li>CNN을 통해서 Feature를 추출</li>
<li>추출한 Feature를 바탕으로 RNN에 입력</li>
<li>RNN 에서는 단어를 예측하면서 다음에 참조할 Feature map을 선택한다</li>
<li>위의 과정이 반복되면서 문장을 생성한다</li>
</ul>
<center>

<img src="/img/sat.PNG" alt="" width="600px"/>

</center>

<h3 id="Text2Image"><a href="#Text2Image" class="headerlink" title="Text2Image"></a>Text2Image</h3><ul>
<li>Text에서 image를 생성하는 모델</li>
<li>Conditional GAN을 통해서 구현 하였다</li>
<li>Encoder를 통해서 Text로 부터 Feature를 얻는다</li>
<li>Feature를 Conditional Input으로 Conditional GAN을 학습시킨다</li>
</ul>
<center>

<img src="/img/t2i.PNG" alt="" width="600px"/>

</center>


<h2 id="Cross-modal-reasoning"><a href="#Cross-modal-reasoning" class="headerlink" title="Cross modal reasoning"></a>Cross modal reasoning</h2><ul>
<li>Referencing 기법을 통해 학습시킨 모델</li>
<li>두 모델에서 나온 Feature로 Joint embedding을 진행하고 추가적으로 Layer를 배치하여 하나의 Task를 푸는 형태로 디자인 되어있다</li>
</ul>
<center>

<img src="/img/cmr.PNG" alt="" width="600px"/>

</center>

<ul>
<li>위의 Show attend and tell은 Translating와 Referencing이 둘다 사용된 모델이다<ul>
<li>실제로 학습할 때는 word token을 같이 받음</li>
</ul>
</li>
</ul>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a target="_blank" rel="noopener" href="https://boostcamp.connect.or.kr/program_ai.html">Naver Connect Boostcamp - ai tech</a></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-03-14T01:15:18.000Z" title="2022. 3. 14. 오전 10:15:18">2022-03-14</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-03-18T15:28:24.750Z" title="2022. 3. 19. 오전 12:28:24">2022-03-19</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">5분안에 읽기 (약 789 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/03/14/boostcamp/week/week9/CNN-6/">부스트 캠프 ai tech 9주 2일차 Conditional Generative Model</a></h1><div class="content"><hr>
<h2 id="Conditional-Generative-Model"><a href="#Conditional-Generative-Model" class="headerlink" title="Conditional Generative Model"></a>Conditional Generative Model</h2><ul>
<li><code>사용자가 컨트롤이 가능한</code> Generative Model</li>
<li>서로 다른 두 도메인을 변화시켜주는 Task</li>
<li>통역모델, 음성의 고품질 전환, 요약모델 등 다양한 분야에서 응용가능하다</li>
</ul>
<h2 id="Conditional-GAN"><a href="#Conditional-GAN" class="headerlink" title="Conditional GAN"></a>Conditional GAN</h2><p>랜덤으로 생성되는 Latent Noise z 만 받는 GAN과는 다르게 latent Noise z + Conditional Input이나, domain Data의 형식으로 받는다</p>
<center>

<img src="/img/cgan.PNG" alt="" width="600px"/>

</center>

<ul>
<li>CV 응용분야<ul>
<li>Style Transfer</li>
<li>Super resolution</li>
<li>Colorization</li>
</ul>
</li>
</ul>
<h3 id="Super-Resolution"><a href="#Super-Resolution" class="headerlink" title="Super Resolution"></a>Super Resolution</h3><ul>
<li>해상도가 낮은 이미지를 높은 이미지로 변환시키는 Task</li>
<li>Super Resolution을 위한 기존의 Naive Regression model에서는 MAELoss나 MSELoss를 사용<ul>
<li>MAE와 MSELoss는 이미지를 전체 이미지의 평균값으로 생성하는 경향이 존재해서 이미지가 뿌옇게 생성됨</li>
</ul>
</li>
<li>Super Resolution GANLoss를 이용하여 좀더 선명한 품실의 image를 얻어냈다<ul>
<li>GAN Loss는 전체 이미지의 분포로 접근해서 생성하는 경향이 있었기에 MAE, MSE에 비해서 덜 뿌연 이미지를 생성</li>
</ul>
</li>
</ul>
<center>

<img src="/img/SR.PNG" alt="" width="600px"/>

</center>

<center>

<img src="/img/SR2.PNG" alt="" width="500px"/>

</center>

<h2 id="Image-Translation-Model"><a href="#Image-Translation-Model" class="headerlink" title="Image Translation Model"></a>Image Translation Model</h2><h3 id="Pix2Pix"><a href="#Pix2Pix" class="headerlink" title="Pix2Pix"></a>Pix2Pix</h3><ul>
<li><p>Image Translations Task를 위한 GAN model</p>
</li>
<li><p>Generator는 Segmentation Masking Data를 사용하여 이미지를 생성한다</p>
</li>
<li><p>Discriminator는 Segmentation Masking Data + image를 가지고 진짜인지 가짜인지 판별한다</p>
</li>
<li><p>Total Loss</p>
<ul>
<li>GAN Loss + L1 Loss<ul>
<li>논문에서는 GAN Loss로 Cross Entropy를 사용</li>
</ul>
</li>
<li>L1 Loss : 형태는 Ground Truth와 비슷하지만 Blurry한 이미지가 생성</li>
<li>GAN Loss : Sharp한 이미지가 형성되지만 형태가 불안전한 이미지가 생성</li>
</ul>
</li>
</ul>
<p>$$<br>G^{*} &#x3D; arg, \underset{G}{min}, \underset{D}{max}, \mathcal{L}_{cGAN}(G,D) + \lambda \mathcal{L}_{L1}(G)<br>$$</p>
<center>

<img src="/img/Pix2pix.PNG" alt="" width="600px"/>

</center>

<h3 id="CycleGAN"><a href="#CycleGAN" class="headerlink" title="CycleGAN"></a>CycleGAN</h3><ul>
<li>Pix2Pix와 같은 Image Translation Model 이지만 Unpair Data를 변환시켜줄 수 있다<ul>
<li>Pix2Pix의 Pair Image라는 제약상황에서 벗어날 수 있다</li>
</ul>
</li>
</ul>
<center>

<img src="/img/cycle1.PNG" alt="" width="400px"/>

</center>

<ul>
<li>Generator<ul>
<li>Input으로 변환시킬 Image를 받는다</li>
<li>Unet의 Decoder처럼 단계적으로 Size를 확장시키면서 생성하는구조의 Generator를 가진다</li>
</ul>
</li>
<li>Discriminator<ul>
<li>Input으로 Image를 받고 Real, Fake를 판단</li>
<li>PatchGAN의 형태를 가짐</li>
</ul>
</li>
<li>CycleGAN에서는 2개의 Discriminator와 2개의 Generator가 존재해서 서로 Cycle을 이룬다</li>
</ul>
<center>

<img src="/img/cycle2.PNG" alt="" width="400px"/>

</center>

<ul>
<li>Total Loss<ul>
<li>GAN Loss + Cycle Consistency Loss</li>
<li>GAN Loss : adversarial losses를 적용</li>
<li>Cycle Consistency Loss : mode collapse 문제를 막기위해 도입한 함수<ul>
<li>변환된 이미지를 재변환(reconstruct)시켰을 때 Real Image사이의 L1 Distance Loss</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>$$<br>\mathcal{L} &#x3D; \mathcal{L}_{GAN}(X \rightarrow Y) + \mathcal{L}_{GAN}(Y \rightarrow X) + \mathcal{L}_{cycle}(G, F)<br>$$</p>
<h2 id="Perceptiaul-Loss"><a href="#Perceptiaul-Loss" class="headerlink" title="Perceptiaul Loss"></a>Perceptiaul Loss</h2><ul>
<li><p>GAN은 학습시키기 힘들다</p>
</li>
<li><p>High quality output을 위한 Loss</p>
</li>
<li><p>Adversarial Loss</p>
<ul>
<li>학습과 구현의 난이도가 높다</li>
<li>Data 만 존재하면 Pretrained 모델이 없어도 좋은 성능을 낼 수 있다</li>
</ul>
</li>
<li><p>Perceptiaul Loss</p>
<ul>
<li>학습 및 구현의 용이성</li>
<li>Pretrained 모델을 통해서 만 구현이 가능</li>
</ul>
</li>
</ul>
<h3 id="Perceptiaul-Loss-구조"><a href="#Perceptiaul-Loss-구조" class="headerlink" title="Perceptiaul Loss 구조"></a>Perceptiaul Loss 구조</h3><p>Perceptiaul Loss는 다음과 같이 이미지를 변환시켜주는 Image Transform Network과 pretrained Model로 이루어진 Loss Network 2개로 구현 할 수 있다.<br>우선 Image Transform Network는 바꿀 이미지를 넣어서 새로운 이미지로 생성하는 역할을 한다. Loss Network는 생성된 이미지 $\hat{y}$, 이미지를 바꾸고싶은 Style Target $y_{s}$와 </p>
<center>

<img src="/img/PL1.PNG" alt="" width="600px"/>

</center>



<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a target="_blank" rel="noopener" href="https://boostcamp.connect.or.kr/program_ai.html">Naver Connect Boostcamp - ai tech</a></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-03-14T00:27:09.000Z" title="2022. 3. 14. 오전 9:27:09">2022-03-14</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-03-15T01:36:34.220Z" title="2022. 3. 15. 오전 10:36:34">2022-03-15</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">6분안에 읽기 (약 875 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/03/14/boostcamp/week/week9/CNN-5/">부스트 캠프 ai tech 9주 1일차 Instance &amp; Panoptic Segmantation</a></h1><div class="content"><hr>
<h1 id="Instance-Segmantation"><a href="#Instance-Segmantation" class="headerlink" title="Instance Segmantation"></a>Instance Segmantation</h1><ul>
<li>Pixel 단위의 Classification 뿐만 아니라 객체간의 구분도 판단하는 Task</li>
</ul>
<h2 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h2><ul>
<li>Faster R-CNN를 기반으로 Instance Segmentation Task를 해결하기 위해 디자인된 모델</li>
<li>Keyword<ul>
<li>RoI-Align</li>
<li>Mask Branch(head)</li>
</ul>
</li>
</ul>
<h3 id="RoI-Align"><a href="#RoI-Align" class="headerlink" title="RoI Align"></a>RoI Align</h3><p>기존 RoI Pooling은 반올림을 해서 Pooling을 연산해서 조금 부정확 하더라도 BBox를 찾는것이기 때문에 괜찮았지만, Segmentation Task에서는 Mask가 이질적으로 변할 가능성이 있기 때문에 소수점 자리까지 연산이 가능하도록 한 RoI 기법</p>
<h3 id="Mask-Branch"><a href="#Mask-Branch" class="headerlink" title="Mask Branch"></a>Mask Branch</h3><p>Faster RCNN의 Neck에서 BBox 별로 추출한 Feature에 따로 Mask Branch를 두어 Binary Classfication을 수행한다</p>
<h2 id="YOLOACT"><a href="#YOLOACT" class="headerlink" title="YOLOACT"></a>YOLOACT</h2><h2 id="YoloActEDGE"><a href="#YoloActEDGE" class="headerlink" title="YoloActEDGE"></a>YoloActEDGE</h2><h1 id="Panoptic-Segmentation"><a href="#Panoptic-Segmentation" class="headerlink" title="Panoptic Segmentation"></a>Panoptic Segmentation</h1><p>Instance + Semantic Segmentation Task</p>
<h2 id="UPSNet"><a href="#UPSNet" class="headerlink" title="UPSNet"></a>UPSNet</h2><p>Backbone Network를 통해서 뽑은 Feature를 이용하여 Semantic, Instance Feature로 가공한 뒤 Panoptic head로 합치는 과정을 거치는 모델</p>
<ul>
<li>Keypoint<ul>
<li>3개의 head<ul>
<li>Semantic head</li>
<li>Instance head</li>
<li>Panoptic head</li>
</ul>
</li>
</ul>
</li>
</ul>
<center>

<img src="/img/UPSNet.PNG" alt="" width="600px"/>

</center>

<h3 id="Heads-Design"><a href="#Heads-Design" class="headerlink" title="Heads Design"></a>Heads Design</h3><ul>
<li>Instance Head</li>
<li>Semantic Head</li>
</ul>
<ol>
<li>Semantic head와 Instance head에서 Feature를 추출<ul>
<li>Instance head에서는 각 Object에 대해서 mask Feature를 얻음</li>
<li>Semantic Head에서는 Instance의 클래스가 겹치는 $X_{thing}$ 과 배경 $X_{stuff}$ Feature를 받아옴</li>
</ul>
</li>
<li>Instance Feature를 적절히 Resize하여 가공하고 Semantic Head의 $X_{thing}$ 과의 합연산을 통하여 각 물체에 대한 Instace Mask를 얻는다</li>
<li>Segmentation Feature에서 Max값만 추출한 Feature Map에서 Instance Feature와 합연산 처리가 된 부분을 제거한다<ul>
<li>이는 Unknown Class에 해당하는 새로운 채널로 추가된다</li>
<li>Semantic Feature와 Instance Feature간의 충돌을 해결해 주는 역할을 한다</li>
</ul>
</li>
<li>2와 3에서 만들어진 Feature들과 배경을 나타내는 $X_{stuff}$를 Concat시켜서 최종적인 Panoptic Segmentation mask를 예측</li>
</ol>
<center>

<img src="/img/USPhead.PNG" alt="" width="600px"/>

</center>

<h2 id="VPSNet"><a href="#VPSNet" class="headerlink" title="VPSNet"></a>VPSNet</h2><ul>
<li>UPSNet을 Video에서도 동작하도록 디자인된 모델</li>
<li>기존 UPSNet과 동일한 구조를 가지고 추가적으로 Track Head가 추가되어 동일 객체에 대해서 Instance Segmentation이 잘 진행되도록 한다</li>
</ul>
<h1 id="Landmark-Localization"><a href="#Landmark-Localization" class="headerlink" title="Landmark Localization"></a>Landmark Localization</h1><p>얼굴이나 사람의 포즈를 추정하고 Tracking 하는데 사용되는 기술이다</p>
<center>

<img src="/img/Landmark.PNG" alt="" width="600px"/>

</center>

<ul>
<li><p>Coordinate regression</p>
<ul>
<li>Landmark 별 Regression을 진행하는 방법</li>
<li>기존에 많이 사용하던 방식이지만 부정확 했다</li>
</ul>
</li>
<li><p>Heatmap Classification</p>
<ul>
<li>Segmentation과 비슷하게 모든 픽셀에 대해서 Landmark 인지를 연산하는 방법</li>
<li>Coordinate regression보다 좋은 성능을 보여주었지만, 모든 픽셀에 대하여 연산을 진행하다보니 더 많은 연산이 필요하다</li>
</ul>
</li>
</ul>
<h2 id="Hourglass-Network"><a href="#Hourglass-Network" class="headerlink" title="Hourglass Network"></a>Hourglass Network</h2><ul>
<li>Stacked hourglass modules</li>
<li>Skip Connection</li>
</ul>
<center>

<img src="/img/hourglass.PNG" alt="" width="600px"/>

</center>

<h3 id="Hourglass-Module"><a href="#Hourglass-Module" class="headerlink" title="Hourglass Module"></a>Hourglass Module</h3><ul>
<li>Unet과 비슷한 구조<ul>
<li>서로 대칭되는 Layer가 존재한다</li>
</ul>
</li>
<li>Feature를 대칭되는 Layer로 넘겨줄 때 CNN을 통해서 걸러지고 + 연산이 이루어진다</li>
</ul>
<center>

<img src="/img/hourglassmodule.PNG" alt="" width="600px"/>

</center>

<h2 id="DensePose"><a href="#DensePose" class="headerlink" title="DensePose"></a>DensePose</h2><p>이미지를 3D Surface(UV map)로 표현해주는 모델</p>
<ul>
<li>Fast R-CNN + 3D surface regression branch</li>
</ul>
<h2 id="RetinaFace"><a href="#RetinaFace" class="headerlink" title="RetinaFace"></a>RetinaFace</h2><ul>
<li>얼굴에 대해서 여러가지 task를 동시에 처리하는 모델<ul>
<li>Gender Classification</li>
<li>Face Detection</li>
<li>5 Landmark Regression</li>
<li>3D Mashup</li>
</ul>
</li>
<li>FPN + Multi-task branches</li>
<li>Backbone + target Branch<ul>
<li>우리가 원하는 모델 디자인 가능</li>
</ul>
</li>
</ul>
<h1 id="Detecting-objects-as-keypoints"><a href="#Detecting-objects-as-keypoints" class="headerlink" title="Detecting objects as keypoints"></a>Detecting objects as keypoints</h1><h2 id="CornerNet"><a href="#CornerNet" class="headerlink" title="CornerNet"></a>CornerNet</h2><p>Top-left, Bottom-right 2 point만 예측하면 BBox를 만들수 있다는 것에서 시작한 모델</p>
<ul>
<li>속도는 매우 빠른편</li>
<li>성능은 좋지 않다</li>
</ul>
<h2 id="CenterNet"><a href="#CenterNet" class="headerlink" title="CenterNet"></a>CenterNet</h2><p>CornerNet에서 Center까지 같이 예측하는 모델<br>추가적으로 Center를 예측하면서 성능의 상승이 이루어졌다</p>
<ul>
<li>CenterNet(1)<ul>
<li>Top-left, Bottom-right, Center</li>
</ul>
</li>
<li>CenterNet(2)<ul>
<li>Center, height, width</li>
</ul>
</li>
</ul>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a target="_blank" rel="noopener" href="https://boostcamp.connect.or.kr/program_ai.html">Naver Connect Boostcamp - ai tech</a></li>
<li><a target="_blank" rel="noopener" href="https://cdm98.tistory.com/40?category=769423&fbclid=IwAR0m0_Lsg4r8VVtsLDOGm5aEepzES4o_r6FfnwoXmCdooYwl1SkuTojTATs">USPNet Review</a></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-03-10T01:15:57.000Z" title="2022. 3. 10. 오전 10:15:57">2022-03-10</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-03-12T14:20:49.314Z" title="2022. 3. 12. 오후 11:20:49">2022-03-12</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">7분안에 읽기 (약 1017 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/03/10/boostcamp/week/week8/CNN-4/">부스트 캠프 ai tech 8주 4일차 CNN visualization</a></h1><div class="content"><hr>
<h2 id="CNN-visualization-의-필요성"><a href="#CNN-visualization-의-필요성" class="headerlink" title="CNN visualization 의 필요성"></a>CNN visualization 의 필요성</h2><ul>
<li>기존 DeepLearning 모델은 내부를 볼 수 없는 시스템(Black Box)라고 여겨졌다<ul>
<li>실제로는 내부 Parameter 값을 볼 수 있지만 weight들로 이루어진 Metrix만 존재하기 때문에 해석하기 힘들다</li>
</ul>
</li>
<li>CNN visualization에서는 weight들을 시각화 해서 설명가능하게 만들어 준다<ul>
<li>filter들을 시각화 함으로써 어떤방식으로 동작하는지 설명이 가능해진다</li>
<li>weight의 GradCAM을 통하여 어떤부분에 모델이 집중하는지 보여주면서 왜 잘 동작했는지, 동작하지 않았는지를 더 쉽게 설명이 가능하다</li>
<li>시각화된 결과를 기반으로 추가적인 성능 향상을 위한 가설을 세울 수 있다</li>
</ul>
</li>
</ul>
<h1 id="CNN-Visualization"><a href="#CNN-Visualization" class="headerlink" title="CNN Visualization"></a>CNN Visualization</h1><ul>
<li>CNN Visualization에는 다양한 방법들이 존재한다</li>
<li>이 글에서는 Model behavior analysis와 Model Decision analysis부분에 대해서 다룬다</li>
</ul>
<center>

<img src="/img/CNNViz.PNG" alt="" width="600px"/>

</center>

<h1 id="Model-behavior-analysis"><a href="#Model-behavior-analysis" class="headerlink" title="Model behavior analysis"></a>Model behavior analysis</h1><ul>
<li>모델 자체의 행동에 집중하여 분석하는 기법</li>
</ul>
<h2 id="Embedding-feature-analysis"><a href="#Embedding-feature-analysis" class="headerlink" title="Embedding feature analysis"></a>Embedding feature analysis</h2><p>High Level의 Layer에서 얻어지는 Feature를 분석하는 기법이다</p>
<h3 id="Nearest-Neighbors-in-Feature-Space"><a href="#Nearest-Neighbors-in-Feature-Space" class="headerlink" title="Nearest Neighbors in Feature Space"></a>Nearest Neighbors in Feature Space</h3><ul>
<li>Nearest Neighbors를 이용한 모델 시각화이다</li>
<li>Neural Networks를 이용하여 High Level의 Feature를 뽑고 이를 이용하여 DB를 생성한다</li>
<li>test Data를 Model에 넣어서 kNN으로 모델이 생성한 High dimensional Feature Space를 확인한다</li>
<li>예제를 보고 판단하기 때문에 전체적인 부분을 확인하기는 힘들다</li>
</ul>
<h3 id="t-SNE"><a href="#t-SNE" class="headerlink" title="t-SNE"></a>t-SNE</h3><ul>
<li>위의 모델이 생성한 High demensional Feature Space를 Low dimensional Space로 변화시켜서 시각화하는 방법</li>
<li>Feature의 전체적인 그림을 그려주어서 Feature Space를 어느정도 이해할 수 있도록 도와주는 역할을 한다</li>
</ul>
<center>

<img src="/img/tSNE.PNG" alt="" width="600px"/>

</center>

<h2 id="Activation-investigation"><a href="#Activation-investigation" class="headerlink" title="Activation investigation"></a>Activation investigation</h2><p>Mid ~ High Level Layer에서 이루어지는 Feature 분석 기법이다</p>
<h3 id="Layer-Activation"><a href="#Layer-Activation" class="headerlink" title="Layer Activation"></a>Layer Activation</h3><ul>
<li>mid to high level hidden unit의 행동을 파악해보는 기법</li>
<li>특정 Layer의 특정 Node를 가공하여 어느부분을 집중적으로 보는 node인지를 masking한다</li>
</ul>
<h3 id="Maximally-activating-patches"><a href="#Maximally-activating-patches" class="headerlink" title="Maximally activating patches"></a>Maximally activating patches</h3><ul>
<li>Hidden Node 별로 가중치가 가장 높은 부분을 뜯는 기법</li>
<li>국부적인 부분에 적합하여 Mid Level Feature에서 사용한다</li>
</ul>
<h3 id="Class-visualization"><a href="#Class-visualization" class="headerlink" title="Class visualization"></a>Class visualization</h3><ul>
<li>예제 데이터를 사용하지 않고 네트워크의 parameter로 이미지를 시각화 하는 방법</li>
</ul>
<center>

<img src="/img/classviz.PNG" alt="" width="600px"/>

</center>

<ul>
<li>특정 class에 대한 네트워크의 예상치를 확인하는 방법이다</li>
<li>이것을 보고 주변객체와의 연관성 등도 파악이 가능하며 데이터의 편향성이 존재하는지도 파악 할 수 있다</li>
<li>특정 이미지를 넣어서 확인하는 것이 아닌 dummy 이미지를 넣어서 확인한다</li>
</ul>
<h2 id="Model-Decision-analysis"><a href="#Model-Decision-analysis" class="headerlink" title="Model Decision analysis"></a>Model Decision analysis</h2><ul>
<li>모델의 특정 입력에 대해서 경향에 집중하여 분석하는 기법</li>
</ul>
<h3 id="Occulsion-map"><a href="#Occulsion-map" class="headerlink" title="Occulsion map"></a>Occulsion map</h3><ul>
<li>특정 부분에 Occusion patch를 이용하여 가린 이미지들로 뽑은 Score바탕으로 heapmap을 구성하는 방법</li>
<li>특정 이미지의 스코어 영향을 미치는 영역을 파악할 수 있다.</li>
</ul>
<h3 id="via-Backpropatation"><a href="#via-Backpropatation" class="headerlink" title="via Backpropatation"></a>via Backpropatation</h3><ul>
<li>특정 이미지에 대해서 classification하는데에 영향을 미친 부분을 heapmap으로 표시하는 기법</li>
</ul>
<ol>
<li>특정 이미지에 대한 class의 스코어를 얻는다</li>
<li>Backpropagation을 통해서 입력 이미지의 Gradient를 구한다</li>
<li>얻어진 Graident의 magnitude를 구한다<ul>
<li>얼마나 큰 영향을 끼쳤는지가 중요하기 때문에 부호를 제거한다</li>
</ul>
</li>
<li>해당 map을 시각화</li>
</ol>
<h3 id="Class-Activation-Mapping"><a href="#Class-Activation-Mapping" class="headerlink" title="Class Activation Mapping"></a>Class Activation Mapping</h3><ul>
<li>특정이미지에 대해서 어떤 결과가 나왔고 어떤부분을 참조하였는지를 보여주는 기법</li>
<li>Global Average Pooling + FC Layer가 있는 모델에서만 사용이 가능하다</li>
</ul>
<h3 id="Grad-CAM"><a href="#Grad-CAM" class="headerlink" title="Grad CAM"></a>Grad CAM</h3><ul>
<li>CAM과 같이 특정이미지에 대해서 어떤 결과가 나왔고 어떤부분을 참조하였는지를 보여주는 기법</li>
<li>CNN Backbone이기만 하면 어떤 모델이든지 사용이 가능하다</li>
</ul>
<center>

<img src="/img/GradCAM.PNG" alt="" width="600px"/>

</center>

<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a target="_blank" rel="noopener" href="https://boostcamp.connect.or.kr/program_ai.html">Naver Connect Boostcamp - ai tech</a></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-03-09T06:47:55.000Z" title="2022. 3. 9. 오후 3:47:55">2022-03-09</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-03-11T14:11:27.066Z" title="2022. 3. 11. 오후 11:11:27">2022-03-11</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">14분안에 읽기 (약 2095 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/03/09/boostcamp/week/week8/CNN-3/">부스트 캠프 ai tech 8주 3일차 Object Detection</a></h1><div class="content"><hr>
<h2 id="Object-Detection"><a href="#Object-Detection" class="headerlink" title="Object Detection"></a>Object Detection</h2><ul>
<li>특정 오브젝트가 어디에 위치해있고, 그 오브젝트가 무엇인지를 탐지하는 Task를 말한다</li>
<li>Classification과 Box Localization을 같이 하는 Task</li>
<li>자율주행이나 OCR등에서 사용된다</li>
<li>Classification과 Box Localization을 따로 연산하는 Two Stage 방식과 동시에 연산하는 One Stage 방식이 존재한다</li>
</ul>
<h2 id="Two-Stage-Object-Detection"><a href="#Two-Stage-Object-Detection" class="headerlink" title="Two Stage Object Detection"></a>Two Stage Object Detection</h2><ul>
<li>이미지의 BBox를 추출하고 이 BBox로 Classification을 진행하는 모델을 말한다</li>
<li>R-CNN 계열의 모델이 여기에 속한다</li>
<li>2단계로 연산을 하기 때문에 연산속도는 느린편에 속하지만 정확도가 높다</li>
</ul>
<h3 id="Selective-Search"><a href="#Selective-Search" class="headerlink" title="Selective Search"></a>Selective Search</h3><ul>
<li>이미지로부터 BBox를 만들어 내는 알고리즘</li>
<li>이미지의 색상단위로 Over Segmentation을 진행하고 규칙에 따라 점점 합쳐나가는 알고리즘<ul>
<li>Color Similarity</li>
<li>Texture Similarity</li>
<li>Size Similarity</li>
<li>Shape Similarity</li>
<li>A final meta-similarity measure</li>
</ul>
</li>
</ul>
<center>

<img src="/img/selectivesearch.PNG" alt="" width="600px"/>

</center>

<h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h2><p>Classification이 바로 Object Detection에 응용된 모델이다.<br>모델은 아래와 같이 간단한 단계로 BBox를 구하고, Classification을 진행한다  </p>
<ol>
<li>Selective Search를 사용하여 물체가 있을 법한 후보를 선택한다(~2k)</li>
<li>선택된 후보군 전체에 대해서 이미지의 크기를 재가공하여 Classification 모델에 집어넣는다 (2000개의 후보들에 대해서 모두 CNN, SVM 연산)</li>
</ol>
<center>

<img src="/img/RCNN.PNG" alt="" width="500px"/>

</center>

<ul>
<li>RCNN은 초기 모델인 만큼 다양한 문제점 또한 존재한다<ul>
<li>BBox를 뽑아내는 알고리즘은 Seletive Search 같은 Huristic 알고리즘이기 때문에 학습이 불가능해서 성능향상이 크지 않다</li>
<li>Selective Search는 Cpu에서 연산이 이루어지기 때문에 시간도 많이 소요된다</li>
<li>후보 전체에 대하여 Classfication을 한번씩 진행하다보니 연산량이 많아져서 시간소모가 크다</li>
</ul>
</li>
</ul>
<h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h2><p>기존의 R-CNN의 연산이 매우 오래걸린것을 해결한 모델이다.<br>Roi Pooling을 이용하여 모든 후보에 대해서 Convolution Network 에 입력하던것을 단 1번으로 줄였다.  </p>
<ul>
<li>Keyword<ul>
<li>RoI Pooling</li>
<li>SPPNet</li>
</ul>
</li>
</ul>
<center>

<img src="/img/fastRCNN.PNG" alt="" width="500px"/>

</center>

<h3 id="RoI-Pooling을-이용한-Fast-R-CNN"><a href="#RoI-Pooling을-이용한-Fast-R-CNN" class="headerlink" title="RoI Pooling을 이용한 Fast R-CNN"></a>RoI Pooling을 이용한 Fast R-CNN</h3><ol>
<li>전체 이미지를 CNN에 통과시켜서 Feature Map을 추출한다</li>
<li>Seletive Search등의 Region Proposal Method를 이용하여 RoI를 찾는다</li>
<li>찾은 RoI를 미리 뽑아둔 Feature Map에 투영하여 RoI에 해당하는 부분에 대해서 Pooling을 진행해서(SPPNet 이용) Classification을 위한 FC Layer의 input Size에 맞춘다</li>
<li>softmax연산을 통하여 Classification을 진행한다</li>
<li>Bounding Box Regression을 통하여 BBox의 위치를 재조정한다 </li>
</ol>
<p>R-CNN에서 RoI Pooling을 도입하여 연산속도면에서 획기적인 발전을 이룩한 모델이지만 여러 한계점이 존재했다</p>
<ul>
<li>아직도 Seletive Search를 사용하기 때문에 BBox 검출에 대해서 큰 학습이 이루어지지 않았기 때문에 성능면에서는 큰 차이가 없다</li>
<li>모델 뒷부분의 성능은 개선되었지만 BBox 검출 속도는 그대로기 때문에 Bottleneck 현상이 발생한다 </li>
</ul>
<h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h2><p>Fast R-CNN의 단점이었던 Seletive Search를 Nueral Network(RPN)로 대체함으로써 End to End로 학습이 가능해진 모델이다  </p>
<ul>
<li>Keyword<ul>
<li>RPN</li>
<li>NMS</li>
<li>IoU (Intersection over Union)<ul>
<li>두 BBox가 얼마나 잘 겹쳐있는지를 판단하는 Metric</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h3><ul>
<li>Region Proposal Network </li>
</ul>
<p>R-CNN 계열에서 RoI를 생성하던 Region Proposal Method를 대체하는 Network이다.<br>RPN에서는 다양한 모양의 BBox를 출력 해 내기 위해서 미리 특정 크기의 Anchor Box들을 구현해 놓고 이 Anchor Box들과 대조하여 IoU를 계산한다.</p>
<ul>
<li>Faster RCNN 에서는 3개의 Scale과 3개의 비율을 조합하여 9개의 Anchor Box를 미리 정해 두었다.</li>
</ul>
<p>RPN에서는 Slide Window 방식으로 Anchor Box를 이용하여 물체가 존재하는지에 대한 유무와 BBox의 delta 값을 Feature Map으로 부터 추출한다</p>
<ul>
<li>Delta : 고정된 크기의 Anchor Box를 실제 BBox에 일치시키는 이동 정보를 담고있는 벡터를 말한다</li>
</ul>
<p>결론적으로 RPN을 학습시키면 물체가 존재할 가능성이 높은 BBox를 도출하는 쪽으로 학습이 진행된다</p>
<center>

<img src="/img/FasterRCNN.PNG" alt="" width="500px"/>

</center>

<p>RPN으로 부터 추출된 BBox 좌표를 기준으로 기존에 뽑아두었던 Feature Map에 RoI Pooling을 적용하고 Classification과 Box Regression을 진행한다</p>
<h3 id="Non-Maximum-Suppression"><a href="#Non-Maximum-Suppression" class="headerlink" title="Non Maximum Suppression"></a>Non Maximum Suppression</h3><ul>
<li>RPN으로 생성된 수많은 BBox중 중복되는 Box들을 지우는 알고리즘</li>
</ul>
<ol>
<li>동일한 클래스에 대해서 Sorting을 통해 Comfidence 순서로 정렬 시킨다</li>
<li>가장 Confidence가 높은 BBox와 IoU가 일정 이상인 BBox는 중복되었다고 판단하고 삭제한다</li>
</ol>
<h3 id="추가적인-사항"><a href="#추가적인-사항" class="headerlink" title="추가적인 사항"></a>추가적인 사항</h3><ul>
<li>실제로 Faster RCNN을 학습시킬때는 RPN과 RCNN 모델을 따로 각각 학습을 시킨뒤에 붙였다고 한다</li>
</ul>
<h2 id="One-Stage-Object-Detection"><a href="#One-Stage-Object-Detection" class="headerlink" title="One Stage Object Detection"></a>One Stage Object Detection</h2><ul>
<li>이미지의 BBox를 추출하면서 Classification까지 동시에 진행하는 모델을 말한다</li>
<li>1단계로 연산을 하기 때문에 실시간 처리속도가 높지만 정확도가 2 Stage Detector 보다 조금 떨어진다</li>
</ul>
<h2 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h2><ul>
<li>One Stage Object Detection Model의 대표적인 모델</li>
<li>Faster RCNN과 유사하게 Anchor Box와 Box Regression을 통해서 BBox를 예측한다</li>
<li>Anchor Box의 위치를 찾는것과 동시에 Class Probability map을 생성한다</li>
<li>Class Probability map과 BBox를 합쳐서 detection을 마친다</li>
</ul>
<center>

<img src="/img/yolo.PNG" alt="" width="600px"/>

</center>

<ul>
<li>초당 처리 프레임 수는 Faster RCNN을 앞섰지만, 성능면에서는 조금 떨어지는 경향을 보였다</li>
</ul>
<h2 id="Single-Shot-MultiBox-Detector-SSD"><a href="#Single-Shot-MultiBox-Detector-SSD" class="headerlink" title="Single Shot MultiBox Detector(SSD)"></a>Single Shot MultiBox Detector(SSD)</h2><ul>
<li>YOLO의 정확도 문제가 개선된 One Stage 모델이다</li>
<li>아래의 그림과 같이 여러개의 Feature Map에서 Anchor를 이용하여 Feature를 추출한다</li>
</ul>
<center>

<img src="/img/SSD.PNG" alt="" width="600px"/>

</center>

<ul>
<li>각 레이어마다 Anchor Box에 대한 정보들을 종합하여 최종적으로 NMS를 통해 겹치는 BBox를 제거하여 결과를 낸다</li>
<li>이를 통하여 Yolo와 비슷한 수준의 처리속도와 Faster RCNN을 넘는 성능을 보여주었다</li>
</ul>
<center>

<img src="/img/SSD1.PNG" alt="" width="600px"/>

</center>

<h2 id="RetinaNet"><a href="#RetinaNet" class="headerlink" title="RetinaNet"></a>RetinaNet</h2><ul>
<li>Focal Loss와 FPN구조를 도입함으로써 One Stage Detector의 성능을 더욱 끌어올린 모델 구조</li>
<li>Keyword<ul>
<li>Focal Loss</li>
<li>Feature Pyramid Network(FPN)</li>
</ul>
</li>
</ul>
<h3 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a>Focal Loss</h3><ul>
<li>One Stage Detector의 고질적인 문제인 적은 Positive Sample 문제를 해결하기 위해 고안된 Loss 함수이다<ul>
<li>Positive Sample은 단 하나지만 Negative Sample은 엄청나게 많이 발생하기 때문에 Class imbalance 문제가 발생한다</li>
</ul>
</li>
<li>Cross Entropy loss로 부터 고안된 Loss 함수</li>
<li>맞추기 쉬운 Sample에 대해서 발생하는 weight를 낮추고 맞추기 힘든 Sample에 대해서는 높은 weight를 주게 된다<ul>
<li>loss값은 Focal loss가 작지만 같은 지점의 Gradient를 보면 Focal Loss가 훨씬 크다</li>
</ul>
</li>
</ul>
<center>

<img src="/img/focal.PNG" alt="" width="500px"/>

</center>

<h3 id="Feature-Pyramid-Networks"><a href="#Feature-Pyramid-Networks" class="headerlink" title="Feature Pyramid Networks"></a>Feature Pyramid Networks</h3><ul>
<li>서로다른 해상도의 Feature Map을 쌓아올린 형태를 가지는 CNN 모델이다</li>
<li>입력층에 가까울수록 Low Level의 Feature를 가지고, 출력층에 가까울수록 High Level(Global Level)의 Feature를 보유하는 CNN의 특성을 이용하였다</li>
</ul>
<center>

<img src="/img/FPN1.PNG" alt="" width="500px"/>

</center>

<ul>
<li>(a)는 다양한 Scale의 Feature 맵을 사용해서 다양한 크기의 Object를 탐색하는 것이 가능하고 성능도 좋다. 하지만 여러장의 이미지에서 모두 Feature Map을 추출해야하기 때문에 느린 처리속도를 가진다</li>
<li>기존의 Yolo는 단일 Scale의 Feature 맵을 사용하는 (b) 방식을 사용했다. 모델구조가 단순하여 빠른 처리가 가능하지만, 성능이 떨어지는 단점이 존재한다</li>
<li>SSD에서 사용한 방식인 (c)는 다양항 크기의 Feature Map을 사용하여 좋은 성능과 빠른 처리속도를 보여주었지만, Backbone을 지나서 충분하게 High Level 수준의 Feature들을 합쳐서 결과를 내기 때문에, 더 높은 해상도의 Low Level Feature Map(High Resolution map)을 사용 하지 않아 Small Object 검출에 한계가 있다고 논문에서 서술했다</li>
<li>FPN에서는 (d)의 구조를 통하여 Low Level 부터 High Level 까지의 Feature Map을 전부 사용하여 검출이 힘든 작은 물체 까지 잘 검출하는 모습을 보여주었다</li>
</ul>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a target="_blank" rel="noopener" href="https://boostcamp.connect.or.kr/program_ai.html">Naver Connect Boostcamp - ai tech</a></li>
</ul>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/tags/CV/page/0/">이전</a></div><div class="pagination-next"><a href="/tags/CV/page/2/">다음</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/tags/CV/">1</a></li><li><a class="pagination-link" href="/tags/CV/page/2/">2</a></li></ul></nav></div><div class="column column-left is-3-tablet is-3-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="KyuBum Shin"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">KyuBum Shin</p><p class="is-size-6 is-block">Student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Incheon</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">포스트</p><a href="/archives"><p class="title">130</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">카테고리</p><a href="/categories"><p class="title">26</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">태그</p><a href="/tags"><p class="title">52</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/KyubumShin" target="_blank" rel="noopener">팔로우</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/KyubumShin"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">링크</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/KyubumShin" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">카테고리</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Computer-Science/"><span class="level-start"><span class="level-item">Computer Science</span></span><span class="level-end"><span class="level-item tag">8</span></span></a><ul><li><a class="level is-mobile" href="/categories/Computer-Science/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Computer-Science/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Computer-Science/OS/"><span class="level-start"><span class="level-item">OS</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Computer-Science/%EC%9E%90%EB%A3%8C%EA%B5%AC%EC%A1%B0/"><span class="level-start"><span class="level-item">자료구조</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Data-Engineering/"><span class="level-start"><span class="level-item">Data Engineering</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/Data-Engineering/Bigdata/"><span class="level-start"><span class="level-item">Bigdata</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/DeepLearning/"><span class="level-start"><span class="level-item">DeepLearning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/DeepLearning/Basic/"><span class="level-start"><span class="level-item">Basic</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Paper/"><span class="level-start"><span class="level-item">Paper</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/Paper/CV/"><span class="level-start"><span class="level-item">CV</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/Paper/CV/OB/"><span class="level-start"><span class="level-item">OB</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/Programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/Programming/Docker/"><span class="level-start"><span class="level-item">Docker</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Programming/Python/tip/"><span class="level-start"><span class="level-item">tip</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/boostcamp/"><span class="level-start"><span class="level-item">boostcamp</span></span><span class="level-end"><span class="level-item tag">93</span></span></a><ul><li><a class="level is-mobile" href="/categories/boostcamp/Dairy/"><span class="level-start"><span class="level-item">Dairy</span></span><span class="level-end"><span class="level-item tag">34</span></span></a></li><li><a class="level is-mobile" href="/categories/boostcamp/Peer-Session/"><span class="level-start"><span class="level-item">Peer Session</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/boostcamp/Problems/"><span class="level-start"><span class="level-item">Problems</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/boostcamp/week/"><span class="level-start"><span class="level-item">week</span></span><span class="level-end"><span class="level-item tag">55</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%B0%B1%EC%97%94%EB%93%9C/"><span class="level-start"><span class="level-item">백엔드</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%B0%B1%EC%97%94%EB%93%9C/%EB%A9%94%EC%84%B8%EC%A7%80%ED%81%90/"><span class="level-start"><span class="level-item">메세지큐</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%9D%BC%EC%83%81/"><span class="level-start"><span class="level-item">일상</span></span><span class="level-end"><span class="level-item tag">48</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%9D%BC%EC%83%81/TIL/"><span class="level-start"><span class="level-item">TIL</span></span><span class="level-end"><span class="level-item tag">47</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9D%BC%EC%83%81/%EA%B3%84%ED%9A%8D/"><span class="level-start"><span class="level-item">계획</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">최근 글</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-07-16T12:24:11.000Z">2022-07-16</time></p><p class="title"><a href="/2022/07/16/paper/Ghostnet/">GhostNet: More Features from Cheap Operations</a></p><p class="categories"><a href="/categories/Paper/">Paper</a> / <a href="/categories/Paper/CV/">CV</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-06-22T13:38:19.000Z">2022-06-22</time></p><p class="title"><a href="/2022/06/22/Few-Shot-Learning/">Meta Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-06-20T02:02:36.000Z">2022-06-20</time></p><p class="title"><a href="/2022/06/20/DE/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A5%BC-%EC%A7%80%ED%83%B1%ED%95%98%EB%8A%94-%EA%B8%B0%EC%88%A0-1/">빅데이터를 지탱하는 기술 (1)</a></p><p class="categories"><a href="/categories/Data-Engineering/">Data Engineering</a> / <a href="/categories/Data-Engineering/Bigdata/">Bigdata</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-06-01T00:50:53.000Z">2022-06-01</time></p><p class="title"><a href="/2022/06/01/tip/walrus/">Walrus 연산자</a></p><p class="categories"><a href="/categories/Programming/">Programming</a> / <a href="/categories/Programming/Python/">Python</a> / <a href="/categories/Programming/Python/tip/">tip</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-23T01:19:47.000Z">2022-04-23</time></p><p class="title"><a href="/2022/04/23/tip/conda-cuda-%EC%84%A4%EC%B9%98/">cudatoolkit 버전별로 관리하기</a></p><p class="categories"><a href="/categories/Programming/">Programming</a> / <a href="/categories/Programming/Python/">Python</a> / <a href="/categories/Programming/Python/tip/">tip</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">아카이브</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">7월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">6월 2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">4월 2022</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/03/"><span class="level-start"><span class="level-item">3월 2022</span></span><span class="level-end"><span class="level-item tag">20</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/02/"><span class="level-start"><span class="level-item">2월 2022</span></span><span class="level-end"><span class="level-item tag">34</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">1월 2022</span></span><span class="level-end"><span class="level-item tag">39</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/10/"><span class="level-start"><span class="level-item">10월 2021</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/09/"><span class="level-start"><span class="level-item">9월 2021</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/08/"><span class="level-start"><span class="level-item">8월 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">태그</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/3D/"><span class="tag">3D</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Ai-Math/"><span class="tag">Ai Math</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Ai-serving/"><span class="tag">Ai serving</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bigdata/"><span class="tag">Bigdata</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CNN/"><span class="tag">CNN</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CNN-Viz/"><span class="tag">CNN Viz</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CV/"><span class="tag">CV</span><span class="tag">12</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Engineering/"><span class="tag">Data Engineering</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-visualization/"><span class="tag">Data visualization</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DeepLearning/"><span class="tag">DeepLearning</span><span class="tag">25</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docker/"><span class="tag">Docker</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Function/"><span class="tag">Function</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN/"><span class="tag">GAN</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Generative-Model/"><span class="tag">Generative Model</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Instance-Segmentation/"><span class="tag">Instance Segmentation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LandMark-Detection/"><span class="tag">LandMark Detection</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MLops/"><span class="tag">MLops</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Multi-modal/"><span class="tag">Multi-modal</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OCR/"><span class="tag">OCR</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Object-Detection/"><span class="tag">Object Detection</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Project-product/"><span class="tag">Project product</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Semantic-Segmentation/"><span class="tag">Semantic Segmentation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Sequencial-Model/"><span class="tag">Sequencial Model</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Transformer/"><span class="tag">Transformer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VAE/"><span class="tag">VAE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/backend/"><span class="tag">backend</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cs/"><span class="tag">cs</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/database/"><span class="tag">database</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/env/"><span class="tag">env</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/message/"><span class="tag">message</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/network/"><span class="tag">network</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/os/"><span class="tag">os</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">11</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pytorch/"><span class="tag">pytorch</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week1/"><span class="tag">week1</span><span class="tag">19</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week10/"><span class="tag">week10</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week11/"><span class="tag">week11</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week12/"><span class="tag">week12</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week13/"><span class="tag">week13</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week14/"><span class="tag">week14</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week2/"><span class="tag">week2</span><span class="tag">15</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week3/"><span class="tag">week3</span><span class="tag">11</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week4/"><span class="tag">week4</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week5/"><span class="tag">week5</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week6/"><span class="tag">week6</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week8/"><span class="tag">week8</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week9/"><span class="tag">week9</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%9D%BC%EC%83%81/"><span class="tag">일상</span><span class="tag">47</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%9E%90%EB%A3%8C%EA%B5%AC%EC%A1%B0/"><span class="tag">자료구조</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%95%A0%EC%9D%BC/"><span class="tag">할일</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">업데이트 소식 받기</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="구독"></div></div></form></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="구독"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="KyuBum&#039;s Dev Blog" height="28"></a><p class="is-size-7"><span>&copy; 2022 KyuBum Shin</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ko");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="맨 위로" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "이 웹 사이트는 귀하의 경험을 향상시키기 위해 Cookie를 사용합니다.",
          dismiss: "무시",
          allow: "허용",
          deny: "거부",
          link: "더 알아보기",
          policy: "Cookie 정책",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="입력 하세요..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"입력 하세요...","untitled":"(제목 없음)","posts":"포스트","pages":"페이지","categories":"카테고리","tags":"태그"});
        });</script></body></html>