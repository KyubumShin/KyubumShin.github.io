<!doctype html>
<html lang="ko"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>카테고리: boostcamp - KyuBum&#039;s Dev Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="KyuBum Shin"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="KyuBum Shin"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="KyuBum&#039;s Dev Blog"><meta property="og:url" content="https://kyubumshin.github.io/"><meta property="og:site_name" content="KyuBum&#039;s Dev Blog"><meta property="og:locale" content="ko_KR"><meta property="og:image" content="https://kyubumshin.github.io/img/og_image.png"><meta property="article:author" content="KyuBum Shin"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://kyubumshin.github.io"},"headline":"KyuBum's Dev Blog","image":["https://kyubumshin.github.io/img/og_image.png"],"author":{"@type":"Person","name":"KyuBum Shin"},"publisher":{"@type":"Organization","name":"KyuBum's Dev Blog","logo":{"@type":"ImageObject","url":"https://kyubumshin.github.io/img/logo.svg"}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.0.2"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="KyuBum&#039;s Dev Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="검색" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-9-tablet is-9-desktop is-9-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">카테고리</a></li><li class="is-active"><a href="#" aria-current="page">boostcamp</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-01-26T05:50:44.000Z" title="2022. 1. 26. 오후 2:50:44">2022-01-26</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-01-28T04:45:40.245Z" title="2022. 1. 28. 오후 1:45:40">2022-01-28</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">5분안에 읽기 (약 736 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/01/26/boostcamp/week/week2/pytorch-6/">부스트 캠프 ai tech 2주 3일차 Pytorch (5)</a></h1><div class="content"><hr>
<ul>
<li>학습시킨 모델을 다른사람들에게 공유하거나, 보관하기 위에서는 메모리에 있는 Model들을 따로 파일로 만들어서 저장할 필요가 있는데 본 글에서는 저장을 어떻게 해야하는지, 그리고 이를 이용한 Tranfer Learning 에 대해서 다룰 예정이다  </li>
</ul>
<h2 id="5-Pytorch-Model-Save-amp-Load"><a href="#5-Pytorch-Model-Save-amp-Load" class="headerlink" title="5. Pytorch Model Save &amp; Load"></a>5. Pytorch Model Save &amp; Load</h2><ul>
<li>torch.save()<ul>
<li>학습의 결과를 저장하기 위한 함수이다</li>
<li>모델의 Layer들과 Parameter, Buffer를 저장한다</li>
<li>학습 중간중간 Model을 저장해서 최선의 성능을 가지는 결과모델을 선택하는 방식으로 사용 할 수 있다 (Checkpoint)<blockquote>
<ul>
<li>model : 학습한 모델</li>
<li>PATH : 모델을 저장할 directory</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모델의 weight 만 저장하는 방법</span></span><br><span class="line">torch.save(model.state_dict(), PATH)</span><br><span class="line"><span class="comment"># 모델의 weight와 내부모듈 구조, Buffer까지 저장하는 방법</span></span><br><span class="line">torch.save(model, PATH)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>checkpoints<ul>
<li>학습의 중간 결과를 저장해서 최선의 성능을 가지는 결과모델을 선택하는 방법</li>
<li>보통 early stopping 기법과 함께 사용한다<ul>
<li>early stopping : Loss와 Metric값을 지속적으로 확인 하면서 일정 기간이상 줄지 않으면 학습을 멈추는 방법</li>
</ul>
</li>
<li>일반적으로 epoch, loss, mertic을 함께 저장하여 확인한다  </li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.save(&#123;</span><br><span class="line">    <span class="string">&#x27;epoch&#x27;</span>: epoch,</span><br><span class="line">    <span class="string">&#x27;model_state_dict&#x27;</span>: model.state_dict(),</span><br><span class="line">    <span class="string">&#x27;optimizer_state_dict&#x27;</span>: optimizer.state_dict(),</span><br><span class="line">    <span class="string">&#x27;loss&#x27;</span>: epoch_loss,</span><br><span class="line">    &#125;, <span class="string">f&quot;saved/checkpoint_model_<span class="subst">&#123;epoch&#125;</span>_<span class="subst">&#123;epoch_loss/<span class="built_in">len</span>(dataloader)&#125;</span>_<span class="subst">&#123;epoch_acc/<span class="built_in">len</span>(dataloader)&#125;</span>.pt&quot;</span>)</span><br><span class="line"></span><br><span class="line">checkpoint = torch.load(PATH)</span><br><span class="line">model.load_state_dict(checkpoint[<span class="string">&#x27;model_state_dict&#x27;</span>])</span><br><span class="line">optimizer.load_state_dict(checkpoint[<span class="string">&#x27;optimizer_state_dict&#x27;</span>])</span><br><span class="line">epoch = checkpoint[<span class="string">&#x27;epoch&#x27;</span>]</span><br><span class="line">loss = checkpoint[<span class="string">&#x27;loss&#x27;</span>]</span><br></pre></td></tr></table></figure>

<h2 id="6-Transfer-Learning"><a href="#6-Transfer-Learning" class="headerlink" title="6. Transfer Learning"></a>6. Transfer Learning</h2><ul>
<li>다른 데이터셋으로 만든 모델을 현재 데이터셋에 맞춰서 다시 학습시키는 방법<ul>
<li>일반적으로 큰 데이터셋으로 학습시킨 모델(ex Imagenet 10K로 학습시킨 resnet50 등등)의 성능이 다른 데이터셋에 적용시키는것이 처음부터 학습하는 모델보다 학습이 빠르고, 학습이 잘된다</li>
</ul>
</li>
<li>현재 DeepLearning에서 가장 일반적인 학습 방법이다</li>
<li>기존의 pretrained 된 모델을 backbone 모델이라고 하며 여기서 일부 Layer만 변경시켜서 학습을 수행한다</li>
<li>CV : Pytorch 공식 비전 라이브러리 TorchVision 이나 torch image model(timm)을 많이 이용한다</li>
<li>NLP : transformer 전문 라이브러리인 HuggingFace를 많이 사용한다</li>
</ul>
<h3 id="6-1-Freezing"><a href="#6-1-Freezing" class="headerlink" title="6.1 Freezing"></a>6.1 <strong>Freezing</strong></h3><ul>
<li>pretrained model을 활용할때 모델의 일부분을 freeze 시켜 파라미터의 업데이트가 일어나는것을 막는 방법</li>
<li>DeepLearning의 특성상 학습이 계속 진행되면서 파라미터가 바뀌면 과거에 학습했던 정보가 희석되는 현상이 일어나는데 특히 pretrained 모델에게 안좋은 영향을 준다</li>
<li>pytorch의 requires_grad를 비활성화 시키거나 hook를 이용해서 backward의 input_grad를 0으로 고정시켜버리는 것으로도 가능하다</li>
</ul>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a target="_blank" rel="noopener" href="https://boostcamp.connect.or.kr/program_ai.html">Naver Connect Boostcamp - ai tech</a></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-01-26T01:50:44.000Z" title="2022. 1. 26. 오전 10:50:44">2022-01-26</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-01-28T04:54:19.713Z" title="2022. 1. 28. 오후 1:54:19">2022-01-28</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">5분안에 읽기 (약 730 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/01/26/boostcamp/week/week2/pytorch-5/">부스트 캠프 ai tech 2주 3일차 Pytorch (4)</a></h1><div class="content"><hr>
<h2 id="4-Dataset-amp-DataLoader"><a href="#4-Dataset-amp-DataLoader" class="headerlink" title="4. Dataset &amp; DataLoader"></a>4. <strong>Dataset &amp; DataLoader</strong></h2><ul>
<li>pytorch에서 생성한 모델을 학습시키기 위해 데이터를 공급해주는 유틸리티</li>
</ul>
<h3 id="4-1-Dataset"><a href="#4-1-Dataset" class="headerlink" title="4.1 Dataset"></a>4.1 <strong>Dataset</strong></h3><ul>
<li>Data를 담고 있는 Class</li>
<li>pytorch Dataset은 아래와 같이 3가지의 기본 Method로 구성되어있다</li>
<li>__init__: 초기화 함수. 필요한 변수들을 선언하고, data를 load하는 부분이다</li>
<li>__len__: 데이터의 개수를 반환하는 함수. Dataloader에서 길이등을 반환하는데 쓰인다</li>
<li>__get_item__(index): index번째의 data를 반환하는 함수. tensor로 return 해준다.</li>
<li>데이터에 따라 Map style과 iterable style로 나뉜다<ul>
<li>Map style : 일반적인 data 구조</li>
<li>iterable style : random으로 읽기 어렵거나 data에 따라 batchsize가 달라지는 data. 시계열 데이터 등에 적합하다</li>
</ul>
</li>
<li>Map style 코드는 아래와 같다  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, path</span>):</span></span><br><span class="line">        self.data = pd.read_csv(path)</span><br><span class="line">        self.X = self.data.drop([<span class="string">&#x27;label&#x27;</span>])</span><br><span class="line">        self.y = self.data[<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__get_item__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.X.iloc[idx], self.y[idx]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.X)</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="4-2-DataLoader"><a href="#4-2-DataLoader" class="headerlink" title="4.2 DataLoader"></a>4.2 <strong>DataLoader</strong></h3><ul>
<li>Dataset을 iterable 하게 사용할 수 있도록 도와주는 Utility</li>
<li>data loading 순서 커스터마이징, 자동 batch 설정, Single-Multi process data loading등 여러가지 기능을 지원한다</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DataLoader(dataset, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>, sampler=<span class="literal">None</span>,</span><br><span class="line">           batch_sampler=<span class="literal">None</span>, num_workers=<span class="number">0</span>, collate_fn=<span class="literal">None</span>,</span><br><span class="line">           pin_memory=<span class="literal">False</span>, drop_last=<span class="literal">False</span>, timeout=<span class="number">0</span>,</span><br><span class="line">           worker_init_fn=<span class="literal">None</span>, *, prefetch_factor=<span class="number">2</span>,</span><br><span class="line">           persistent_workers=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<ol>
<li>dataset<ul>
<li><code>torch.utils.data.Dataset</code> parameter</li>
</ul>
</li>
<li>batch_size <ul>
<li>Data를 불러올 때 배치사이즈를 설정하는 항목  </li>
</ul>
</li>
<li>shuffle <ul>
<li>Data load 순서를 항상 랜덤하게 뽑을지를 결정하는 항목</li>
<li>torch.manual_seed 를 통해 랜덤값을 고정할 수도 있다  </li>
</ul>
</li>
<li>sampler<ul>
<li>Data의 index를 컨트롤 하는 방법</li>
<li><code>torch.utils.data.Sampler</code> 객체를 사용한다</li>
<li>SequentialSampler : 항상 같은 순서로 elements들을 sampling한다</li>
<li>RandomSampler : 랜덤하게 sampling 한다. replacement 가능, random의 범위를 지정 가능하다 (default=len(dataset))</li>
<li>SubsetRandomSampler : 랜덤하게 sampling 한다 위의 두 기능은 없다</li>
<li>WeigthRandomSampler : 가중치에 따라 뽑히는 확률이 달라진다</li>
<li>BatchSampler : Batch단위로 sampling을 해준다</li>
<li>DistributedSampler : Multi GPU에서 분산처리를 할때 사용한다  </li>
</ul>
</li>
<li>batch_sampler<ul>
<li>sampler와 같지만 기본적으로 BatchSampler가 적용된 상태이다</li>
</ul>
</li>
<li>num_workers <ul>
<li>GPU에 Data를 load 할때 사용할 process의 수를 결정한다</li>
</ul>
</li>
<li>collate_fn <ul>
<li>sample list를 합쳐서 tensor의 minibatch로 바꿔주는 기능. map style의 dataset에서 사용한다</li>
<li>데이터마다의 길이가 다른 NLP에서 많이 사용한다</li>
</ul>
</li>
<li>pin_memory <ul>
<li>pin memory를 사용하여 GPU에 더 빠르게 data를 load하는 방법. </li>
<li>추가적인 메모리 자원이 필요하다. 보통 parallel 모델에서 많이 사용한다 </li>
</ul>
</li>
<li>drop_last <ul>
<li>Data의 전체 개수가 batchsize로 나누어 떨어지지 않을때 마지막 batch를 drop를 결정하는 parameter</li>
</ul>
</li>
</ol>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a target="_blank" rel="noopener" href="https://boostcamp.connect.or.kr/program_ai.html">Naver Connect Boostcamp - ai tech</a></li>
<li><a target="_blank" rel="noopener" href="https://subinium.github.io/pytorch-dataloader/">[Pytorch] DataLoader parameter별 용도</a></li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">Pytorch DataLoader 공식문서</a></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-01-25T05:17:24.000Z" title="2022. 1. 25. 오후 2:17:24">2022-01-25</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-01-27T05:58:09.333Z" title="2022. 1. 27. 오후 2:58:09">2022-01-27</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/%EC%9D%BC%EC%83%81/">일상</a><span> / </span><a class="link-muted" href="/categories/%EC%9D%BC%EC%83%81/TIL/">TIL</a><span> / </span><a class="link-muted" href="/categories/boostcamp/Dairy/">Dairy</a></span><span class="level-item">몇 초안에 읽기 (약 86 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/01/25/boostcamp/Dairy/Week2-Day-Review-2/">Week2 - Day 2 Review</a></h1><div class="content"><hr>
<h2 id="1-오늘-하루-한-것"><a href="#1-오늘-하루-한-것" class="headerlink" title="1. 오늘 하루 한 것"></a>1. 오늘 하루 한 것</h2><ul>
<li>강의<ul>
<li>pytorch 4, 5강</li>
</ul>
</li>
<li>과제<ul>
<li>pytorch 기본과제1 끝내기, 기본과제2 중간까지</li>
</ul>
</li>
<li>정리<ul>
<li>pytorch 기본과제1, 4강, 5강</li>
</ul>
</li>
</ul>
<h2 id="2-피어세션에서-한-것"><a href="#2-피어세션에서-한-것" class="headerlink" title="2. 피어세션에서 한 것"></a>2. 피어세션에서 한 것</h2><ul>
<li>4강 코드가 안돌아가요</li>
<li>코테 문제 코드 리뷰</li>
</ul>
<h2 id="3-내일-할것"><a href="#3-내일-할것" class="headerlink" title="3. 내일 할것"></a>3. 내일 할것</h2><ul>
<li>기본과제2 끝내기</li>
</ul>
<h2 id="4-하루-느낀점"><a href="#4-하루-느낀점" class="headerlink" title="4. 하루 느낀점"></a>4. 하루 느낀점</h2><ul>
<li>오늘은 좀 할만하…지??</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-01-24T07:16:44.000Z" title="2022. 1. 24. 오후 4:16:44">2022-01-24</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-02-01T13:00:59.490Z" title="2022. 2. 1. 오후 10:00:59">2022-02-01</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/Peer-Session/">Peer Session</a></span><span class="level-item">1분안에 읽기 (약 147 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/01/24/boostcamp/Peer%20Session/Peer2/">Week2 Peer Session</a></h1><div class="content"><hr>
<h3 id="1-tensor가-벡터형태-일-때-Backward를-진행하면-왜-아래와-같이-표기해야하나"><a href="#1-tensor가-벡터형태-일-때-Backward를-진행하면-왜-아래와-같이-표기해야하나" class="headerlink" title="1. tensor가 벡터형태 일 때 Backward를 진행하면 왜 아래와 같이 표기해야하나?"></a>1. tensor가 벡터형태 일 때 Backward를 진행하면 왜 아래와 같이 표기해야하나?</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">b = tensor([<span class="number">4</span>, <span class="number">1</span>])</span><br><span class="line">Q = a*<span class="number">2</span> + b**<span class="number">2</span></span><br><span class="line">Q.backward(gradient = tensor([<span class="number">1</span>, <span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
<ul>
<li>Pytorch에서는 scalar 값이 아닌 tensor에서는 backward의 시작점으로 보지 않기 때문에 벡터에 따로 gradient를 지정해 줘야한다.  </li>
</ul>
<h3 id="2-optimizer-zero-grad"><a href="#2-optimizer-zero-grad" class="headerlink" title="2. optimizer.zero_grad()"></a>2. optimizer.zero_grad()</h3><ul>
<li>Gradient를 초기화 해주는 함수를 말한다</li>
<li>만약 초기화 해주지 않는다면 tensor가 backward 연산될때마다 grad에 더해져서 제대로 학습되지 않게 될 것이다  </li>
</ul>
<h3 id="3-GIL"><a href="#3-GIL" class="headerlink" title="3. GIL"></a>3. GIL</h3><ul>
<li>Global Interpreter Lock</li>
<li><a href="/2022/01/28/python/GIL/" title="Global Interpritor Lock">Global Interpritor Lock</a></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-01-24T06:30:11.000Z" title="2022. 1. 24. 오후 3:30:11">2022-01-24</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-03-09T07:22:00.978Z" title="2022. 3. 9. 오후 4:22:00">2022-03-09</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">9분안에 읽기 (약 1366 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/01/24/boostcamp/week/week2/pytorch-3/">부스트 캠프 ai tech 2주 2일차 Pytorch (3)</a></h1><div class="content"><hr>
<h2 id="3-torch-nn"><a href="#3-torch-nn" class="headerlink" title="3. torch.nn"></a>3. <strong>torch.nn</strong></h2><ul>
<li>Pytorch의 Nerual Network와 관련된 기능들이 있는 모듈이다</li>
<li>Neural Network와 관련된 Layer, Function들이 속해있다<ul>
<li>Layer : 1층의 인공신경망을 이야기한다. input으로 들어온 값을 선형연산이나, 비선형연산을 통해 output을 return해 주는 class이다</li>
<li>Function : 활성화함수 등의 Neural Network의 연산을 하기위해 필요한 함수을 이야기한다  </li>
</ul>
</li>
</ul>
<h3 id="3-1-nn-Module"><a href="#3-1-nn-Module" class="headerlink" title="3.1 nn.Module"></a>3.1 <strong>nn.Module</strong></h3><ul>
<li>Custom Network(모델)를 만들기 위해서 지원하는 module이다</li>
<li>nn.Module은 내부에 Module을 포함할 수 있다<ul>
<li>여러층으로 쌓이는 모양으로 인해 Layer라고도 부른다</li>
<li>Layer가 모여서 Model을 이룬다</li>
</ul>
</li>
<li>기본적으로 아래와 같은코드를 베이스로 만들 수 있다<ul>
<li>super : nn.Module에서 Attribute를 상속받기위한 선언. 이것이 없으면 빈 깡통 클래스이다</li>
<li>forward : 순전파를 구현하는 함수  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TestNet, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x_out</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="3-2-Container"><a href="#3-2-Container" class="headerlink" title="3.2 Container"></a>3.2 <strong>Container</strong></h3><ul>
<li>Layer들을 묶어서 보관하기 위한 저장소</li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#containers">Containers  - PyTorch 공식 문서</a></li>
</ul>
<ol>
<li>nn.Sequential()<ul>
<li>여러 모듈을 하나로 묶어서 하나의 모듈처럼 사용할 수 있는 Container</li>
<li>순차적인 Layer들을 하나로 묶어서 깔끔하게 관리 할 수 있다</li>
</ul>
</li>
<li>nn.ModuleList()<ul>
<li>여러 모듈을 list처럼 한군데 모아두는 Container</li>
<li>indexing을 통해 필요한 모듈을 꺼내 쓸 수 있다</li>
<li>일반적인 List와 다르게 Attribute여도 Class를 print할 때 외부에 출력된다</li>
</ul>
</li>
<li>nn.ModuleDict()<ul>
<li>여러 모듈을 dict처럼 한군데 모아두는 Container</li>
<li>Key값을 통해 필요한 모듈을 불러올 수 있다</li>
<li>ModuleList()와 같이 Class를 print할 때 외부에 출력된다  </li>
</ul>
</li>
</ol>
<h3 id="3-3-Parameter-amp-Buffer"><a href="#3-3-Parameter-amp-Buffer" class="headerlink" title="3.3 Parameter &amp; Buffer"></a>3.3 <strong>Parameter &amp; Buffer</strong></h3><ul>
<li>Parameter<ul>
<li>모듈안에 임시로 저장되는 특별한 Tensor</li>
<li>일반적인 Tensor attribute와는 다르게 기울기 계산이 가능하고, 모델저장시에 같이 저장된다</li>
<li>RNN 같이 parameter가 반복되고, 갱신이 필요한 경우 사용된다</li>
<li>또한 모듈속의 내부모듈들의 tensor는 전부 parameter로 지정된다</li>
<li>Parameter()로 선언 할 수 있다</li>
</ul>
</li>
<li>Buffer<ul>
<li>모듈안에 임시로 저장되는 Tensor</li>
<li>모델저장시에 같이 저장된다</li>
<li>config용의 정보등을 저장할 때 사용한다</li>
<li>nn.Module의 register_buffer로 등록할 수 있다</li>
</ul>
</li>
</ul>
<h3 id="3-4-Module-내부-살펴보기"><a href="#3-4-Module-내부-살펴보기" class="headerlink" title="3.4 Module 내부 살펴보기"></a>3.4 <strong>Module 내부 살펴보기</strong></h3><ul>
<li>nn.module에는 내부의 여러 attribute를 볼 수 있는 기능이 존재한다</li>
<li>내부의 모듈, Parameter, buffer 등 여러 attribute가 ObjectDict형태로 저장되어 불러올 수 있다</li>
</ul>
<ol>
<li>submodule<ul>
<li>모듈속 모듈인 submodule은 아래의 함수들로 살펴 볼 수 있다</li>
<li>named_children<ul>
<li>module에 바로 아래단계에 속한 submodule만 보여준다</li>
</ul>
</li>
<li>named_modules<ul>
<li>submodule 뿐만아니라 module에 속해있는 모든 module을 보여준다</li>
</ul>
</li>
</ul>
</li>
<li>parameter<ul>
<li>named_parameters를 통해 parameter를 호출이 가능하다</li>
</ul>
</li>
<li>buffer<ul>
<li>named_buffers를 통해 buffer 호출이 가능하다</li>
</ul>
</li>
</ol>
<h3 id="3-5-hook"><a href="#3-5-hook" class="headerlink" title="3.5 hook"></a>3.5 <strong>hook</strong></h3><ul>
<li>package화 된 코드에서 custom 코드를 중간에 실행시킬 수 있도록 만들어 놓은 인터페이스</li>
<li>pytorch에는 등록하는 대상에 따른 2가지 종류의 hook<ul>
<li>Tensor에 등록하는 Tensor hook</li>
<li>Module에 등록하는 Module hook  </li>
</ul>
</li>
<li>실행 시점에 따른 5가지 종류의 hook이 존재한다<ul>
<li>forward pre hooks : forward 연산 전에 실행되는 hook</li>
<li>forward hooks : forward 연산 후에 실행되는 hook</li>
<li>backward_hooks : backward 연산이 수행될때 마다 실행되는 hook. 현재는 사용하는걸 권장하지 않는다</li>
<li>full backward hooks : backward 연산이 수행될때 마다 실행되는 hook</li>
<li>state dict hooks : load_state_dict 함수가 모듈 내부에서 실행하는 hook, 직접적으로 user가 잘 사용하지는 않는다</li>
</ul>
</li>
</ul>
<ol>
<li><strong>Tensor hook</strong><ul>
<li>Tensor에 대한 Backward Propagation 후에 작동하는 hook</li>
<li>torch.Tensor.register_hook 을 통하여 hook을 등록 할 수 있다</li>
<li>torch.Tensor._backward_hooks 을 통하여 등록한 hook을 확인 할 수 있다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hook</span>(<span class="params">grad</span>):</span></span><br><span class="line">  <span class="keyword">pass</span></span><br><span class="line">tensor.register_hook(hook)</span><br><span class="line">tensor_backward_hooks() <span class="comment"># OrderedDict([(0, &lt;function __main__.hook(grad)&gt;)])</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><strong>Module hook</strong><ul>
<li>Module hook은 3개의 종류의 hook으로 사용된다</li>
<li>forward pre hooks</li>
<li>forward hooks</li>
<li><del>backward_hooks</del></li>
<li>full backward hooks</li>
</ul>
</li>
</ol>
<ul>
<li>forward pre hooks<ul>
<li>forward 연산이 일어나기 전 시점에서 실행되는 hook</li>
<li>parameter로 module과 input으로 받고 input을 수정해서 return 할 수 있다</li>
<li>Module.register_forward_pre_hook(hook)으로 등록이 가능하다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">forward_pre_hook(module, <span class="built_in">input</span>) -&gt; <span class="literal">None</span> <span class="keyword">or</span> modified <span class="built_in">input</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>forward hooks<ul>
<li>forward 연산이 일어난 뒤 시점에서 실행되는 hook</li>
<li>parameter로 module, input, output으로 받고, output을 수정해서 return 할 수 있다</li>
<li>input값또한 수정이 가능하지만 forward 연산에 변화는 없다</li>
<li>Module.register_forward_hook(hook)으로 등록이 가능하다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">forward_hook(module, <span class="built_in">input</span>, output) -&gt; <span class="literal">None</span> <span class="keyword">or</span> modified output</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>full backward hooks<ul>
<li>backward 연산이 수행될때 마다 실행되는 hook</li>
<li>parameter로 module, grad_input, grad_output으로 받고, 새로운 grad_input return 할 수 있다</li>
<li>parameter인 grad_input 자체를 수정하면 Error가 발생할 수 있다</li>
<li>Module.register_full_backward_hook(hook)으로 등록이 가능하다  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">full_backward_hooks(module, grad_input, grad_output) -&gt; <span class="literal">None</span> <span class="keyword">or</span> modified grad_input</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="3-6-apply"><a href="#3-6-apply" class="headerlink" title="3.6 apply"></a>3.6 <strong>apply</strong></h3><ul>
<li>특정 함수를 Module과 Module에 속한 submodule에 적용하는 함수</li>
<li>weight 초기화나, 내부 모듈에 특정한 method를 추가할 때 사용할 수 있다</li>
<li>weight_initialization<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_initialization</span>(<span class="params">module</span>):</span></span><br><span class="line">    module_name = module.__class__.__name__</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;Function&#x27;</span> <span class="keyword">in</span> module_name:</span><br><span class="line">        module.W.data.fill_(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li>
<li>make_method<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">function_repr</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">f&#x27;name=<span class="subst">&#123;self.name&#125;</span>&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_repr</span>(<span class="params">module</span>):</span></span><br><span class="line">    module_name = module.__class__.__name__</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;Function&#x27;</span> <span class="keyword">in</span> module_name:</span><br><span class="line">        module.extra_repr = partial(function_repr, module)</span><br></pre></td></tr></table></figure></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-01-24T04:55:45.000Z" title="2022. 1. 24. 오후 1:55:45">2022-01-24</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-01-27T05:58:08.433Z" title="2022. 1. 27. 오후 2:58:08">2022-01-27</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/%EC%9D%BC%EC%83%81/">일상</a><span> / </span><a class="link-muted" href="/categories/%EC%9D%BC%EC%83%81/TIL/">TIL</a><span> / </span><a class="link-muted" href="/categories/boostcamp/Dairy/">Dairy</a></span><span class="level-item">몇 초안에 읽기 (약 91 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/01/24/boostcamp/Dairy/week2-Day1/">Week2 - Day 1 Review</a></h1><div class="content"><h2 id="오늘-하루-한-것"><a href="#오늘-하루-한-것" class="headerlink" title="오늘 하루 한 것"></a>오늘 하루 한 것</h2><ul>
<li>강의<ul>
<li>pytorch 1, 2, 3 강의</li>
</ul>
</li>
<li>과제<ul>
<li>pytorch 기본과제1 중간정도</li>
</ul>
</li>
<li>정리<ul>
<li>pytorch 1, 2, 3 강의, 기본과제1</li>
</ul>
</li>
</ul>
<h2 id="피어세션에서-한-것"><a href="#피어세션에서-한-것" class="headerlink" title="피어세션에서 한 것"></a>피어세션에서 한 것</h2><ul>
<li>backward 함수의 파라미터로 gradient를 넘기는것에 대한 의미</li>
<li>reshape, view 차이점</li>
</ul>
<h2 id="내일-할것"><a href="#내일-할것" class="headerlink" title="내일 할것"></a>내일 할것</h2><ul>
<li>기본과제 정리 끝내기</li>
</ul>
<h2 id="하루-느낀점"><a href="#하루-느낀점" class="headerlink" title="하루 느낀점"></a>하루 느낀점</h2><ul>
<li>부덕아… 눈이 너무 아파</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-01-24T01:42:40.000Z" title="2022. 1. 24. 오전 10:42:40">2022-01-24</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-01-25T06:15:01.612Z" title="2022. 1. 25. 오후 3:15:01">2022-01-25</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">8분안에 읽기 (약 1256 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/01/24/boostcamp/week/week2/pytorch-2/">부스트 캠프 ai tech 2주 1일차 Pytorch (2)</a></h1><div class="content"><hr>
<h2 id="2-유용한-torch-함수들"><a href="#2-유용한-torch-함수들" class="headerlink" title="2. 유용한 torch 함수들"></a>2. <strong>유용한 torch 함수들</strong></h2><ul>
<li>torch의 내장함수들 중 자주 쓰일만한 녀석들의 정리글이다<br><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#tensors">pytorch 공식문서 - 링크</a></li>
</ul>
<ol>
<li>Tensors</li>
<li>Creation Ops</li>
<li>indexing, Slicing, Joining, Mutating</li>
<li></li>
</ol>
<h3 id="2-1-Tensors"><a href="#2-1-Tensors" class="headerlink" title="2.1 Tensors"></a>2.1 <strong>Tensors</strong></h3><ol>
<li>is_*<ul>
<li>데이터 형태가 tensor인지 판단, tensor의 내부 데이터 등의 여러가지 판단을 하는 함수<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">0</span>])</span><br><span class="line">is_tensor(x) <span class="comment"># True</span></span><br><span class="line">is_nonzero(x) <span class="comment"># False, input : single element tensor</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>torch.numel(x)<ul>
<li>전체 element가 몇개인지 출력하는 함수  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">torch.numel(a) <span class="comment"># 120</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h3 id="2-2-Creation-Ops"><a href="#2-2-Creation-Ops" class="headerlink" title="2.2 Creation Ops"></a>2.2 <strong>Creation Ops</strong></h3><ol>
<li>torch.from_numpy<ul>
<li>ndarray를 torch.Tensor로 바꾸는 함수</li>
</ul>
</li>
<li>torch.zeros(size), empty(size), ones(size)<ul>
<li>0, 빈, 1로 이루어진 tensor를 size 형태로 생성하는 함수</li>
<li>numpy와 같은 기능을 한다</li>
</ul>
</li>
<li>torch.zeros_like(tensor), empty_like(tensor), ones_like(tensor)<ul>
<li>tensor의 size를 가진 0, 빈, 1로 이루어진 tensor를 생성하는 함수</li>
<li>numpy와 같은 기능을 한다</li>
</ul>
</li>
<li>torch.arrange(start, end, step)<ul>
<li>numpy의 arrange와 같은 기능을 하는 함수</li>
<li>start 부터 end 까지 step마다의 수를 가진 1D-tensor를 생성한다</li>
</ul>
</li>
<li>torch.linspace(start, end, steps)<ul>
<li>start에서 end의 구간의 길이를 steps개로 균등하게 나누는 1D-tensor를 생성한다</li>
</ul>
</li>
<li>torch.full(size, fill_value), torch.full_like(tensor, fill_value)<ul>
<li>fill_value로 채워진 tensor를 생성한다</li>
</ul>
</li>
</ol>
<h3 id="2-3-indexing-Slicing-Joining-Mutating-함수"><a href="#2-3-indexing-Slicing-Joining-Mutating-함수" class="headerlink" title="2.3 indexing, Slicing, Joining, Mutating 함수"></a>2.3 <strong>indexing, Slicing, Joining, Mutating 함수</strong></h3><ol>
<li>torch.index_select(input, dim, index)<ul>
<li>특정한 index에 위치한 데이터를 모아서 return 해주는 함수<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">A = torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">torch.index_select(A, <span class="number">1</span>, torch.tensor([<span class="number">0</span>]))</span><br><span class="line">===========================================</span><br><span class="line">output:</span><br><span class="line">tensor([[<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">3.</span>]])</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>torch.gather(input, dim, index)<ul>
<li>특정한 index들에 위치한 데이터를 모아서 return 해주는 함수<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">torch.gather(t, <span class="number">1</span>, torch.tensor([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>]]))</span><br><span class="line">==================================================</span><br><span class="line">output:</span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">3</span>]])</span><br><span class="line">==================================================</span><br><span class="line">index calculate:</span><br><span class="line">out[i][j][k] = <span class="built_in">input</span>[index[i][j][k]][j][k]  <span class="comment"># if dim == 0</span></span><br><span class="line">out[i][j][k] = <span class="built_in">input</span>[i][index[i][j][k]][k]  <span class="comment"># if dim == 1</span></span><br><span class="line">out[i][j][k] = <span class="built_in">input</span>[i][j][index[i][j][k]]  <span class="comment"># if dim == 2</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>torch.cat(tensors, dim) == torch.concat <ul>
<li>tensors들을 합치는 함수</li>
<li>기준이 되는 dim을 제외하고 같은 shape를 가지고 있어야한다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">torch.cat((x,y), <span class="number">0</span>).size() <span class="comment"># torch.Size([3, 3])</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>torch.chunk(input, chunks, dim)<ul>
<li>tensor를 chunk의 갯수만큼으로 분리해주는 함수</li>
<li>chunks의 갯수가 넘어가지 않는 선에서 같은 size의 tensor로 분리해준다</li>
<li>나누어 떨어지지 않는경우 마지막 tensor의 사이즈의 크기가 다를 수도 있다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.arange(<span class="number">13</span>).chunk(<span class="number">6</span>)</span><br><span class="line">=========================</span><br><span class="line">output:</span><br><span class="line">(tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]),</span><br><span class="line"> tensor([<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]),</span><br><span class="line"> tensor([<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]),</span><br><span class="line"> tensor([ <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]),</span><br><span class="line"> tensor([<span class="number">12</span>]))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                  [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(torch.chunk(t, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">===========================</span><br><span class="line">output:</span><br><span class="line">(tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>]]), tensor([[<span class="number">3</span>],</span><br><span class="line">        [<span class="number">6</span>]]))</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>torch.Tensor.scatter_(dim, index, src, reduce=None)<ul>
<li>Tensor에 index에 맞춰서 src를 삽입하는 함수이다</li>
<li>reduce에 add, multiple을 넣어서 더하거나 곱하기도로 바꿀 수 있다</li>
<li>torch.gather와 반대로 작동한다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">src = torch.arange(<span class="number">1</span>, <span class="number">11</span>).reshape((<span class="number">2</span>, <span class="number">5</span>)) </span><br><span class="line"><span class="comment"># tensor([[ 1,  2,  3,  4,  5], [ 6,  7,  8,  9, 10]])</span></span><br><span class="line">index = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>]])</span><br><span class="line">torch.zeros(<span class="number">3</span>, <span class="number">5</span>, dtype=src.dtype).scatter_(<span class="number">0</span>, index, src)</span><br><span class="line">==========================================================</span><br><span class="line">output:</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">==========================================================</span><br><span class="line">index calculate:</span><br><span class="line">self[index[i][j][k]][j][k] = src[i][j][k]  <span class="comment"># if dim == 0</span></span><br><span class="line">self[i][index[i][j][k]][k] = src[i][j][k]  <span class="comment"># if dim == 1</span></span><br><span class="line">self[i][j][index[i][j][k]] = src[i][j][k]  <span class="comment"># if dim == 2</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>torch.stack(tensors, dim)<ul>
<li>지정하는 차원으로 확장해서 tensor를 쌓아주는 함수이다</li>
<li>두 차원이 정확하게 일치해야 쌓기가 가능하다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>) <span class="comment"># 3, 1, 3</span></span><br><span class="line">y = torch.rand(<span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>) <span class="comment"># 3, 1, 3</span></span><br><span class="line">torch.stack((x,y), dim=<span class="number">2</span>).size() <span class="comment">#torch.Size([3, 1, 2, 3])</span></span><br></pre></td></tr></table></figure>
<h3 id="2-4-random-Sampling"><a href="#2-4-random-Sampling" class="headerlink" title="2.4 random Sampling"></a>2.4 <strong>random Sampling</strong></h3></li>
</ul>
</li>
</ol>
<ul>
<li>자주 쓰이지만 numpy와 비슷해서 문서를 참고하는편이 좋을듯 하다</li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#random-sampling">Random sampling - PyTorch 공식 문서</a></li>
<li>torch.seed(), torch.manual_seed(int)<ul>
<li>Seed값을 고정해서 랜덤한 변수를 고정시킬 수 있다</li>
<li>manual_seed는 직접 시드값을 입력할 수 있다</li>
</ul>
</li>
</ul>
<h3 id="2-5-Pointwise-Ops"><a href="#2-5-Pointwise-Ops" class="headerlink" title="2.5 Pointwise Ops"></a>2.5 <strong>Pointwise Ops</strong></h3><ul>
<li>수학 연산과 관련된 기능을 포함하는 함수군<ul>
<li>numpy와 비슷하다</li>
</ul>
</li>
</ul>
<ol>
<li>torch.sqrt(tensor)<ul>
<li>각 tensor의 element에 대한 제곱근을 구해주는 함수</li>
</ul>
</li>
<li>torch.exp(tensor)<ul>
<li>각 tensor의 element에 대한 $e^x$</li>
</ul>
</li>
<li>torch.pow(tensor)<ul>
<li>각 tensor의 element에 대한 $x^2$  </li>
</ul>
</li>
</ol>
<h3 id="2-6-Reduction-Ops"><a href="#2-6-Reduction-Ops" class="headerlink" title="2.6 Reduction Ops"></a>2.6 <strong>Reduction Ops</strong></h3><ul>
<li>조건에 따라 특정한 tensor의 값을 가져오는 함수군</li>
<li>대부분 numpy와 동일하게 작동한다  </li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#reduction-ops">Reduction Ops - PyTorch 공식 문서</a>  </li>
</ul>
<h3 id="2-7-Comparison-Ops"><a href="#2-7-Comparison-Ops" class="headerlink" title="2.7 Comparison Ops"></a>2.7 <strong>Comparison Ops</strong></h3><ul>
<li>비교와 관련된 기능을 포함하고 있는 함수군</li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#comparison-ops">Comparison Ops - PyTorch 공식 문서</a></li>
</ul>
<ol>
<li><p>torch.argsort(tensor)</p>
<ul>
<li>tensor를 sort하는 index를 return 해준다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randint(<span class="number">1</span>, <span class="number">10</span>, (<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">a</span><br><span class="line">torch.argsort(a)</span><br><span class="line">================</span><br><span class="line">output:</span><br><span class="line">tensor([[<span class="number">9</span>, <span class="number">5</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">4</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">8</span>, <span class="number">6</span>]])</span><br><span class="line">tensor([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>torch.eq, torch.gt, torch.ge</p>
<ul>
<li>tensor의 값들이 같은지, 더 큰지, 이상인지를 판단하는 함수들이다</li>
</ul>
</li>
<li><p>torch.allclose(input, other, trol, atol)</p>
<ul>
<li>input tensor와 other의 원소들의 차가 특정 범위인지를 판단하는 함수<br>$$<br>|\operatorname{input} - \operatorname{other}| \leq atol + rtol \times|other|<br>$$</li>
</ul>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.allclose(torch.tensor([<span class="number">10.1</span>, <span class="number">1e-9</span>]), torch.tensor([<span class="number">10.0</span>, <span class="number">1e-08</span>]))</span><br><span class="line"><span class="comment"># False</span></span><br></pre></td></tr></table></figure>

<h3 id="2-8-Other-Operations"><a href="#2-8-Other-Operations" class="headerlink" title="2.8 Other Operations"></a>2.8 <strong>Other Operations</strong></h3><ul>
<li>그 외 다양한 기능들이 모여있는 함수들</li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#other-operations">Other Operations - PyTorch 공식 문서</a></li>
</ul>
<ol>
<li>torch.einsum<ul>
<li>Einstein Notation에 따라 연산을 진행하는 함수</li>
<li>Einstein Notation은 특정 index의 집합에 대한 합연산을 간결하게 표시하는 방법이다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">5</span>)</span><br><span class="line">y = torch.randn(<span class="number">4</span>)</span><br><span class="line">torch.einsum(<span class="string">&#x27;i,j-&gt;ij&#x27;</span>, x, y)</span><br><span class="line">============================</span><br><span class="line">output:</span><br><span class="line">tensor([[ <span class="number">0.1156</span>, -<span class="number">0.2897</span>, -<span class="number">0.3918</span>,  <span class="number">0.4963</span>],</span><br><span class="line">        [-<span class="number">0.3744</span>,  <span class="number">0.9381</span>,  <span class="number">1.2685</span>, -<span class="number">1.6070</span>],</span><br><span class="line">        [ <span class="number">0.7208</span>, -<span class="number">1.8058</span>, -<span class="number">2.4419</span>,  <span class="number">3.0936</span>],</span><br><span class="line">        [ <span class="number">0.1713</span>, -<span class="number">0.4291</span>, -<span class="number">0.5802</span>,  <span class="number">0.7350</span>],</span><br><span class="line">        [ <span class="number">0.5704</span>, -<span class="number">1.4290</span>, -<span class="number">1.9323</span>,  <span class="number">2.4480</span>]])</span><br><span class="line">==============================================</span><br><span class="line">As = torch.randn(<span class="number">3</span>,<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">Bs = torch.randn(<span class="number">3</span>,<span class="number">5</span>,<span class="number">4</span>)</span><br><span class="line">torch.einsum(<span class="string">&#x27;bij,bjk-&gt;bik&#x27;</span>, As, Bs)</span><br><span class="line">====================================</span><br><span class="line">output:</span><br><span class="line">tensor([[[-<span class="number">1.0564</span>, -<span class="number">1.5904</span>,  <span class="number">3.2023</span>,  <span class="number">3.1271</span>],</span><br><span class="line">        [-<span class="number">1.6706</span>, -<span class="number">0.8097</span>, -<span class="number">0.8025</span>, -<span class="number">2.1183</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">4.2239</span>,  <span class="number">0.3107</span>, -<span class="number">0.5756</span>, -<span class="number">0.2354</span>],</span><br><span class="line">        [-<span class="number">1.4558</span>, -<span class="number">0.3460</span>,  <span class="number">1.5087</span>, -<span class="number">0.8530</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">2.8153</span>,  <span class="number">1.8787</span>, -<span class="number">4.3839</span>, -<span class="number">1.2112</span>],</span><br><span class="line">        [ <span class="number">0.3728</span>, -<span class="number">2.1131</span>,  <span class="number">0.0921</span>,  <span class="number">0.8305</span>]]])</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h3 id="2-9-BLAS-amp-LAPACK-Ops"><a href="#2-9-BLAS-amp-LAPACK-Ops" class="headerlink" title="2.9 BLAS &amp; LAPACK Ops"></a>2.9 <strong>BLAS &amp; LAPACK Ops</strong></h3><ul>
<li>“BLAS” - Basic Linear Algebra Subprograms</li>
<li>“LAPACK” - Linear Algebra PACKage</li>
<li>선형대수에 관련된 함수군이다</li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#blas-and-lapack-operations">BLAS &amp; LAPACK Ops - PyTorch 공식 문서</a></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-01-24T01:10:35.000Z" title="2022. 1. 24. 오전 10:10:35">2022-01-24</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-01-24T05:30:27.794Z" title="2022. 1. 24. 오후 2:30:27">2022-01-24</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">4분안에 읽기 (약 558 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/01/24/boostcamp/week/week2/pytorch-1/">부스트 캠프 ai tech 2주 1일차 Pytorch (1)</a></h1><div class="content"><hr>
<h2 id="0-pytorch란"><a href="#0-pytorch란" class="headerlink" title="0. pytorch란?"></a>0. pytorch란?</h2><ul>
<li>Meta(구 Facebook) 에서 개발한 딥러닝 프레임워크</li>
<li>numpy + AutoGradient</li>
<li>동적 그래프 기반</li>
</ul>
<h2 id="1-pytorch-기본"><a href="#1-pytorch-기본" class="headerlink" title="1. pytorch 기본"></a>1. pytorch 기본</h2><ul>
<li>pytorch 에서는 Tensor class를 사용한다</li>
<li>Tensor<ul>
<li>numpy의 ndarray와 사실상 동일하다<ul>
<li>내장 함수도 대부분 비슷한 기능이 존재한다</li>
</ul>
</li>
<li>tensor가 가질수 있는 type은 ndarray와 동일하나 GPU 사용이 가능한 차이가 존재한다  </li>
</ul>
</li>
</ul>
<h3 id="1-1-기본-Tensor-함수"><a href="#1-1-기본-Tensor-함수" class="headerlink" title="1.1 기본 Tensor 함수"></a>1.1 기본 Tensor 함수</h3><ul>
<li>list &gt; tensor<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">data = [[<span class="number">3</span>, <span class="number">5</span>],[<span class="number">10</span>, <span class="number">5</span>]]</span><br><span class="line">x_data = torch.tensor(data)</span><br><span class="line"><span class="comment">##########################</span></span><br><span class="line">output:</span><br><span class="line">tensor([[ <span class="number">3</span>,  <span class="number">5</span>],</span><br><span class="line">        [<span class="number">10</span>,  <span class="number">5</span>]])</span><br></pre></td></tr></table></figure></li>
<li>ndArray &gt; tensor<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nd_array_ex = np.array(data)</span><br><span class="line">tensor_array = torch.from_numpy(nd_array_ex)</span><br><span class="line"><span class="comment">############################################</span></span><br><span class="line">output:</span><br><span class="line">tensor([[ <span class="number">3</span>,  <span class="number">5</span>],</span><br><span class="line">        [<span class="number">10</span>,  <span class="number">5</span>]])</span><br></pre></td></tr></table></figure></li>
<li>tensor &gt; ndarray<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor_array.numpy()</span><br><span class="line"><span class="comment">####################</span></span><br><span class="line">output:</span><br><span class="line">array([[ <span class="number">3</span>,  <span class="number">5</span>],</span><br><span class="line">       [<span class="number">10</span>,  <span class="number">5</span>]])</span><br></pre></td></tr></table></figure></li>
<li>flatten<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data = [[<span class="number">3</span>, <span class="number">5</span>, <span class="number">20</span>],[<span class="number">10</span>, <span class="number">5</span>, <span class="number">50</span>], [<span class="number">1</span>, <span class="number">5</span>, <span class="number">10</span>]]</span><br><span class="line">x_data = torch.tensor(data)</span><br><span class="line">x_data.flatten()</span><br><span class="line"><span class="comment">################</span></span><br><span class="line">output:</span><br><span class="line">tensor([ <span class="number">3</span>,  <span class="number">5</span>, <span class="number">20</span>, <span class="number">10</span>,  <span class="number">5</span>, <span class="number">50</span>,  <span class="number">1</span>,  <span class="number">5</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure></li>
<li>one_like<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.ones_like(x_data)</span><br><span class="line"><span class="comment">#######################</span></span><br><span class="line">output:</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br></pre></td></tr></table></figure></li>
<li>shape, dtype<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_data.shape <span class="comment"># torch.Size([3, 3])</span></span><br><span class="line">x_data.dtype <span class="comment"># torch.int64</span></span><br></pre></td></tr></table></figure></li>
<li>GPU load<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="1-2-Tensor-handling"><a href="#1-2-Tensor-handling" class="headerlink" title="1.2 Tensor handling"></a>1.2 Tensor handling</h3><ul>
<li>view &amp; reshape<ul>
<li>tensor의 shape를 변경하는 함수</li>
<li>view는 input tensor와 return tensor가 데이터를 공유하여 항상 같은 주소값들을 가진다</li>
<li>reshape은 tensor의 복사본 혹은 view를 반환한다<ul>
<li>원본과 동일한 tensor값이 필요할 경우에는 view를 사용하거나 clone()을 이용해야한다</li>
</ul>
</li>
</ul>
</li>
<li>squeeze &amp; unsqueeze<ul>
<li>차원의 개수가 1인 차원을 축소, 확장하는 함수</li>
<li>unsqueeze(index) : index에 1인 차원을 삽입해서 차원을 확장한다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tensor_ex = torch.rand(size=(<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">tensor_ex.squeeze().shape <span class="comment"># torch.Size([2, 2])</span></span><br><span class="line">tensor_ex = torch.rand(size=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">tensor_ex.unsqueeze(<span class="number">0</span>).shape <span class="comment"># torch.Size([1, 2, 2])</span></span><br><span class="line">tensor_ex = torch.rand(size=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">tensor_ex.unsqueeze(<span class="number">1</span>).shape <span class="comment"># torch.Size([2, 1, 2])</span></span><br><span class="line">tensor_ex = torch.rand(size=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">tensor_ex.unsqueeze(<span class="number">2</span>).shape <span class="comment"># torch.Size([2, 2, 1])</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="1-3-Tensor-operation"><a href="#1-3-Tensor-operation" class="headerlink" title="1.3 Tensor operation"></a>1.3 Tensor operation</h3><ul>
<li>numpy와 동일하게 operation에 대해서 broadcasting을 지원한다</li>
<li>행렬곱셈 연산은 mm 및 matmul을 사용한다<ul>
<li>dot은 1차원 벡터와 스칼라 연산에서만 사용가능</li>
<li>mm과 matmul은 2차원이상의 행렬연산에서만 사용가능</li>
<li>mm은 broadcasting을 지원하지 않지만 matmul은 지원한다</li>
</ul>
</li>
</ul>
<h3 id="1-4-Tensor-operation-for-ML-DL-formula"><a href="#1-4-Tensor-operation-for-ML-DL-formula" class="headerlink" title="1.4 Tensor operation for ML/DL formula"></a>1.4 Tensor operation for ML/DL formula</h3><ul>
<li>nn.functional을 이용한 다양한 연산가능</li>
<li>softmax, argmax, one_hot 등등</li>
</ul>
<h3 id="1-5-AutoGrad"><a href="#1-5-AutoGrad" class="headerlink" title="1.5 AutoGrad"></a>1.5 AutoGrad</h3><ul>
<li>자동 미분</li>
<li>tensor에 requires_grad=True로 설정해서 자동으로 gradient 추적이 가능하다<ul>
<li>기본적으로 nn모듈의 선형연산들은 default로 True로 설정되어있어 잘 쓰지 않는다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(data, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>backward() 함수를 통하여 Backpropagation 수행</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-01-21T03:22:13.000Z" title="2022. 1. 21. 오후 12:22:13">2022-01-21</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-01-28T05:21:35.710Z" title="2022. 1. 28. 오후 2:21:35">2022-01-28</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/Problems/">Problems</a></span><span class="level-item">3분안에 읽기 (약 452 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/01/21/boostcamp/Problems/work1/">Week1 Homework</a></h1><div class="content"><hr>
<h2 id="1-기초과제"><a href="#1-기초과제" class="headerlink" title="1. 기초과제"></a>1. 기초과제</h2><ul>
<li>과제 내용<ul>
<li>간단한 python의 built-in function 과 문자열 처리를 위한 과제</li>
</ul>
</li>
<li>결과<ul>
<li>정규식 라이브러리 <code>re</code>를 사용하여 문자열을 처리해주었다</li>
</ul>
</li>
<li>회고<ul>
<li>기초과제 + 첫주라 그런지 난이도는 어렵지 않았다</li>
<li>문제표현상 애매한 점이있었지만, 조교님과의 소통으로 해결함</li>
</ul>
</li>
</ul>
<h2 id="2-심화과제"><a href="#2-심화과제" class="headerlink" title="2. 심화과제"></a>2. 심화과제</h2><ul>
<li>여기서부터 진정한 과제</li>
</ul>
<h3 id="1-경사하강법-구현"><a href="#1-경사하강법-구현" class="headerlink" title="1. 경사하강법 구현"></a>1. 경사하강법 구현</h3><ul>
<li>말 그대로 경사하강법의 구현</li>
<li>차근차근 수식을 코드로 바꾸면 되기때문에 어려운부분은 없었다</li>
</ul>
<h3 id="2-BPTT-구현"><a href="#2-BPTT-구현" class="headerlink" title="2. BPTT 구현"></a>2. BPTT 구현</h3><ul>
<li>과정<ul>
<li>RNN의 Backpropagation은 처음 구현하는거라 시행착오가 많았다</li>
</ul>
</li>
<li>numpy를 거의 사용 안하고 문제를 풀었는데 조금 연습을 할 필요가 있어보인다</li>
</ul>
<h3 id="3-최대가능도-계산-증명-및-구현"><a href="#3-최대가능도-계산-증명-및-구현" class="headerlink" title="3. 최대가능도 계산 증명 및 구현"></a>3. 최대가능도 계산 증명 및 구현</h3><ul>
<li>가우시안 분포를 통한 최대가능도 계산</li>
</ul>
<blockquote>
<ul>
<li>로그우도함수<br>$$<br>\begin{aligned}<br>L(\theta|x) &amp;= \sum_{i=1}^{n}\left( -\frac{1}{2}\log2\pi\sigma^2 +\log \exp\left(-\frac{(x_i-\mu)^2}{2\sigma^2}\right) \right)\\<br>&amp;= -\frac{n}{2}\log2\pi\sigma^2 - \sum_{i=1}^{n}\frac{(x_i-\mu)^2}{2\sigma^2}\\<br>&amp;= -\frac{n}{2}\log2\pi\sigma^2 - \frac{1}{2\sigma^2}\sum_{i=1}^{n}(x_i-\mu)^2<br>\end{aligned}<br>$$</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>모평균의 추정량<br>$$<br>\begin{aligned}<br>\frac{\partial L(\theta|x)}{\partial \mu}&amp;= -\frac{1}{2\sigma^2}\sum_{i=1}^{n}\frac{\partial}{\partial \mu}\left(x_i^2-2x_i\mu+\mu^2\right)\\<br>&amp;= -\frac{1}{2\sigma^2}\sum_{i=1}^{n}(-2x_i + 2\mu)\\<br>&amp;= \frac{X-n\mu}{\sigma^2}\\<br>\hat{\mu} &amp;= \frac{1}{n}X\\<br>&amp;= \frac{1}{n}\sum_{i=1}^{n}x_i<br>\end{aligned}<br>$$</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>모분산의 추정량<br>$$\frac{\partial L(\theta|x)}{\partial \sigma}  = -\frac{n}{\sigma} + \frac{1}{\sigma^3}\sum_{i=1}^{n}(x_i-\mu)^2$$<br>$$\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^{n}(x_i-\mu)^2$$</li>
</ul>
</blockquote>
<h3 id="후기"><a href="#후기" class="headerlink" title="후기"></a>후기</h3><ul>
<li>우린 앞으로 편미분과 평생을 같이 살아야한다고</li>
<li>역시 스스로 힘으로 문제를 해결하면 기분이 좋다</li>
<li>다음 심화과제는 어떨지 궁금하다</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-01-21T02:38:02.000Z" title="2022. 1. 21. 오전 11:38:02">2022-01-21</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-01-26T06:21:34.975Z" title="2022. 1. 26. 오후 3:21:34">2022-01-26</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/%EC%9D%BC%EC%83%81/">일상</a><span> / </span><a class="link-muted" href="/categories/%EC%9D%BC%EC%83%81/TIL/">TIL</a><span> / </span><a class="link-muted" href="/categories/boostcamp/Dairy/">Dairy</a></span><span class="level-item">몇 초안에 읽기 (약 95 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/01/21/boostcamp/Dairy/Day5/">Week1 - Day 5 Review</a></h1><div class="content"><hr>
<h2 id="오늘-하루-한-것"><a href="#오늘-하루-한-것" class="headerlink" title="오늘 하루 한 것"></a>오늘 하루 한 것</h2><ul>
<li>정리<ul>
<li>RNN</li>
<li>딥러닝 기초</li>
</ul>
</li>
</ul>
<h2 id="피어세션에서-한-것"><a href="#피어세션에서-한-것" class="headerlink" title="피어세션에서 한 것"></a>피어세션에서 한 것</h2><ul>
<li>무어 펜로즈 역행렬 간단하게 설명하기</li>
<li>일주일 회고</li>
</ul>
<h2 id="주말에-할-것"><a href="#주말에-할-것" class="headerlink" title="주말에 할 것"></a>주말에 할 것</h2><ul>
<li>추가 정리<ul>
<li>정규식</li>
<li>pickles</li>
<li>__init__.py, __main__.py</li>
<li>numpy, pandas 정리</li>
<li>KL diverence</li>
<li>여러가지 활성함수</li>
</ul>
</li>
</ul>
<h2 id="하루-느낀점"><a href="#하루-느낀점" class="headerlink" title="하루 느낀점"></a>하루 느낀점</h2><ul>
<li>마지막 날이지만 주말에 정리할게 많다.</li>
</ul>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/categories/boostcamp/page/6/">이전</a></div><div class="pagination-next"><a href="/categories/boostcamp/page/8/">다음</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/categories/boostcamp/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/categories/boostcamp/page/6/">6</a></li><li><a class="pagination-link is-current" href="/categories/boostcamp/page/7/">7</a></li><li><a class="pagination-link" href="/categories/boostcamp/page/8/">8</a></li><li><a class="pagination-link" href="/categories/boostcamp/page/9/">9</a></li><li><a class="pagination-link" href="/categories/boostcamp/page/10/">10</a></li></ul></nav></div><div class="column column-left is-3-tablet is-3-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="KyuBum Shin"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">KyuBum Shin</p><p class="is-size-6 is-block">Student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Incheon</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">포스트</p><a href="/archives"><p class="title">123</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">카테고리</p><a href="/categories"><p class="title">26</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">태그</p><a href="/tags"><p class="title">47</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/KyubumShin" target="_blank" rel="noopener">팔로우</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/KyubumShin"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">링크</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/KyubumShin" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">카테고리</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Computer-Science/"><span class="level-start"><span class="level-item">Computer Science</span></span><span class="level-end"><span class="level-item tag">8</span></span></a><ul><li><a class="level is-mobile" href="/categories/Computer-Science/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Computer-Science/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Computer-Science/OS/"><span class="level-start"><span class="level-item">OS</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Computer-Science/%EC%9E%90%EB%A3%8C%EA%B5%AC%EC%A1%B0/"><span class="level-start"><span class="level-item">자료구조</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/DeepLearning/"><span class="level-start"><span class="level-item">DeepLearning</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/DeepLearning/Basic/"><span class="level-start"><span class="level-item">Basic</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/DeepLearning/Pytorch-Lightning/"><span class="level-start"><span class="level-item">Pytorch Lightning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Paper/"><span class="level-start"><span class="level-item">Paper</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Paper/CV/"><span class="level-start"><span class="level-item">CV</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Paper/CV/OB/"><span class="level-start"><span class="level-item">OB</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Paper/CV/Transformer/"><span class="level-start"><span class="level-item">Transformer</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/Programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Programming/Docker/"><span class="level-start"><span class="level-item">Docker</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/Programming/Python/tip/"><span class="level-start"><span class="level-item">tip</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/boostcamp/"><span class="level-start"><span class="level-item">boostcamp</span></span><span class="level-end"><span class="level-item tag">91</span></span></a><ul><li><a class="level is-mobile" href="/categories/boostcamp/Dairy/"><span class="level-start"><span class="level-item">Dairy</span></span><span class="level-end"><span class="level-item tag">33</span></span></a></li><li><a class="level is-mobile" href="/categories/boostcamp/Peer-Session/"><span class="level-start"><span class="level-item">Peer Session</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/boostcamp/Problems/"><span class="level-start"><span class="level-item">Problems</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/boostcamp/week/"><span class="level-start"><span class="level-item">week</span></span><span class="level-end"><span class="level-item tag">54</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%B0%B1%EC%97%94%EB%93%9C/"><span class="level-start"><span class="level-item">백엔드</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%B0%B1%EC%97%94%EB%93%9C/%EB%A9%94%EC%84%B8%EC%A7%80%ED%81%90/"><span class="level-start"><span class="level-item">메세지큐</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%9D%BC%EC%83%81/"><span class="level-start"><span class="level-item">일상</span></span><span class="level-end"><span class="level-item tag">47</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%9D%BC%EC%83%81/TIL/"><span class="level-start"><span class="level-item">TIL</span></span><span class="level-end"><span class="level-item tag">46</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9D%BC%EC%83%81/%EA%B3%84%ED%9A%8D/"><span class="level-start"><span class="level-item">계획</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">최근 글</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-13T13:05:09.000Z">2022-04-13</time></p><p class="title"><a href="/2022/04/13/boostcamp/Dairy/Week13-Day-1-Review/">Week12 - Day 1~3 Review</a></p><p class="categories"><a href="/categories/boostcamp/">boostcamp</a> / <a href="/categories/%EC%9D%BC%EC%83%81/">일상</a> / <a href="/categories/%EC%9D%BC%EC%83%81/TIL/">TIL</a> / <a href="/categories/boostcamp/Dairy/">Dairy</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-07T13:28:54.000Z">2022-04-07</time></p><p class="title"><a href="/2022/04/07/boostcamp/Dairy/Week12-Day-4-Review/">Week12 - Day 4 Review</a></p><p class="categories"><a href="/categories/boostcamp/">boostcamp</a> / <a href="/categories/%EC%9D%BC%EC%83%81/">일상</a> / <a href="/categories/%EC%9D%BC%EC%83%81/TIL/">TIL</a> / <a href="/categories/boostcamp/Dairy/">Dairy</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-06T14:41:01.000Z">2022-04-06</time></p><p class="title"><a href="/2022/04/06/boostcamp/Dairy/Week12-Day-3-Review/">Week12 - Day 3 Review</a></p><p class="categories"><a href="/categories/boostcamp/">boostcamp</a> / <a href="/categories/%EC%9D%BC%EC%83%81/">일상</a> / <a href="/categories/%EC%9D%BC%EC%83%81/TIL/">TIL</a> / <a href="/categories/boostcamp/Dairy/">Dairy</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-05T13:27:58.000Z">2022-04-05</time></p><p class="title"><a href="/2022/04/05/boostcamp/Dairy/Week12-Day-1-2-Review/">Week12 - Day 1~2 Review</a></p><p class="categories"><a href="/categories/boostcamp/">boostcamp</a> / <a href="/categories/%EC%9D%BC%EC%83%81/">일상</a> / <a href="/categories/%EC%9D%BC%EC%83%81/TIL/">TIL</a> / <a href="/categories/boostcamp/Dairy/">Dairy</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-04T02:30:12.000Z">2022-04-04</time></p><p class="title"><a href="/2022/04/04/paper/ViT/">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></p><p class="categories"><a href="/categories/Paper/">Paper</a> / <a href="/categories/Paper/CV/">CV</a> / <a href="/categories/Paper/CV/Transformer/">Transformer</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">아카이브</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">4월 2022</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/03/"><span class="level-start"><span class="level-item">3월 2022</span></span><span class="level-end"><span class="level-item tag">21</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/02/"><span class="level-start"><span class="level-item">2월 2022</span></span><span class="level-end"><span class="level-item tag">34</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">1월 2022</span></span><span class="level-end"><span class="level-item tag">39</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/10/"><span class="level-start"><span class="level-item">10월 2021</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/09/"><span class="level-start"><span class="level-item">9월 2021</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/08/"><span class="level-start"><span class="level-item">8월 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">태그</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/3D/"><span class="tag">3D</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Ai-Math/"><span class="tag">Ai Math</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Ai-serving/"><span class="tag">Ai serving</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CNN/"><span class="tag">CNN</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CNN-Viz/"><span class="tag">CNN Viz</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CV/"><span class="tag">CV</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-visualization/"><span class="tag">Data visualization</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DeepLearning/"><span class="tag">DeepLearning</span><span class="tag">24</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docker/"><span class="tag">Docker</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Function/"><span class="tag">Function</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN/"><span class="tag">GAN</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Generative-Model/"><span class="tag">Generative Model</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Instance-Segmentation/"><span class="tag">Instance Segmentation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LandMark-Detection/"><span class="tag">LandMark Detection</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Multi-modal/"><span class="tag">Multi-modal</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Object-Detection/"><span class="tag">Object Detection</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Project-product/"><span class="tag">Project product</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pytorch-Lightning/"><span class="tag">Pytorch Lightning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Semantic-Segmentation/"><span class="tag">Semantic Segmentation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Sequencial-Model/"><span class="tag">Sequencial Model</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Transformer/"><span class="tag">Transformer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VAE/"><span class="tag">VAE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/backend/"><span class="tag">backend</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cs/"><span class="tag">cs</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/database/"><span class="tag">database</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/message/"><span class="tag">message</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/network/"><span class="tag">network</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/os/"><span class="tag">os</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">11</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pytorch/"><span class="tag">pytorch</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week1/"><span class="tag">week1</span><span class="tag">19</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week10/"><span class="tag">week10</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week11/"><span class="tag">week11</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week12/"><span class="tag">week12</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week13/"><span class="tag">week13</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week2/"><span class="tag">week2</span><span class="tag">15</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week3/"><span class="tag">week3</span><span class="tag">11</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week4/"><span class="tag">week4</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week5/"><span class="tag">week5</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week6/"><span class="tag">week6</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week8/"><span class="tag">week8</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week9/"><span class="tag">week9</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%9D%BC%EC%83%81/"><span class="tag">일상</span><span class="tag">46</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%9E%90%EB%A3%8C%EA%B5%AC%EC%A1%B0/"><span class="tag">자료구조</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%95%A0%EC%9D%BC/"><span class="tag">할일</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">업데이트 소식 받기</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="구독"></div></div></form></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="구독"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="KyuBum&#039;s Dev Blog" height="28"></a><p class="is-size-7"><span>&copy; 2022 KyuBum Shin</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ko");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="맨 위로" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "이 웹 사이트는 귀하의 경험을 향상시키기 위해 Cookie를 사용합니다.",
          dismiss: "무시",
          allow: "허용",
          deny: "거부",
          link: "더 알아보기",
          policy: "Cookie 정책",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="입력 하세요..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"입력 하세요...","untitled":"(제목 없음)","posts":"포스트","pages":"페이지","categories":"카테고리","tags":"태그"});
        });</script></body></html>