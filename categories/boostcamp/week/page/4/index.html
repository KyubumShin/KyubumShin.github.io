<!doctype html>
<html lang="ko"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>카테고리: week - KyuBum&#039;s Dev Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="KyuBum Shin"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="KyuBum Shin"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="KyuBum&#039;s Dev Blog"><meta property="og:url" content="https://kyubumshin.github.io/"><meta property="og:site_name" content="KyuBum&#039;s Dev Blog"><meta property="og:locale" content="ko_KR"><meta property="og:image" content="https://kyubumshin.github.io/img/og_image.png"><meta property="article:author" content="KyuBum Shin"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://kyubumshin.github.io"},"headline":"KyuBum's Dev Blog","image":["https://kyubumshin.github.io/img/og_image.png"],"author":{"@type":"Person","name":"KyuBum Shin"},"publisher":{"@type":"Organization","name":"KyuBum's Dev Blog","logo":{"@type":"ImageObject","url":"https://kyubumshin.github.io/img/logo.svg"}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.0.2"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="KyuBum&#039;s Dev Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="검색" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-9-tablet is-9-desktop is-9-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">카테고리</a></li><li><a href="/categories/boostcamp/">boostcamp</a></li><li class="is-active"><a href="#" aria-current="page">week</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-01-27T06:34:38.000Z" title="2022. 1. 27. 오후 3:34:38">2022-01-27</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-01-28T04:41:52.854Z" title="2022. 1. 28. 오후 1:41:52">2022-01-28</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">1분안에 읽기 (약 182 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/01/27/boostcamp/week/week2/pytorch-10/">부스트 캠프 ai tech 2주 4일차 Pytorch (8)</a></h1><div class="content"><hr>
<h3 id="Pytorch-Troubleshooting"><a href="#Pytorch-Troubleshooting" class="headerlink" title="Pytorch Troubleshooting"></a>Pytorch Troubleshooting</h3><ul>
<li>OOM : Out Of Memory<ul>
<li>GPU의 메모리가 터질때 발생하는 현상…</li>
<li>왜 발생했는지 알기힘듬</li>
<li>메모리의 이전상황의 파악이 어려움</li>
</ul>
</li>
<li>OOM의 해결방법<ul>
<li>보통 이 아래방법으로 대부분 해결된다</li>
<li>Batchsize를 줄여서 메모리 부하를 줄인다</li>
<li>torch.cuda.empty_cache()를 이용하여 GPU의 메모리를 clear 한 뒤에 학습시킨다</li>
</ul>
</li>
<li>그 외에 신경쓰면 좋을점<ul>
<li>GPUtil Module 사용하기</li>
<li>tensor.no_grad() 사용하기</li>
<li>적절하게 del 명령어 사용하기</li>
<li>다양한 batchsize로 돌려서 가능한 batchsize 알아보기</li>
<li>tensor의 float 32를 float 16으로 줄여보기</li>
</ul>
</li>
</ul>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a target="_blank" rel="noopener" href="https://boostcamp.connect.or.kr/program_ai.html">Naver Connect Boostcamp - ai tech</a></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-01-27T04:36:25.000Z" title="2022. 1. 27. 오후 1:36:25">2022-01-27</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-01-28T04:42:57.448Z" title="2022. 1. 28. 오후 1:42:57">2022-01-28</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">3분안에 읽기 (약 407 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/01/27/boostcamp/week/week2/pytorch-9/">부스트 캠프 ai tech 2주 4일차 Pytorch (7)</a></h1><div class="content"><hr>
<h2 id="8-Hyperparameter-Tuning"><a href="#8-Hyperparameter-Tuning" class="headerlink" title="8. Hyperparameter Tuning"></a>8. Hyperparameter Tuning</h2><ul>
<li>Hyperparameter 란?<ul>
<li>Learning Rate, Model의 inputsize, optimizer, loss function, batchsize 등의 모델이 스스로 학습하지 않는 값을 말한다</li>
<li>이 Hyperparameter를 조절하여 성능을 올리는 방법을 Hyperparameter Tuning이라고 부른다</li>
<li>생각보다 스펙타클하게 성능이 좋아지지는 않는다</li>
</ul>
</li>
</ul>
<h3 id="8-1-Gradient-Search"><a href="#8-1-Gradient-Search" class="headerlink" title="8.1 Gradient Search"></a>8.1 Gradient Search</h3><ul>
<li>parameter에 따른 기울기값을 계산한뒤 큰값을 내는(빠르게 학습이 가능한) parameter를 찾는 기법</li>
<li>보통 Learning rate를 찾는데 사용하는 기법이다</li>
<li>특정 간격마다의 값으로 검색하는 Grid Layout과 랜덤한 값으로 검색하는 Random Layout 등 여러가지 방법이 존재한다</li>
<li>최근에는 베이지안 기반의 기법들이 주도하고 있다<ul>
<li>BOHB(Baysian Optimizer Hyperband)</li>
</ul>
</li>
</ul>
<h3 id="8-2-Ray-라이브러리"><a href="#8-2-Ray-라이브러리" class="headerlink" title="8.2 Ray 라이브러리"></a>8.2 Ray 라이브러리</h3><ul>
<li>ML과 DL의 병렬 처리를 위해 개발된 모듈이다</li>
<li>Hyperparameter Search를 위한 다양한 모듈을 제공한다</li>
<li>ML과 DL을 위해 개발되긴 했는데 분산처리(Multiprocessing)코드를 단순하고, 범용적으로 작성할 수 있게 도와준다</li>
<li>병렬처리 양식으로 학습을 시행해서 성능이 좋지않은 process들을 제외해 가면서 최적의 hyperparameter를 찾는다</li>
<li>아래쪽의 참고 문서를 보는것을 추천한다</li>
</ul>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a target="_blank" rel="noopener" href="https://boostcamp.connect.or.kr/program_ai.html">Naver Connect Boostcamp - ai tech</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/automl/HpBandSter">Baysian Optimizer Hyperband</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1807.01774.pdf">BOHB: Robust and Efficient Hyperparameter Optimization at Scale
</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ray-project/ray">Ray - github</a></li>
<li><a target="_blank" rel="noopener" href="https://zzsza.github.io/mlops/2021/01/03/python-ray/">Python Ray 사용법 - Python 병렬처리, 분산처리</a></li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html">HYPERPARAMETER TUNING WITH RAY TUNE</a></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-01-27T00:51:53.000Z" title="2022. 1. 27. 오전 9:51:53">2022-01-27</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-01-28T04:41:54.410Z" title="2022. 1. 28. 오후 1:41:54">2022-01-28</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">6분안에 읽기 (약 943 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/01/27/boostcamp/week/week2/pytorch-8/">부스트 캠프 ai tech 2주 4일차 Pytorch (6)</a></h1><div class="content"><hr>
<h2 id="7-Multi-GPU-학습"><a href="#7-Multi-GPU-학습" class="headerlink" title="7. Multi GPU 학습"></a>7. <strong>Multi GPU 학습</strong></h2><ul>
<li>데이터의 양이 방대해짐에 따라서 모든 데이터들을 전부 메모리에 올리는것이 물리적으로 힘들고, 시간적으로도 소요가 많이 되어서 이를 해결하기 위해 여러대의 GPU를 병렬적으로 사용하는 방법이다</li>
<li>크게 2가지의 방법으로 나뉜다<ul>
<li>Model 병렬화 : 모델을 나눠서 학습한다</li>
<li>Data 병렬화 : 데이터를 나눠서 학습한다  </li>
</ul>
</li>
</ul>
<h3 id="7-1-Model-병렬화"><a href="#7-1-Model-병렬화" class="headerlink" title="7.1 Model 병렬화"></a>7.1 Model 병렬화</h3><ul>
<li><p>모델을 나눠서 여러대의 GPU에 올려서 연산하는 방법</p>
</li>
<li><p>모델의 크기가 너무 커서 GPU에 올라가지 않는 상황에서 사용한다</p>
</li>
<li><p>모델의 병목화, 파이프라인 구축 등의 문제로 인해 구현 난이도가 높다</p>
<ul>
<li>pipeline을 제대로 구축하지 않으면 한 GPU가 연산하는동안 다른 GPU가 놀고있는 상황이 발생해 오히려 Single GPU보다 못한 상황이 발생 할 수 있다</li>
</ul>
</li>
<li><p>Model Pipeline Paralle 예시 - Pytorch 공식 모델 병렬화 Tutorial</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PipelineParallelResNet50</span>(<span class="params">ModelParallelResNet50</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, split_size=<span class="number">20</span>, *args, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PipelineParallelResNet50, self).__init__(*args, **kwargs)</span><br><span class="line">        self.split_size = split_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        splits = <span class="built_in">iter</span>(x.split(self.split_size, dim=<span class="number">0</span>))</span><br><span class="line">        s_next = <span class="built_in">next</span>(splits)</span><br><span class="line">        s_prev = self.seq1(s_next).to(<span class="string">&#x27;cuda:1&#x27;</span>)</span><br><span class="line">        ret = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> s_next <span class="keyword">in</span> splits:</span><br><span class="line">            <span class="comment"># A. s_prev는 두 번째 GPU에서 실행됩니다.</span></span><br><span class="line">            s_prev = self.seq2(s_prev)</span><br><span class="line">            ret.append(self.fc(s_prev.view(s_prev.size(<span class="number">0</span>), -<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># B. s_next는 A.와 동시에 진행되면서 첫 번째 GPU에서 실행됩니다.</span></span><br><span class="line">            s_prev = self.seq1(s_next).to(<span class="string">&#x27;cuda:1&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        s_prev = self.seq2(s_prev)</span><br><span class="line">        ret.append(self.fc(s_prev.view(s_prev.size(<span class="number">0</span>), -<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> torch.cat(ret)</span><br></pre></td></tr></table></figure></li>
<li><p>아래의 그림에서 2장의 레이어 사이에 교차하는 부분이 병렬 GPU간의 교차 통신이다</p>
<ul>
<li>Model 병렬화는 꽤 예전 논문인 AlexNet에서도 사용되고 있었다</li>
</ul>
</li>
</ul>
<center>

<p><img src="/img/Alexnet.png" alt="Alexnet"></p>
</center>

<h3 id="7-2-Data-병렬화"><a href="#7-2-Data-병렬화" class="headerlink" title="7.2 Data 병렬화"></a>7.2 Data 병렬화</h3><ul>
<li>데이터를 나눠서 GPU에 할당한 후 결과의 평균을 취하는 방법</li>
<li>각 데이터에 대한 연산을 여러 GPU에서 동시에 수행해서 학습의 효율을 높인다<ul>
<li>합칠때 </li>
</ul>
</li>
<li>pytorch에서는 두 가지 방식을 제공한다<ul>
<li>DataParallel : 단순하게 데이터를 분배한 뒤 평균을 취하는 방식</li>
<li>Distributed DataParallel : GPU에서 모든 연산이 끝난뒤에 결과만을 공유하는 방식</li>
</ul>
</li>
</ul>
<ol>
<li>DataParallel:<ul>
<li>단순하게 GPU에 데이터를 분배한 뒤 평균을 취하는 방식</li>
<li>연산이 끝난뒤에 하나의 GPU에 loss값을 모아서 gradient를 만들고 이것들 다시 다른 GPU에 전달을 해준다</li>
<li>하나의 GPU가 특별하게 자원을 많이 사용하는 문제가 발생할 수 있다<ul>
<li>합쳐진 loss연산을 GPU는 더 많은 메모리를 사용하기 때문에 Batch사이즈의 감소등의 방법으로 메모리 부하를 줄여야 할 수도 있다</li>
</ul>
</li>
<li>매우 간단하게 pytorch에서 구현 할 수 있다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parallel_model = torch.nn.DataParallel(model) <span class="comment"># 나머지는 일반 모델과 동일</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>Distributed DataParallel:<ul>
<li>GPU에서 모든 연산이 끝난뒤에 결과만을 공유하는 방식</li>
<li>Loss, Backward 계산 모두 각각의 GPU에서 이루어지며 연산이 완전히 끝나고 평균값이 결과로 출력된다</li>
<li>개별 GPU마다 CPU에서 프로세스를 생성해서 할당한다 (Multiprocessing)</li>
<li>DataParallel과는 다르게 <code>DataLoader</code>에서 Shuffle 대신에 DistributeSampler를 사용하고, pin_memory를 활성화 시킨다</li>
</ul>
</li>
</ol>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a><strong>reference</strong></h3><ul>
<li><a target="_blank" rel="noopener" href="https://boostcamp.connect.or.kr/program_ai.html">Naver Connect Boostcamp - ai tech</a></li>
<li><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">Alexnet 논문 - ImageNet Classification with Deep Convolutional Neural Networks</a>  </li>
<li><a target="_blank" rel="noopener" href="https://tutorials.pytorch.kr/intermediate/model_parallel_tutorial.html">Pytorch 공식 모델 병렬화 Tutorial</a></li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">Pytorch Distributed Data Parallel Tutorial</a></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-01-26T05:50:44.000Z" title="2022. 1. 26. 오후 2:50:44">2022-01-26</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-01-28T04:45:40.245Z" title="2022. 1. 28. 오후 1:45:40">2022-01-28</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">5분안에 읽기 (약 736 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/01/26/boostcamp/week/week2/pytorch-6/">부스트 캠프 ai tech 2주 3일차 Pytorch (5)</a></h1><div class="content"><hr>
<ul>
<li>학습시킨 모델을 다른사람들에게 공유하거나, 보관하기 위에서는 메모리에 있는 Model들을 따로 파일로 만들어서 저장할 필요가 있는데 본 글에서는 저장을 어떻게 해야하는지, 그리고 이를 이용한 Tranfer Learning 에 대해서 다룰 예정이다  </li>
</ul>
<h2 id="5-Pytorch-Model-Save-amp-Load"><a href="#5-Pytorch-Model-Save-amp-Load" class="headerlink" title="5. Pytorch Model Save &amp; Load"></a>5. Pytorch Model Save &amp; Load</h2><ul>
<li>torch.save()<ul>
<li>학습의 결과를 저장하기 위한 함수이다</li>
<li>모델의 Layer들과 Parameter, Buffer를 저장한다</li>
<li>학습 중간중간 Model을 저장해서 최선의 성능을 가지는 결과모델을 선택하는 방식으로 사용 할 수 있다 (Checkpoint)<blockquote>
<ul>
<li>model : 학습한 모델</li>
<li>PATH : 모델을 저장할 directory</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모델의 weight 만 저장하는 방법</span></span><br><span class="line">torch.save(model.state_dict(), PATH)</span><br><span class="line"><span class="comment"># 모델의 weight와 내부모듈 구조, Buffer까지 저장하는 방법</span></span><br><span class="line">torch.save(model, PATH)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>checkpoints<ul>
<li>학습의 중간 결과를 저장해서 최선의 성능을 가지는 결과모델을 선택하는 방법</li>
<li>보통 early stopping 기법과 함께 사용한다<ul>
<li>early stopping : Loss와 Metric값을 지속적으로 확인 하면서 일정 기간이상 줄지 않으면 학습을 멈추는 방법</li>
</ul>
</li>
<li>일반적으로 epoch, loss, mertic을 함께 저장하여 확인한다  </li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.save(&#123;</span><br><span class="line">    <span class="string">&#x27;epoch&#x27;</span>: epoch,</span><br><span class="line">    <span class="string">&#x27;model_state_dict&#x27;</span>: model.state_dict(),</span><br><span class="line">    <span class="string">&#x27;optimizer_state_dict&#x27;</span>: optimizer.state_dict(),</span><br><span class="line">    <span class="string">&#x27;loss&#x27;</span>: epoch_loss,</span><br><span class="line">    &#125;, <span class="string">f&quot;saved/checkpoint_model_<span class="subst">&#123;epoch&#125;</span>_<span class="subst">&#123;epoch_loss/<span class="built_in">len</span>(dataloader)&#125;</span>_<span class="subst">&#123;epoch_acc/<span class="built_in">len</span>(dataloader)&#125;</span>.pt&quot;</span>)</span><br><span class="line"></span><br><span class="line">checkpoint = torch.load(PATH)</span><br><span class="line">model.load_state_dict(checkpoint[<span class="string">&#x27;model_state_dict&#x27;</span>])</span><br><span class="line">optimizer.load_state_dict(checkpoint[<span class="string">&#x27;optimizer_state_dict&#x27;</span>])</span><br><span class="line">epoch = checkpoint[<span class="string">&#x27;epoch&#x27;</span>]</span><br><span class="line">loss = checkpoint[<span class="string">&#x27;loss&#x27;</span>]</span><br></pre></td></tr></table></figure>

<h2 id="6-Transfer-Learning"><a href="#6-Transfer-Learning" class="headerlink" title="6. Transfer Learning"></a>6. Transfer Learning</h2><ul>
<li>다른 데이터셋으로 만든 모델을 현재 데이터셋에 맞춰서 다시 학습시키는 방법<ul>
<li>일반적으로 큰 데이터셋으로 학습시킨 모델(ex Imagenet 10K로 학습시킨 resnet50 등등)의 성능이 다른 데이터셋에 적용시키는것이 처음부터 학습하는 모델보다 학습이 빠르고, 학습이 잘된다</li>
</ul>
</li>
<li>현재 DeepLearning에서 가장 일반적인 학습 방법이다</li>
<li>기존의 pretrained 된 모델을 backbone 모델이라고 하며 여기서 일부 Layer만 변경시켜서 학습을 수행한다</li>
<li>CV : Pytorch 공식 비전 라이브러리 TorchVision 이나 torch image model(timm)을 많이 이용한다</li>
<li>NLP : transformer 전문 라이브러리인 HuggingFace를 많이 사용한다</li>
</ul>
<h3 id="6-1-Freezing"><a href="#6-1-Freezing" class="headerlink" title="6.1 Freezing"></a>6.1 <strong>Freezing</strong></h3><ul>
<li>pretrained model을 활용할때 모델의 일부분을 freeze 시켜 파라미터의 업데이트가 일어나는것을 막는 방법</li>
<li>DeepLearning의 특성상 학습이 계속 진행되면서 파라미터가 바뀌면 과거에 학습했던 정보가 희석되는 현상이 일어나는데 특히 pretrained 모델에게 안좋은 영향을 준다</li>
<li>pytorch의 requires_grad를 비활성화 시키거나 hook를 이용해서 backward의 input_grad를 0으로 고정시켜버리는 것으로도 가능하다</li>
</ul>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a target="_blank" rel="noopener" href="https://boostcamp.connect.or.kr/program_ai.html">Naver Connect Boostcamp - ai tech</a></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-01-26T01:50:44.000Z" title="2022. 1. 26. 오전 10:50:44">2022-01-26</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-01-28T04:54:19.713Z" title="2022. 1. 28. 오후 1:54:19">2022-01-28</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">5분안에 읽기 (약 730 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/01/26/boostcamp/week/week2/pytorch-5/">부스트 캠프 ai tech 2주 3일차 Pytorch (4)</a></h1><div class="content"><hr>
<h2 id="4-Dataset-amp-DataLoader"><a href="#4-Dataset-amp-DataLoader" class="headerlink" title="4. Dataset &amp; DataLoader"></a>4. <strong>Dataset &amp; DataLoader</strong></h2><ul>
<li>pytorch에서 생성한 모델을 학습시키기 위해 데이터를 공급해주는 유틸리티</li>
</ul>
<h3 id="4-1-Dataset"><a href="#4-1-Dataset" class="headerlink" title="4.1 Dataset"></a>4.1 <strong>Dataset</strong></h3><ul>
<li>Data를 담고 있는 Class</li>
<li>pytorch Dataset은 아래와 같이 3가지의 기본 Method로 구성되어있다</li>
<li>__init__: 초기화 함수. 필요한 변수들을 선언하고, data를 load하는 부분이다</li>
<li>__len__: 데이터의 개수를 반환하는 함수. Dataloader에서 길이등을 반환하는데 쓰인다</li>
<li>__get_item__(index): index번째의 data를 반환하는 함수. tensor로 return 해준다.</li>
<li>데이터에 따라 Map style과 iterable style로 나뉜다<ul>
<li>Map style : 일반적인 data 구조</li>
<li>iterable style : random으로 읽기 어렵거나 data에 따라 batchsize가 달라지는 data. 시계열 데이터 등에 적합하다</li>
</ul>
</li>
<li>Map style 코드는 아래와 같다  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, path</span>):</span></span><br><span class="line">        self.data = pd.read_csv(path)</span><br><span class="line">        self.X = self.data.drop([<span class="string">&#x27;label&#x27;</span>])</span><br><span class="line">        self.y = self.data[<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__get_item__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.X.iloc[idx], self.y[idx]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.X)</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="4-2-DataLoader"><a href="#4-2-DataLoader" class="headerlink" title="4.2 DataLoader"></a>4.2 <strong>DataLoader</strong></h3><ul>
<li>Dataset을 iterable 하게 사용할 수 있도록 도와주는 Utility</li>
<li>data loading 순서 커스터마이징, 자동 batch 설정, Single-Multi process data loading등 여러가지 기능을 지원한다</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DataLoader(dataset, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>, sampler=<span class="literal">None</span>,</span><br><span class="line">           batch_sampler=<span class="literal">None</span>, num_workers=<span class="number">0</span>, collate_fn=<span class="literal">None</span>,</span><br><span class="line">           pin_memory=<span class="literal">False</span>, drop_last=<span class="literal">False</span>, timeout=<span class="number">0</span>,</span><br><span class="line">           worker_init_fn=<span class="literal">None</span>, *, prefetch_factor=<span class="number">2</span>,</span><br><span class="line">           persistent_workers=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<ol>
<li>dataset<ul>
<li><code>torch.utils.data.Dataset</code> parameter</li>
</ul>
</li>
<li>batch_size <ul>
<li>Data를 불러올 때 배치사이즈를 설정하는 항목  </li>
</ul>
</li>
<li>shuffle <ul>
<li>Data load 순서를 항상 랜덤하게 뽑을지를 결정하는 항목</li>
<li>torch.manual_seed 를 통해 랜덤값을 고정할 수도 있다  </li>
</ul>
</li>
<li>sampler<ul>
<li>Data의 index를 컨트롤 하는 방법</li>
<li><code>torch.utils.data.Sampler</code> 객체를 사용한다</li>
<li>SequentialSampler : 항상 같은 순서로 elements들을 sampling한다</li>
<li>RandomSampler : 랜덤하게 sampling 한다. replacement 가능, random의 범위를 지정 가능하다 (default=len(dataset))</li>
<li>SubsetRandomSampler : 랜덤하게 sampling 한다 위의 두 기능은 없다</li>
<li>WeigthRandomSampler : 가중치에 따라 뽑히는 확률이 달라진다</li>
<li>BatchSampler : Batch단위로 sampling을 해준다</li>
<li>DistributedSampler : Multi GPU에서 분산처리를 할때 사용한다  </li>
</ul>
</li>
<li>batch_sampler<ul>
<li>sampler와 같지만 기본적으로 BatchSampler가 적용된 상태이다</li>
</ul>
</li>
<li>num_workers <ul>
<li>GPU에 Data를 load 할때 사용할 process의 수를 결정한다</li>
</ul>
</li>
<li>collate_fn <ul>
<li>sample list를 합쳐서 tensor의 minibatch로 바꿔주는 기능. map style의 dataset에서 사용한다</li>
<li>데이터마다의 길이가 다른 NLP에서 많이 사용한다</li>
</ul>
</li>
<li>pin_memory <ul>
<li>pin memory를 사용하여 GPU에 더 빠르게 data를 load하는 방법. </li>
<li>추가적인 메모리 자원이 필요하다. 보통 parallel 모델에서 많이 사용한다 </li>
</ul>
</li>
<li>drop_last <ul>
<li>Data의 전체 개수가 batchsize로 나누어 떨어지지 않을때 마지막 batch를 drop를 결정하는 parameter</li>
</ul>
</li>
</ol>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><ul>
<li><a target="_blank" rel="noopener" href="https://boostcamp.connect.or.kr/program_ai.html">Naver Connect Boostcamp - ai tech</a></li>
<li><a target="_blank" rel="noopener" href="https://subinium.github.io/pytorch-dataloader/">[Pytorch] DataLoader parameter별 용도</a></li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">Pytorch DataLoader 공식문서</a></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-01-24T06:30:11.000Z" title="2022. 1. 24. 오후 3:30:11">2022-01-24</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-03-09T07:22:00.978Z" title="2022. 3. 9. 오후 4:22:00">2022-03-09</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">9분안에 읽기 (약 1366 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/01/24/boostcamp/week/week2/pytorch-3/">부스트 캠프 ai tech 2주 2일차 Pytorch (3)</a></h1><div class="content"><hr>
<h2 id="3-torch-nn"><a href="#3-torch-nn" class="headerlink" title="3. torch.nn"></a>3. <strong>torch.nn</strong></h2><ul>
<li>Pytorch의 Nerual Network와 관련된 기능들이 있는 모듈이다</li>
<li>Neural Network와 관련된 Layer, Function들이 속해있다<ul>
<li>Layer : 1층의 인공신경망을 이야기한다. input으로 들어온 값을 선형연산이나, 비선형연산을 통해 output을 return해 주는 class이다</li>
<li>Function : 활성화함수 등의 Neural Network의 연산을 하기위해 필요한 함수을 이야기한다  </li>
</ul>
</li>
</ul>
<h3 id="3-1-nn-Module"><a href="#3-1-nn-Module" class="headerlink" title="3.1 nn.Module"></a>3.1 <strong>nn.Module</strong></h3><ul>
<li>Custom Network(모델)를 만들기 위해서 지원하는 module이다</li>
<li>nn.Module은 내부에 Module을 포함할 수 있다<ul>
<li>여러층으로 쌓이는 모양으로 인해 Layer라고도 부른다</li>
<li>Layer가 모여서 Model을 이룬다</li>
</ul>
</li>
<li>기본적으로 아래와 같은코드를 베이스로 만들 수 있다<ul>
<li>super : nn.Module에서 Attribute를 상속받기위한 선언. 이것이 없으면 빈 깡통 클래스이다</li>
<li>forward : 순전파를 구현하는 함수  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TestNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TestNet, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x_out</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="3-2-Container"><a href="#3-2-Container" class="headerlink" title="3.2 Container"></a>3.2 <strong>Container</strong></h3><ul>
<li>Layer들을 묶어서 보관하기 위한 저장소</li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/nn.html#containers">Containers  - PyTorch 공식 문서</a></li>
</ul>
<ol>
<li>nn.Sequential()<ul>
<li>여러 모듈을 하나로 묶어서 하나의 모듈처럼 사용할 수 있는 Container</li>
<li>순차적인 Layer들을 하나로 묶어서 깔끔하게 관리 할 수 있다</li>
</ul>
</li>
<li>nn.ModuleList()<ul>
<li>여러 모듈을 list처럼 한군데 모아두는 Container</li>
<li>indexing을 통해 필요한 모듈을 꺼내 쓸 수 있다</li>
<li>일반적인 List와 다르게 Attribute여도 Class를 print할 때 외부에 출력된다</li>
</ul>
</li>
<li>nn.ModuleDict()<ul>
<li>여러 모듈을 dict처럼 한군데 모아두는 Container</li>
<li>Key값을 통해 필요한 모듈을 불러올 수 있다</li>
<li>ModuleList()와 같이 Class를 print할 때 외부에 출력된다  </li>
</ul>
</li>
</ol>
<h3 id="3-3-Parameter-amp-Buffer"><a href="#3-3-Parameter-amp-Buffer" class="headerlink" title="3.3 Parameter &amp; Buffer"></a>3.3 <strong>Parameter &amp; Buffer</strong></h3><ul>
<li>Parameter<ul>
<li>모듈안에 임시로 저장되는 특별한 Tensor</li>
<li>일반적인 Tensor attribute와는 다르게 기울기 계산이 가능하고, 모델저장시에 같이 저장된다</li>
<li>RNN 같이 parameter가 반복되고, 갱신이 필요한 경우 사용된다</li>
<li>또한 모듈속의 내부모듈들의 tensor는 전부 parameter로 지정된다</li>
<li>Parameter()로 선언 할 수 있다</li>
</ul>
</li>
<li>Buffer<ul>
<li>모듈안에 임시로 저장되는 Tensor</li>
<li>모델저장시에 같이 저장된다</li>
<li>config용의 정보등을 저장할 때 사용한다</li>
<li>nn.Module의 register_buffer로 등록할 수 있다</li>
</ul>
</li>
</ul>
<h3 id="3-4-Module-내부-살펴보기"><a href="#3-4-Module-내부-살펴보기" class="headerlink" title="3.4 Module 내부 살펴보기"></a>3.4 <strong>Module 내부 살펴보기</strong></h3><ul>
<li>nn.module에는 내부의 여러 attribute를 볼 수 있는 기능이 존재한다</li>
<li>내부의 모듈, Parameter, buffer 등 여러 attribute가 ObjectDict형태로 저장되어 불러올 수 있다</li>
</ul>
<ol>
<li>submodule<ul>
<li>모듈속 모듈인 submodule은 아래의 함수들로 살펴 볼 수 있다</li>
<li>named_children<ul>
<li>module에 바로 아래단계에 속한 submodule만 보여준다</li>
</ul>
</li>
<li>named_modules<ul>
<li>submodule 뿐만아니라 module에 속해있는 모든 module을 보여준다</li>
</ul>
</li>
</ul>
</li>
<li>parameter<ul>
<li>named_parameters를 통해 parameter를 호출이 가능하다</li>
</ul>
</li>
<li>buffer<ul>
<li>named_buffers를 통해 buffer 호출이 가능하다</li>
</ul>
</li>
</ol>
<h3 id="3-5-hook"><a href="#3-5-hook" class="headerlink" title="3.5 hook"></a>3.5 <strong>hook</strong></h3><ul>
<li>package화 된 코드에서 custom 코드를 중간에 실행시킬 수 있도록 만들어 놓은 인터페이스</li>
<li>pytorch에는 등록하는 대상에 따른 2가지 종류의 hook<ul>
<li>Tensor에 등록하는 Tensor hook</li>
<li>Module에 등록하는 Module hook  </li>
</ul>
</li>
<li>실행 시점에 따른 5가지 종류의 hook이 존재한다<ul>
<li>forward pre hooks : forward 연산 전에 실행되는 hook</li>
<li>forward hooks : forward 연산 후에 실행되는 hook</li>
<li>backward_hooks : backward 연산이 수행될때 마다 실행되는 hook. 현재는 사용하는걸 권장하지 않는다</li>
<li>full backward hooks : backward 연산이 수행될때 마다 실행되는 hook</li>
<li>state dict hooks : load_state_dict 함수가 모듈 내부에서 실행하는 hook, 직접적으로 user가 잘 사용하지는 않는다</li>
</ul>
</li>
</ul>
<ol>
<li><strong>Tensor hook</strong><ul>
<li>Tensor에 대한 Backward Propagation 후에 작동하는 hook</li>
<li>torch.Tensor.register_hook 을 통하여 hook을 등록 할 수 있다</li>
<li>torch.Tensor._backward_hooks 을 통하여 등록한 hook을 확인 할 수 있다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hook</span>(<span class="params">grad</span>):</span></span><br><span class="line">  <span class="keyword">pass</span></span><br><span class="line">tensor.register_hook(hook)</span><br><span class="line">tensor_backward_hooks() <span class="comment"># OrderedDict([(0, &lt;function __main__.hook(grad)&gt;)])</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><strong>Module hook</strong><ul>
<li>Module hook은 3개의 종류의 hook으로 사용된다</li>
<li>forward pre hooks</li>
<li>forward hooks</li>
<li><del>backward_hooks</del></li>
<li>full backward hooks</li>
</ul>
</li>
</ol>
<ul>
<li>forward pre hooks<ul>
<li>forward 연산이 일어나기 전 시점에서 실행되는 hook</li>
<li>parameter로 module과 input으로 받고 input을 수정해서 return 할 수 있다</li>
<li>Module.register_forward_pre_hook(hook)으로 등록이 가능하다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">forward_pre_hook(module, <span class="built_in">input</span>) -&gt; <span class="literal">None</span> <span class="keyword">or</span> modified <span class="built_in">input</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>forward hooks<ul>
<li>forward 연산이 일어난 뒤 시점에서 실행되는 hook</li>
<li>parameter로 module, input, output으로 받고, output을 수정해서 return 할 수 있다</li>
<li>input값또한 수정이 가능하지만 forward 연산에 변화는 없다</li>
<li>Module.register_forward_hook(hook)으로 등록이 가능하다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">forward_hook(module, <span class="built_in">input</span>, output) -&gt; <span class="literal">None</span> <span class="keyword">or</span> modified output</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>full backward hooks<ul>
<li>backward 연산이 수행될때 마다 실행되는 hook</li>
<li>parameter로 module, grad_input, grad_output으로 받고, 새로운 grad_input return 할 수 있다</li>
<li>parameter인 grad_input 자체를 수정하면 Error가 발생할 수 있다</li>
<li>Module.register_full_backward_hook(hook)으로 등록이 가능하다  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">full_backward_hooks(module, grad_input, grad_output) -&gt; <span class="literal">None</span> <span class="keyword">or</span> modified grad_input</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="3-6-apply"><a href="#3-6-apply" class="headerlink" title="3.6 apply"></a>3.6 <strong>apply</strong></h3><ul>
<li>특정 함수를 Module과 Module에 속한 submodule에 적용하는 함수</li>
<li>weight 초기화나, 내부 모듈에 특정한 method를 추가할 때 사용할 수 있다</li>
<li>weight_initialization<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_initialization</span>(<span class="params">module</span>):</span></span><br><span class="line">    module_name = module.__class__.__name__</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;Function&#x27;</span> <span class="keyword">in</span> module_name:</span><br><span class="line">        module.W.data.fill_(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li>
<li>make_method<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">function_repr</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">f&#x27;name=<span class="subst">&#123;self.name&#125;</span>&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_repr</span>(<span class="params">module</span>):</span></span><br><span class="line">    module_name = module.__class__.__name__</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;Function&#x27;</span> <span class="keyword">in</span> module_name:</span><br><span class="line">        module.extra_repr = partial(function_repr, module)</span><br></pre></td></tr></table></figure></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-01-24T01:42:40.000Z" title="2022. 1. 24. 오전 10:42:40">2022-01-24</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-01-25T06:15:01.612Z" title="2022. 1. 25. 오후 3:15:01">2022-01-25</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">8분안에 읽기 (약 1256 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/01/24/boostcamp/week/week2/pytorch-2/">부스트 캠프 ai tech 2주 1일차 Pytorch (2)</a></h1><div class="content"><hr>
<h2 id="2-유용한-torch-함수들"><a href="#2-유용한-torch-함수들" class="headerlink" title="2. 유용한 torch 함수들"></a>2. <strong>유용한 torch 함수들</strong></h2><ul>
<li>torch의 내장함수들 중 자주 쓰일만한 녀석들의 정리글이다<br><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#tensors">pytorch 공식문서 - 링크</a></li>
</ul>
<ol>
<li>Tensors</li>
<li>Creation Ops</li>
<li>indexing, Slicing, Joining, Mutating</li>
<li></li>
</ol>
<h3 id="2-1-Tensors"><a href="#2-1-Tensors" class="headerlink" title="2.1 Tensors"></a>2.1 <strong>Tensors</strong></h3><ol>
<li>is_*<ul>
<li>데이터 형태가 tensor인지 판단, tensor의 내부 데이터 등의 여러가지 판단을 하는 함수<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">0</span>])</span><br><span class="line">is_tensor(x) <span class="comment"># True</span></span><br><span class="line">is_nonzero(x) <span class="comment"># False, input : single element tensor</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>torch.numel(x)<ul>
<li>전체 element가 몇개인지 출력하는 함수  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">torch.numel(a) <span class="comment"># 120</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h3 id="2-2-Creation-Ops"><a href="#2-2-Creation-Ops" class="headerlink" title="2.2 Creation Ops"></a>2.2 <strong>Creation Ops</strong></h3><ol>
<li>torch.from_numpy<ul>
<li>ndarray를 torch.Tensor로 바꾸는 함수</li>
</ul>
</li>
<li>torch.zeros(size), empty(size), ones(size)<ul>
<li>0, 빈, 1로 이루어진 tensor를 size 형태로 생성하는 함수</li>
<li>numpy와 같은 기능을 한다</li>
</ul>
</li>
<li>torch.zeros_like(tensor), empty_like(tensor), ones_like(tensor)<ul>
<li>tensor의 size를 가진 0, 빈, 1로 이루어진 tensor를 생성하는 함수</li>
<li>numpy와 같은 기능을 한다</li>
</ul>
</li>
<li>torch.arrange(start, end, step)<ul>
<li>numpy의 arrange와 같은 기능을 하는 함수</li>
<li>start 부터 end 까지 step마다의 수를 가진 1D-tensor를 생성한다</li>
</ul>
</li>
<li>torch.linspace(start, end, steps)<ul>
<li>start에서 end의 구간의 길이를 steps개로 균등하게 나누는 1D-tensor를 생성한다</li>
</ul>
</li>
<li>torch.full(size, fill_value), torch.full_like(tensor, fill_value)<ul>
<li>fill_value로 채워진 tensor를 생성한다</li>
</ul>
</li>
</ol>
<h3 id="2-3-indexing-Slicing-Joining-Mutating-함수"><a href="#2-3-indexing-Slicing-Joining-Mutating-함수" class="headerlink" title="2.3 indexing, Slicing, Joining, Mutating 함수"></a>2.3 <strong>indexing, Slicing, Joining, Mutating 함수</strong></h3><ol>
<li>torch.index_select(input, dim, index)<ul>
<li>특정한 index에 위치한 데이터를 모아서 return 해주는 함수<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">A = torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">torch.index_select(A, <span class="number">1</span>, torch.tensor([<span class="number">0</span>]))</span><br><span class="line">===========================================</span><br><span class="line">output:</span><br><span class="line">tensor([[<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">3.</span>]])</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>torch.gather(input, dim, index)<ul>
<li>특정한 index들에 위치한 데이터를 모아서 return 해주는 함수<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">torch.gather(t, <span class="number">1</span>, torch.tensor([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>]]))</span><br><span class="line">==================================================</span><br><span class="line">output:</span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">3</span>]])</span><br><span class="line">==================================================</span><br><span class="line">index calculate:</span><br><span class="line">out[i][j][k] = <span class="built_in">input</span>[index[i][j][k]][j][k]  <span class="comment"># if dim == 0</span></span><br><span class="line">out[i][j][k] = <span class="built_in">input</span>[i][index[i][j][k]][k]  <span class="comment"># if dim == 1</span></span><br><span class="line">out[i][j][k] = <span class="built_in">input</span>[i][j][index[i][j][k]]  <span class="comment"># if dim == 2</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>torch.cat(tensors, dim) == torch.concat <ul>
<li>tensors들을 합치는 함수</li>
<li>기준이 되는 dim을 제외하고 같은 shape를 가지고 있어야한다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">torch.cat((x,y), <span class="number">0</span>).size() <span class="comment"># torch.Size([3, 3])</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>torch.chunk(input, chunks, dim)<ul>
<li>tensor를 chunk의 갯수만큼으로 분리해주는 함수</li>
<li>chunks의 갯수가 넘어가지 않는 선에서 같은 size의 tensor로 분리해준다</li>
<li>나누어 떨어지지 않는경우 마지막 tensor의 사이즈의 크기가 다를 수도 있다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.arange(<span class="number">13</span>).chunk(<span class="number">6</span>)</span><br><span class="line">=========================</span><br><span class="line">output:</span><br><span class="line">(tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]),</span><br><span class="line"> tensor([<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]),</span><br><span class="line"> tensor([<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]),</span><br><span class="line"> tensor([ <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]),</span><br><span class="line"> tensor([<span class="number">12</span>]))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                  [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(torch.chunk(t, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">===========================</span><br><span class="line">output:</span><br><span class="line">(tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>]]), tensor([[<span class="number">3</span>],</span><br><span class="line">        [<span class="number">6</span>]]))</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>torch.Tensor.scatter_(dim, index, src, reduce=None)<ul>
<li>Tensor에 index에 맞춰서 src를 삽입하는 함수이다</li>
<li>reduce에 add, multiple을 넣어서 더하거나 곱하기도로 바꿀 수 있다</li>
<li>torch.gather와 반대로 작동한다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">src = torch.arange(<span class="number">1</span>, <span class="number">11</span>).reshape((<span class="number">2</span>, <span class="number">5</span>)) </span><br><span class="line"><span class="comment"># tensor([[ 1,  2,  3,  4,  5], [ 6,  7,  8,  9, 10]])</span></span><br><span class="line">index = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>]])</span><br><span class="line">torch.zeros(<span class="number">3</span>, <span class="number">5</span>, dtype=src.dtype).scatter_(<span class="number">0</span>, index, src)</span><br><span class="line">==========================================================</span><br><span class="line">output:</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">==========================================================</span><br><span class="line">index calculate:</span><br><span class="line">self[index[i][j][k]][j][k] = src[i][j][k]  <span class="comment"># if dim == 0</span></span><br><span class="line">self[i][index[i][j][k]][k] = src[i][j][k]  <span class="comment"># if dim == 1</span></span><br><span class="line">self[i][j][index[i][j][k]] = src[i][j][k]  <span class="comment"># if dim == 2</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>torch.stack(tensors, dim)<ul>
<li>지정하는 차원으로 확장해서 tensor를 쌓아주는 함수이다</li>
<li>두 차원이 정확하게 일치해야 쌓기가 가능하다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>) <span class="comment"># 3, 1, 3</span></span><br><span class="line">y = torch.rand(<span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>) <span class="comment"># 3, 1, 3</span></span><br><span class="line">torch.stack((x,y), dim=<span class="number">2</span>).size() <span class="comment">#torch.Size([3, 1, 2, 3])</span></span><br></pre></td></tr></table></figure>
<h3 id="2-4-random-Sampling"><a href="#2-4-random-Sampling" class="headerlink" title="2.4 random Sampling"></a>2.4 <strong>random Sampling</strong></h3></li>
</ul>
</li>
</ol>
<ul>
<li>자주 쓰이지만 numpy와 비슷해서 문서를 참고하는편이 좋을듯 하다</li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#random-sampling">Random sampling - PyTorch 공식 문서</a></li>
<li>torch.seed(), torch.manual_seed(int)<ul>
<li>Seed값을 고정해서 랜덤한 변수를 고정시킬 수 있다</li>
<li>manual_seed는 직접 시드값을 입력할 수 있다</li>
</ul>
</li>
</ul>
<h3 id="2-5-Pointwise-Ops"><a href="#2-5-Pointwise-Ops" class="headerlink" title="2.5 Pointwise Ops"></a>2.5 <strong>Pointwise Ops</strong></h3><ul>
<li>수학 연산과 관련된 기능을 포함하는 함수군<ul>
<li>numpy와 비슷하다</li>
</ul>
</li>
</ul>
<ol>
<li>torch.sqrt(tensor)<ul>
<li>각 tensor의 element에 대한 제곱근을 구해주는 함수</li>
</ul>
</li>
<li>torch.exp(tensor)<ul>
<li>각 tensor의 element에 대한 $e^x$</li>
</ul>
</li>
<li>torch.pow(tensor)<ul>
<li>각 tensor의 element에 대한 $x^2$  </li>
</ul>
</li>
</ol>
<h3 id="2-6-Reduction-Ops"><a href="#2-6-Reduction-Ops" class="headerlink" title="2.6 Reduction Ops"></a>2.6 <strong>Reduction Ops</strong></h3><ul>
<li>조건에 따라 특정한 tensor의 값을 가져오는 함수군</li>
<li>대부분 numpy와 동일하게 작동한다  </li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#reduction-ops">Reduction Ops - PyTorch 공식 문서</a>  </li>
</ul>
<h3 id="2-7-Comparison-Ops"><a href="#2-7-Comparison-Ops" class="headerlink" title="2.7 Comparison Ops"></a>2.7 <strong>Comparison Ops</strong></h3><ul>
<li>비교와 관련된 기능을 포함하고 있는 함수군</li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#comparison-ops">Comparison Ops - PyTorch 공식 문서</a></li>
</ul>
<ol>
<li><p>torch.argsort(tensor)</p>
<ul>
<li>tensor를 sort하는 index를 return 해준다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randint(<span class="number">1</span>, <span class="number">10</span>, (<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">a</span><br><span class="line">torch.argsort(a)</span><br><span class="line">================</span><br><span class="line">output:</span><br><span class="line">tensor([[<span class="number">9</span>, <span class="number">5</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">4</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">8</span>, <span class="number">6</span>]])</span><br><span class="line">tensor([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>torch.eq, torch.gt, torch.ge</p>
<ul>
<li>tensor의 값들이 같은지, 더 큰지, 이상인지를 판단하는 함수들이다</li>
</ul>
</li>
<li><p>torch.allclose(input, other, trol, atol)</p>
<ul>
<li>input tensor와 other의 원소들의 차가 특정 범위인지를 판단하는 함수<br>$$<br>|\operatorname{input} - \operatorname{other}| \leq atol + rtol \times|other|<br>$$</li>
</ul>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.allclose(torch.tensor([<span class="number">10.1</span>, <span class="number">1e-9</span>]), torch.tensor([<span class="number">10.0</span>, <span class="number">1e-08</span>]))</span><br><span class="line"><span class="comment"># False</span></span><br></pre></td></tr></table></figure>

<h3 id="2-8-Other-Operations"><a href="#2-8-Other-Operations" class="headerlink" title="2.8 Other Operations"></a>2.8 <strong>Other Operations</strong></h3><ul>
<li>그 외 다양한 기능들이 모여있는 함수들</li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#other-operations">Other Operations - PyTorch 공식 문서</a></li>
</ul>
<ol>
<li>torch.einsum<ul>
<li>Einstein Notation에 따라 연산을 진행하는 함수</li>
<li>Einstein Notation은 특정 index의 집합에 대한 합연산을 간결하게 표시하는 방법이다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">5</span>)</span><br><span class="line">y = torch.randn(<span class="number">4</span>)</span><br><span class="line">torch.einsum(<span class="string">&#x27;i,j-&gt;ij&#x27;</span>, x, y)</span><br><span class="line">============================</span><br><span class="line">output:</span><br><span class="line">tensor([[ <span class="number">0.1156</span>, -<span class="number">0.2897</span>, -<span class="number">0.3918</span>,  <span class="number">0.4963</span>],</span><br><span class="line">        [-<span class="number">0.3744</span>,  <span class="number">0.9381</span>,  <span class="number">1.2685</span>, -<span class="number">1.6070</span>],</span><br><span class="line">        [ <span class="number">0.7208</span>, -<span class="number">1.8058</span>, -<span class="number">2.4419</span>,  <span class="number">3.0936</span>],</span><br><span class="line">        [ <span class="number">0.1713</span>, -<span class="number">0.4291</span>, -<span class="number">0.5802</span>,  <span class="number">0.7350</span>],</span><br><span class="line">        [ <span class="number">0.5704</span>, -<span class="number">1.4290</span>, -<span class="number">1.9323</span>,  <span class="number">2.4480</span>]])</span><br><span class="line">==============================================</span><br><span class="line">As = torch.randn(<span class="number">3</span>,<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">Bs = torch.randn(<span class="number">3</span>,<span class="number">5</span>,<span class="number">4</span>)</span><br><span class="line">torch.einsum(<span class="string">&#x27;bij,bjk-&gt;bik&#x27;</span>, As, Bs)</span><br><span class="line">====================================</span><br><span class="line">output:</span><br><span class="line">tensor([[[-<span class="number">1.0564</span>, -<span class="number">1.5904</span>,  <span class="number">3.2023</span>,  <span class="number">3.1271</span>],</span><br><span class="line">        [-<span class="number">1.6706</span>, -<span class="number">0.8097</span>, -<span class="number">0.8025</span>, -<span class="number">2.1183</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">4.2239</span>,  <span class="number">0.3107</span>, -<span class="number">0.5756</span>, -<span class="number">0.2354</span>],</span><br><span class="line">        [-<span class="number">1.4558</span>, -<span class="number">0.3460</span>,  <span class="number">1.5087</span>, -<span class="number">0.8530</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">2.8153</span>,  <span class="number">1.8787</span>, -<span class="number">4.3839</span>, -<span class="number">1.2112</span>],</span><br><span class="line">        [ <span class="number">0.3728</span>, -<span class="number">2.1131</span>,  <span class="number">0.0921</span>,  <span class="number">0.8305</span>]]])</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h3 id="2-9-BLAS-amp-LAPACK-Ops"><a href="#2-9-BLAS-amp-LAPACK-Ops" class="headerlink" title="2.9 BLAS &amp; LAPACK Ops"></a>2.9 <strong>BLAS &amp; LAPACK Ops</strong></h3><ul>
<li>“BLAS” - Basic Linear Algebra Subprograms</li>
<li>“LAPACK” - Linear Algebra PACKage</li>
<li>선형대수에 관련된 함수군이다</li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#blas-and-lapack-operations">BLAS &amp; LAPACK Ops - PyTorch 공식 문서</a></li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-01-24T01:10:35.000Z" title="2022. 1. 24. 오전 10:10:35">2022-01-24</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-01-24T05:30:27.794Z" title="2022. 1. 24. 오후 2:30:27">2022-01-24</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">4분안에 읽기 (약 558 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/01/24/boostcamp/week/week2/pytorch-1/">부스트 캠프 ai tech 2주 1일차 Pytorch (1)</a></h1><div class="content"><hr>
<h2 id="0-pytorch란"><a href="#0-pytorch란" class="headerlink" title="0. pytorch란?"></a>0. pytorch란?</h2><ul>
<li>Meta(구 Facebook) 에서 개발한 딥러닝 프레임워크</li>
<li>numpy + AutoGradient</li>
<li>동적 그래프 기반</li>
</ul>
<h2 id="1-pytorch-기본"><a href="#1-pytorch-기본" class="headerlink" title="1. pytorch 기본"></a>1. pytorch 기본</h2><ul>
<li>pytorch 에서는 Tensor class를 사용한다</li>
<li>Tensor<ul>
<li>numpy의 ndarray와 사실상 동일하다<ul>
<li>내장 함수도 대부분 비슷한 기능이 존재한다</li>
</ul>
</li>
<li>tensor가 가질수 있는 type은 ndarray와 동일하나 GPU 사용이 가능한 차이가 존재한다  </li>
</ul>
</li>
</ul>
<h3 id="1-1-기본-Tensor-함수"><a href="#1-1-기본-Tensor-함수" class="headerlink" title="1.1 기본 Tensor 함수"></a>1.1 기본 Tensor 함수</h3><ul>
<li>list &gt; tensor<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">data = [[<span class="number">3</span>, <span class="number">5</span>],[<span class="number">10</span>, <span class="number">5</span>]]</span><br><span class="line">x_data = torch.tensor(data)</span><br><span class="line"><span class="comment">##########################</span></span><br><span class="line">output:</span><br><span class="line">tensor([[ <span class="number">3</span>,  <span class="number">5</span>],</span><br><span class="line">        [<span class="number">10</span>,  <span class="number">5</span>]])</span><br></pre></td></tr></table></figure></li>
<li>ndArray &gt; tensor<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nd_array_ex = np.array(data)</span><br><span class="line">tensor_array = torch.from_numpy(nd_array_ex)</span><br><span class="line"><span class="comment">############################################</span></span><br><span class="line">output:</span><br><span class="line">tensor([[ <span class="number">3</span>,  <span class="number">5</span>],</span><br><span class="line">        [<span class="number">10</span>,  <span class="number">5</span>]])</span><br></pre></td></tr></table></figure></li>
<li>tensor &gt; ndarray<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor_array.numpy()</span><br><span class="line"><span class="comment">####################</span></span><br><span class="line">output:</span><br><span class="line">array([[ <span class="number">3</span>,  <span class="number">5</span>],</span><br><span class="line">       [<span class="number">10</span>,  <span class="number">5</span>]])</span><br></pre></td></tr></table></figure></li>
<li>flatten<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data = [[<span class="number">3</span>, <span class="number">5</span>, <span class="number">20</span>],[<span class="number">10</span>, <span class="number">5</span>, <span class="number">50</span>], [<span class="number">1</span>, <span class="number">5</span>, <span class="number">10</span>]]</span><br><span class="line">x_data = torch.tensor(data)</span><br><span class="line">x_data.flatten()</span><br><span class="line"><span class="comment">################</span></span><br><span class="line">output:</span><br><span class="line">tensor([ <span class="number">3</span>,  <span class="number">5</span>, <span class="number">20</span>, <span class="number">10</span>,  <span class="number">5</span>, <span class="number">50</span>,  <span class="number">1</span>,  <span class="number">5</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure></li>
<li>one_like<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.ones_like(x_data)</span><br><span class="line"><span class="comment">#######################</span></span><br><span class="line">output:</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br></pre></td></tr></table></figure></li>
<li>shape, dtype<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_data.shape <span class="comment"># torch.Size([3, 3])</span></span><br><span class="line">x_data.dtype <span class="comment"># torch.int64</span></span><br></pre></td></tr></table></figure></li>
<li>GPU load<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="1-2-Tensor-handling"><a href="#1-2-Tensor-handling" class="headerlink" title="1.2 Tensor handling"></a>1.2 Tensor handling</h3><ul>
<li>view &amp; reshape<ul>
<li>tensor의 shape를 변경하는 함수</li>
<li>view는 input tensor와 return tensor가 데이터를 공유하여 항상 같은 주소값들을 가진다</li>
<li>reshape은 tensor의 복사본 혹은 view를 반환한다<ul>
<li>원본과 동일한 tensor값이 필요할 경우에는 view를 사용하거나 clone()을 이용해야한다</li>
</ul>
</li>
</ul>
</li>
<li>squeeze &amp; unsqueeze<ul>
<li>차원의 개수가 1인 차원을 축소, 확장하는 함수</li>
<li>unsqueeze(index) : index에 1인 차원을 삽입해서 차원을 확장한다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tensor_ex = torch.rand(size=(<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">tensor_ex.squeeze().shape <span class="comment"># torch.Size([2, 2])</span></span><br><span class="line">tensor_ex = torch.rand(size=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">tensor_ex.unsqueeze(<span class="number">0</span>).shape <span class="comment"># torch.Size([1, 2, 2])</span></span><br><span class="line">tensor_ex = torch.rand(size=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">tensor_ex.unsqueeze(<span class="number">1</span>).shape <span class="comment"># torch.Size([2, 1, 2])</span></span><br><span class="line">tensor_ex = torch.rand(size=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">tensor_ex.unsqueeze(<span class="number">2</span>).shape <span class="comment"># torch.Size([2, 2, 1])</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="1-3-Tensor-operation"><a href="#1-3-Tensor-operation" class="headerlink" title="1.3 Tensor operation"></a>1.3 Tensor operation</h3><ul>
<li>numpy와 동일하게 operation에 대해서 broadcasting을 지원한다</li>
<li>행렬곱셈 연산은 mm 및 matmul을 사용한다<ul>
<li>dot은 1차원 벡터와 스칼라 연산에서만 사용가능</li>
<li>mm과 matmul은 2차원이상의 행렬연산에서만 사용가능</li>
<li>mm은 broadcasting을 지원하지 않지만 matmul은 지원한다</li>
</ul>
</li>
</ul>
<h3 id="1-4-Tensor-operation-for-ML-DL-formula"><a href="#1-4-Tensor-operation-for-ML-DL-formula" class="headerlink" title="1.4 Tensor operation for ML/DL formula"></a>1.4 Tensor operation for ML/DL formula</h3><ul>
<li>nn.functional을 이용한 다양한 연산가능</li>
<li>softmax, argmax, one_hot 등등</li>
</ul>
<h3 id="1-5-AutoGrad"><a href="#1-5-AutoGrad" class="headerlink" title="1.5 AutoGrad"></a>1.5 AutoGrad</h3><ul>
<li>자동 미분</li>
<li>tensor에 requires_grad=True로 설정해서 자동으로 gradient 추적이 가능하다<ul>
<li>기본적으로 nn모듈의 선형연산들은 default로 True로 설정되어있어 잘 쓰지 않는다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(data, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>backward() 함수를 통하여 Backpropagation 수행</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-01-20T05:04:42.000Z" title="2022. 1. 20. 오후 2:04:42">2022-01-20</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-01-24T01:16:28.934Z" title="2022. 1. 24. 오전 10:16:28">2022-01-24</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">5분안에 읽기 (약 788 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/01/20/boostcamp/week/week1/AIMath-8/">부스트 캠프 ai tech 1주 4일차 Ai Math (8)</a></h1><div class="content"><hr>
<h2 id="8-RNN"><a href="#8-RNN" class="headerlink" title="8. RNN"></a>8. RNN</h2><ul>
<li>연속적인 데이터(Sequence Data)를 주로 다루는 Nerual Network</li>
<li>소리, 문자열, 주가등의 데이터를 분석하는데 사용된다</li>
</ul>
<h3 id="8-1-시계열-데이터"><a href="#8-1-시계열-데이터" class="headerlink" title="8.1 시계열 데이터"></a>8.1 시계열 데이터</h3><ul>
<li><p>독립동등분포 가정을 잘 위배하기 때문에 순서를 바꾸거나 과거에 정보에 손실이 발생하면 데이터의 확률분포 자체가 변해버린다  </p>
</li>
<li><p>베이즈 법칙을 이용하여 다음과 같이 표현이 가능하다<br>$$<br>\begin{aligned}<br>P(X_1, … ,X_{t}) &amp; = P(X_t|X_1, …, X{t-1})P(X_1,…,X_{t-1})\\<br>&amp; = \prod_{s=1}^{t}P(X_s|X_{s-1},…,X_1)<br>\end{aligned}<br>$$</p>
</li>
<li><p>시퀸스 데이터를 다루기 위해서는 길이가 가변적인 데이터를 다룰수 있는 모델이 필요하다</p>
<ul>
<li>조건부에 들어가는 데이터의 길이는 시퀸스마다 가변적이다<br>$$<br>\begin{aligned}<br>X_t &amp;\sim P(X_t|X_{t-1}, … X_{1})\\<br>X_{t+1} &amp;\sim P(X_t|X_{t}, X_{t-1}, … X_{1})<br>\end{aligned}<br>$$</li>
</ul>
</li>
<li><p>고정된길이 $\tau$ 만큼의 시퀸스만 사용하는 모델의 경우 자기회귀모델(Autoregressive Model)이라고 한다</p>
<ul>
<li>매우 오래된 과거의 데이터는 실제 데이터에 큰 영향을 주기 힘들다는 가정하에 세워진 모델이다<br>$$<br>\begin{aligned}<br>X_t &amp;\sim P(X_t|X_{t-1}, … X_{t-\tau})\\<br>X_{t+1} &amp;\sim P(X_t|X_{t}, … X_{t-\tau+1})<br>\end{aligned}<br>$$</li>
</ul>
</li>
<li><p>이전정보를 제외한 나머지 정보들을 잠재변수로 활용하는 모델을 잠재자기회귀모델 이라고 한다</p>
<ul>
<li>앞으로 다룰 RNN도 이 모델에 해당한다<br>$$<br>\begin{aligned}<br>X_t &amp;\sim P(X_t|X_{t-1}, H_t)\\<br>X_{t+1}&amp;\sim P(X_t|X_{t}, H_{t+1})\\<br>H_t&amp;=\operatorname{Net}(H_{t-1}, X_{t-1})<br>\end{aligned}<br>$$</li>
</ul>
</li>
</ul>
<h3 id="8-2-RNN"><a href="#8-2-RNN" class="headerlink" title="8.2 RNN"></a>8.2 RNN</h3><ul>
<li>기본적인 RNN 모델은 아래와 같이 MLP와 유사한 형태를 가지고 있다</li>
<li>RNN은 이전순서의 잠재변수와 현재의 입렬을 활용하여 계산을 이어나간다</li>
<li>RNN의 역전파는 BPTT(Backpropagation Through Time)라고 불리며 연결그래프에 따라 순차적으로 계산한다<blockquote>
<ul>
<li>$S$ : 잠재변수</li>
<li>$X$ : input Data</li>
<li>$W_x$ : $X$의 가중치행렬</li>
<li>$W_{rec}$ : $S$의 가중치 행렬</li>
<li>$\sigma$ : Activate Function</li>
<li>$X$ : 시퀸스 데이터</li>
</ul>
</blockquote>
</li>
<li>RNN의 Network 연산  </li>
</ul>
<p>$$<br>\mathbf{S_{t}} = \sigma (\mathbf{O}_{t-1} + \mathbf{X}_{t}\mathbf{W}_{x})<br>$$<br>$$<br>\mathbf{O_{t}} = \mathbf{S}_{t}\mathbf{W}_{rec}<br>$$</p>
<center>

<p><img src="/img/RNN.PNG" alt="RNN"></p>
</center>

<ul>
<li>BPTT<ul>
<li>RNN의 Backpropagation 을 계산해보면 미분의 곱으로 이루어진 항이 계산된다</li>
<li>시퀸스의 길이만큼의 $W_{rec}$의 역전파가 이루어 질 때 마다 계속해서 미분을 하기 때문에 시퀸스의 길이가 길어질수록 gradient vanishing(기울기 소실)이 발생하여 계산이 불안정해 진다<blockquote>
<ul>
<li>$L$ : loss 함수</li>
<li>$y$ : target</li>
</ul>
</blockquote>
</li>
</ul>
</li>
</ul>
<p>$$<br>\frac{\partial S_{t}}{\partial W_{rec}} = \sum_{i=1}^{t-1} \left( \prod_{j=i+1}^{t} \frac{\partial S_{j}}{\partial S_{j-1}} \right)\frac{\partial S_{i}}{\partial W_{rec}} + \frac{\partial S_{t-1}}{\partial W_{rec}}<br>$$</p>
<ul>
<li>truncated BPTT<ul>
<li>RNN은 시퀸스의 길이가 길어지면 기울기 소실이 발생하여 계산이 불안정해지기 때문에 중간에 연산을 끊어주는 테크닉.</li>
</ul>
</li>
<li>이러한 기울기 소실을 해결하기 위해 등장한 네트워크<ul>
<li>LSTM, GRU</li>
</ul>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-01-20T04:02:33.000Z" title="2022. 1. 20. 오후 1:02:33">2022-01-20</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-02-07T23:47:52.308Z" title="2022. 2. 8. 오전 8:47:52">2022-02-08</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">2분안에 읽기 (약 337 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/01/20/boostcamp/week/week1/AIMath-7/">부스트 캠프 ai tech 1주 3일차 Ai Math (7)</a></h1><div class="content"><hr>
<h2 id="7-CNN"><a href="#7-CNN" class="headerlink" title="7. CNN"></a>7. CNN</h2><ul>
<li>합성곱을 이용한 신경망</li>
</ul>
<h3 id="7-1-Convolution-연산"><a href="#7-1-Convolution-연산" class="headerlink" title="7.1 Convolution 연산"></a>7.1 Convolution 연산</h3><ul>
<li><p>신호(signal)를 커널을 이용해서 국소적으로 증폭 또는 감소시키는 연산을 말한다</p>
</li>
<li><p>CNN 에서 하는 연산은 엄밀하게 말하면 Cross Correlation 연산이다</p>
<blockquote>
<ul>
<li>Cross Correlation<br>$$<br>[f*g](x)  = \int_{\mathbb{R}}f(z)g(x+z)\operatorname{d}z<br>$$</li>
<li>Convolution 연산<br>$$<br>[f*g](x)  = \int_{\mathbb{R}}f(z)g(x-z)\operatorname{d}z<br>$$</li>
</ul>
</blockquote>
</li>
<li><p>다양한 차원에서 연산이 가능하다<br>$$<br>\begin{aligned}<br>&amp;1D \quad [f*g](i) = \sum_{p=1}^{d}f(p)g(i+p)\\<br>&amp;2D \quad [f*g](i,j) = \sum_{p,q}f(p,q)g(i+p,j+q)\\<br>&amp;3D \quad [f*g](i,j,k) = \sum_{p,q,r}f(p)g(i+p,j+q,k+r)<br>\end{aligned}<br>$$</p>
</li>
</ul>
<h3 id="7-1-2D-Convolution-연산"><a href="#7-1-2D-Convolution-연산" class="headerlink" title="7.1 2D Convolution 연산"></a>7.1 2D Convolution 연산</h3><ul>
<li>2차원 convolution 연산은 커널을 input위에서 움직여가면서 선형모델과 합성함수가 적용되는 구조이다.</li>
</ul>
<center>

<p><img src="/img/Convolution_schematic.gif" alt="conv"></p>
</center>

<ul>
<li>출력크기는 아래와 같이 계산된다<blockquote>
<ul>
<li>$H, W$ : 입력크기</li>
<li>$K_{h}, K_{w}$ : 커널의 크기</li>
<li>$O_{h}, O_{w}$ : 출력의 크기<br>$$<br>O_{h} = H-K_{h}+1\\<br>O_{w} = W-K_{w}+1<br>$$</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="7-2-Convolution-연산의-Backpropagation"><a href="#7-2-Convolution-연산의-Backpropagation" class="headerlink" title="7.2 Convolution 연산의 Backpropagation"></a>7.2 Convolution 연산의 Backpropagation</h3><ul>
<li>Convolution 연산의 역전파도 결국에는 선형연산이 여러번 적용된 형태이기 때문에 계산할때 각 커널의 들어오는 모든 gradient를 더하면 된다  </li>
</ul>
<p>$$<br>\frac{\partial \mathcal{L}}{\partial w_{i}} = \sum_{j} \delta_{j} x_{i+j-1}<br>$$</p>
<center>

<p><img src="/img/conv_back.PNG" alt="conv_back"></p>
</center></div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/categories/boostcamp/week/page/3/">이전</a></div><div class="pagination-next"><a href="/categories/boostcamp/week/page/5/">다음</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/categories/boostcamp/week/">1</a></li><li><a class="pagination-link" href="/categories/boostcamp/week/page/2/">2</a></li><li><a class="pagination-link" href="/categories/boostcamp/week/page/3/">3</a></li><li><a class="pagination-link is-current" href="/categories/boostcamp/week/page/4/">4</a></li><li><a class="pagination-link" href="/categories/boostcamp/week/page/5/">5</a></li><li><a class="pagination-link" href="/categories/boostcamp/week/page/6/">6</a></li></ul></nav></div><div class="column column-left is-3-tablet is-3-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="KyuBum Shin"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">KyuBum Shin</p><p class="is-size-6 is-block">Student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Incheon</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">포스트</p><a href="/archives"><p class="title">130</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">카테고리</p><a href="/categories"><p class="title">26</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">태그</p><a href="/tags"><p class="title">52</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/KyubumShin" target="_blank" rel="noopener">팔로우</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/KyubumShin"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">링크</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/KyubumShin" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">카테고리</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Computer-Science/"><span class="level-start"><span class="level-item">Computer Science</span></span><span class="level-end"><span class="level-item tag">8</span></span></a><ul><li><a class="level is-mobile" href="/categories/Computer-Science/Database/"><span class="level-start"><span class="level-item">Database</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Computer-Science/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Computer-Science/OS/"><span class="level-start"><span class="level-item">OS</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Computer-Science/%EC%9E%90%EB%A3%8C%EA%B5%AC%EC%A1%B0/"><span class="level-start"><span class="level-item">자료구조</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Data-Engineering/"><span class="level-start"><span class="level-item">Data Engineering</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/Data-Engineering/Bigdata/"><span class="level-start"><span class="level-item">Bigdata</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/DeepLearning/"><span class="level-start"><span class="level-item">DeepLearning</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/DeepLearning/Basic/"><span class="level-start"><span class="level-item">Basic</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Paper/"><span class="level-start"><span class="level-item">Paper</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/Paper/CV/"><span class="level-start"><span class="level-item">CV</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/Paper/CV/OB/"><span class="level-start"><span class="level-item">OB</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/Programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/Programming/Docker/"><span class="level-start"><span class="level-item">Docker</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Programming/Python/tip/"><span class="level-start"><span class="level-item">tip</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="/categories/boostcamp/"><span class="level-start"><span class="level-item">boostcamp</span></span><span class="level-end"><span class="level-item tag">93</span></span></a><ul><li><a class="level is-mobile" href="/categories/boostcamp/Dairy/"><span class="level-start"><span class="level-item">Dairy</span></span><span class="level-end"><span class="level-item tag">34</span></span></a></li><li><a class="level is-mobile" href="/categories/boostcamp/Peer-Session/"><span class="level-start"><span class="level-item">Peer Session</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/boostcamp/Problems/"><span class="level-start"><span class="level-item">Problems</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/boostcamp/week/"><span class="level-start"><span class="level-item">week</span></span><span class="level-end"><span class="level-item tag">55</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EB%B0%B1%EC%97%94%EB%93%9C/"><span class="level-start"><span class="level-item">백엔드</span></span><span class="level-end"><span class="level-item tag">1</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EB%B0%B1%EC%97%94%EB%93%9C/%EB%A9%94%EC%84%B8%EC%A7%80%ED%81%90/"><span class="level-start"><span class="level-item">메세지큐</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%EC%9D%BC%EC%83%81/"><span class="level-start"><span class="level-item">일상</span></span><span class="level-end"><span class="level-item tag">48</span></span></a><ul><li><a class="level is-mobile" href="/categories/%EC%9D%BC%EC%83%81/TIL/"><span class="level-start"><span class="level-item">TIL</span></span><span class="level-end"><span class="level-item tag">47</span></span></a></li><li><a class="level is-mobile" href="/categories/%EC%9D%BC%EC%83%81/%EA%B3%84%ED%9A%8D/"><span class="level-start"><span class="level-item">계획</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">최근 글</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-07-16T12:24:11.000Z">2022-07-16</time></p><p class="title"><a href="/2022/07/16/paper/Ghostnet/">GhostNet: More Features from Cheap Operations</a></p><p class="categories"><a href="/categories/Paper/">Paper</a> / <a href="/categories/Paper/CV/">CV</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-06-22T13:38:19.000Z">2022-06-22</time></p><p class="title"><a href="/2022/06/22/Few-Shot-Learning/">Meta Learning</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-06-20T02:02:36.000Z">2022-06-20</time></p><p class="title"><a href="/2022/06/20/DE/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A5%BC-%EC%A7%80%ED%83%B1%ED%95%98%EB%8A%94-%EA%B8%B0%EC%88%A0-1/">빅데이터를 지탱하는 기술 (1)</a></p><p class="categories"><a href="/categories/Data-Engineering/">Data Engineering</a> / <a href="/categories/Data-Engineering/Bigdata/">Bigdata</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-06-01T00:50:53.000Z">2022-06-01</time></p><p class="title"><a href="/2022/06/01/tip/walrus/">Walrus 연산자</a></p><p class="categories"><a href="/categories/Programming/">Programming</a> / <a href="/categories/Programming/Python/">Python</a> / <a href="/categories/Programming/Python/tip/">tip</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-04-23T01:19:47.000Z">2022-04-23</time></p><p class="title"><a href="/2022/04/23/tip/conda-cuda-%EC%84%A4%EC%B9%98/">cudatoolkit 버전별로 관리하기</a></p><p class="categories"><a href="/categories/Programming/">Programming</a> / <a href="/categories/Programming/Python/">Python</a> / <a href="/categories/Programming/Python/tip/">tip</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">아카이브</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/07/"><span class="level-start"><span class="level-item">7월 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">6월 2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/04/"><span class="level-start"><span class="level-item">4월 2022</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/03/"><span class="level-start"><span class="level-item">3월 2022</span></span><span class="level-end"><span class="level-item tag">20</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/02/"><span class="level-start"><span class="level-item">2월 2022</span></span><span class="level-end"><span class="level-item tag">34</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">1월 2022</span></span><span class="level-end"><span class="level-item tag">39</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/10/"><span class="level-start"><span class="level-item">10월 2021</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/09/"><span class="level-start"><span class="level-item">9월 2021</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/08/"><span class="level-start"><span class="level-item">8월 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">태그</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/3D/"><span class="tag">3D</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Ai-Math/"><span class="tag">Ai Math</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Ai-serving/"><span class="tag">Ai serving</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Bigdata/"><span class="tag">Bigdata</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CNN/"><span class="tag">CNN</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CNN-Viz/"><span class="tag">CNN Viz</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CV/"><span class="tag">CV</span><span class="tag">12</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Engineering/"><span class="tag">Data Engineering</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-visualization/"><span class="tag">Data visualization</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/DeepLearning/"><span class="tag">DeepLearning</span><span class="tag">25</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docker/"><span class="tag">Docker</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Function/"><span class="tag">Function</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN/"><span class="tag">GAN</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Generative-Model/"><span class="tag">Generative Model</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Instance-Segmentation/"><span class="tag">Instance Segmentation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LandMark-Detection/"><span class="tag">LandMark Detection</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MLops/"><span class="tag">MLops</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Multi-modal/"><span class="tag">Multi-modal</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OCR/"><span class="tag">OCR</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Object-Detection/"><span class="tag">Object Detection</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Project-product/"><span class="tag">Project product</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Semantic-Segmentation/"><span class="tag">Semantic Segmentation</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Sequencial-Model/"><span class="tag">Sequencial Model</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Transformer/"><span class="tag">Transformer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VAE/"><span class="tag">VAE</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/backend/"><span class="tag">backend</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cs/"><span class="tag">cs</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/database/"><span class="tag">database</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/env/"><span class="tag">env</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/message/"><span class="tag">message</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/network/"><span class="tag">network</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/os/"><span class="tag">os</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">11</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pytorch/"><span class="tag">pytorch</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week1/"><span class="tag">week1</span><span class="tag">19</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week10/"><span class="tag">week10</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week11/"><span class="tag">week11</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week12/"><span class="tag">week12</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week13/"><span class="tag">week13</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week14/"><span class="tag">week14</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week2/"><span class="tag">week2</span><span class="tag">15</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week3/"><span class="tag">week3</span><span class="tag">11</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week4/"><span class="tag">week4</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week5/"><span class="tag">week5</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week6/"><span class="tag">week6</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week8/"><span class="tag">week8</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/week9/"><span class="tag">week9</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%9D%BC%EC%83%81/"><span class="tag">일상</span><span class="tag">47</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%9E%90%EB%A3%8C%EA%B5%AC%EC%A1%B0/"><span class="tag">자료구조</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%ED%95%A0%EC%9D%BC/"><span class="tag">할일</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">업데이트 소식 받기</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="구독"></div></div></form></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="구독"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="KyuBum&#039;s Dev Blog" height="28"></a><p class="is-size-7"><span>&copy; 2022 KyuBum Shin</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ko");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="맨 위로" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "이 웹 사이트는 귀하의 경험을 향상시키기 위해 Cookie를 사용합니다.",
          dismiss: "무시",
          allow: "허용",
          deny: "거부",
          link: "더 알아보기",
          policy: "Cookie 정책",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="입력 하세요..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"입력 하세요...","untitled":"(제목 없음)","posts":"포스트","pages":"페이지","categories":"카테고리","tags":"태그"});
        });</script></body></html>