<!doctype html>
<html lang="ko"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>부스트 캠프 ai tech 2주 4일차 Pytorch (6) - KyuBum&#039;s Dev Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="KyuBum Shin"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="KyuBum Shin"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="7. Multi GPU 학습 데이터의 양이 방대해짐에 따라서 모든 데이터들을 전부 메모리에 올리는것이 물리적으로 힘들고, 시간적으로도 소요가 많이 되어서 이를 해결하기 위해 여러대의 GPU를 병렬적으로 사용하는 방법이다 크게 2가지의 방법으로 나뉜다 Model 병렬화 : 모델을 나눠서 학습한다 Data 병렬화 : 데이터를 나눠서 학습한다      7.1"><meta property="og:type" content="blog"><meta property="og:title" content="부스트 캠프 ai tech 2주 4일차 Pytorch (6)"><meta property="og:url" content="https://kyubumshin.github.io/2022/01/27/boostcamp/week/week2/pytorch-8/"><meta property="og:site_name" content="KyuBum&#039;s Dev Blog"><meta property="og:description" content="7. Multi GPU 학습 데이터의 양이 방대해짐에 따라서 모든 데이터들을 전부 메모리에 올리는것이 물리적으로 힘들고, 시간적으로도 소요가 많이 되어서 이를 해결하기 위해 여러대의 GPU를 병렬적으로 사용하는 방법이다 크게 2가지의 방법으로 나뉜다 Model 병렬화 : 모델을 나눠서 학습한다 Data 병렬화 : 데이터를 나눠서 학습한다      7.1"><meta property="og:locale" content="ko_KR"><meta property="og:image" content="https://kyubumshin.github.io/img/Alexnet.png"><meta property="article:published_time" content="2022-01-27T00:51:53.000Z"><meta property="article:modified_time" content="2022-01-28T04:41:54.410Z"><meta property="article:author" content="KyuBum Shin"><meta property="article:tag" content="week2"><meta property="article:tag" content="pytorch"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/Alexnet.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://kyubumshin.github.io/2022/01/27/boostcamp/week/week2/pytorch-8/"},"headline":"부스트 캠프 ai tech 2주 4일차 Pytorch (6)","image":["https://kyubumshin.github.io/img/Alexnet.png"],"datePublished":"2022-01-27T00:51:53.000Z","dateModified":"2022-01-28T04:41:54.410Z","author":{"@type":"Person","name":"KyuBum Shin"},"publisher":{"@type":"Organization","name":"KyuBum's Dev Blog","logo":{"@type":"ImageObject","url":"https://kyubumshin.github.io/img/logo.svg"}},"description":"7. Multi GPU 학습 데이터의 양이 방대해짐에 따라서 모든 데이터들을 전부 메모리에 올리는것이 물리적으로 힘들고, 시간적으로도 소요가 많이 되어서 이를 해결하기 위해 여러대의 GPU를 병렬적으로 사용하는 방법이다 크게 2가지의 방법으로 나뉜다 Model 병렬화 : 모델을 나눠서 학습한다 Data 병렬화 : 데이터를 나눠서 학습한다      7.1"}</script><link rel="canonical" href="https://kyubumshin.github.io/2022/01/27/boostcamp/week/week2/pytorch-8/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-1-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="KyuBum&#039;s Dev Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="검색" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-12"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-01-27T00:51:53.000Z" title="2022. 1. 27. 오전 9:51:53">2022-01-27</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-01-28T04:41:54.410Z" title="2022. 1. 28. 오후 1:41:54">2022-01-28</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">6분안에 읽기 (약 943 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile">부스트 캠프 ai tech 2주 4일차 Pytorch (6)</h1><div class="content"><hr>
<h2 id="7-Multi-GPU-학습"><a href="#7-Multi-GPU-학습" class="headerlink" title="7. Multi GPU 학습"></a>7. <strong>Multi GPU 학습</strong></h2><ul>
<li>데이터의 양이 방대해짐에 따라서 모든 데이터들을 전부 메모리에 올리는것이 물리적으로 힘들고, 시간적으로도 소요가 많이 되어서 이를 해결하기 위해 여러대의 GPU를 병렬적으로 사용하는 방법이다</li>
<li>크게 2가지의 방법으로 나뉜다<ul>
<li>Model 병렬화 : 모델을 나눠서 학습한다</li>
<li>Data 병렬화 : 데이터를 나눠서 학습한다  </li>
</ul>
</li>
</ul>
<h3 id="7-1-Model-병렬화"><a href="#7-1-Model-병렬화" class="headerlink" title="7.1 Model 병렬화"></a>7.1 Model 병렬화</h3><ul>
<li><p>모델을 나눠서 여러대의 GPU에 올려서 연산하는 방법</p>
</li>
<li><p>모델의 크기가 너무 커서 GPU에 올라가지 않는 상황에서 사용한다</p>
</li>
<li><p>모델의 병목화, 파이프라인 구축 등의 문제로 인해 구현 난이도가 높다</p>
<ul>
<li>pipeline을 제대로 구축하지 않으면 한 GPU가 연산하는동안 다른 GPU가 놀고있는 상황이 발생해 오히려 Single GPU보다 못한 상황이 발생 할 수 있다</li>
</ul>
</li>
<li><p>Model Pipeline Paralle 예시 - Pytorch 공식 모델 병렬화 Tutorial</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PipelineParallelResNet50</span>(<span class="params">ModelParallelResNet50</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, split_size=<span class="number">20</span>, *args, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PipelineParallelResNet50, self).__init__(*args, **kwargs)</span><br><span class="line">        self.split_size = split_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        splits = <span class="built_in">iter</span>(x.split(self.split_size, dim=<span class="number">0</span>))</span><br><span class="line">        s_next = <span class="built_in">next</span>(splits)</span><br><span class="line">        s_prev = self.seq1(s_next).to(<span class="string">&#x27;cuda:1&#x27;</span>)</span><br><span class="line">        ret = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> s_next <span class="keyword">in</span> splits:</span><br><span class="line">            <span class="comment"># A. s_prev는 두 번째 GPU에서 실행됩니다.</span></span><br><span class="line">            s_prev = self.seq2(s_prev)</span><br><span class="line">            ret.append(self.fc(s_prev.view(s_prev.size(<span class="number">0</span>), -<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># B. s_next는 A.와 동시에 진행되면서 첫 번째 GPU에서 실행됩니다.</span></span><br><span class="line">            s_prev = self.seq1(s_next).to(<span class="string">&#x27;cuda:1&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        s_prev = self.seq2(s_prev)</span><br><span class="line">        ret.append(self.fc(s_prev.view(s_prev.size(<span class="number">0</span>), -<span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> torch.cat(ret)</span><br></pre></td></tr></table></figure></li>
<li><p>아래의 그림에서 2장의 레이어 사이에 교차하는 부분이 병렬 GPU간의 교차 통신이다</p>
<ul>
<li>Model 병렬화는 꽤 예전 논문인 AlexNet에서도 사용되고 있었다</li>
</ul>
</li>
</ul>
<center>

<p><img src="/img/Alexnet.png" alt="Alexnet"></p>
</center>

<h3 id="7-2-Data-병렬화"><a href="#7-2-Data-병렬화" class="headerlink" title="7.2 Data 병렬화"></a>7.2 Data 병렬화</h3><ul>
<li>데이터를 나눠서 GPU에 할당한 후 결과의 평균을 취하는 방법</li>
<li>각 데이터에 대한 연산을 여러 GPU에서 동시에 수행해서 학습의 효율을 높인다<ul>
<li>합칠때 </li>
</ul>
</li>
<li>pytorch에서는 두 가지 방식을 제공한다<ul>
<li>DataParallel : 단순하게 데이터를 분배한 뒤 평균을 취하는 방식</li>
<li>Distributed DataParallel : GPU에서 모든 연산이 끝난뒤에 결과만을 공유하는 방식</li>
</ul>
</li>
</ul>
<ol>
<li>DataParallel:<ul>
<li>단순하게 GPU에 데이터를 분배한 뒤 평균을 취하는 방식</li>
<li>연산이 끝난뒤에 하나의 GPU에 loss값을 모아서 gradient를 만들고 이것들 다시 다른 GPU에 전달을 해준다</li>
<li>하나의 GPU가 특별하게 자원을 많이 사용하는 문제가 발생할 수 있다<ul>
<li>합쳐진 loss연산을 GPU는 더 많은 메모리를 사용하기 때문에 Batch사이즈의 감소등의 방법으로 메모리 부하를 줄여야 할 수도 있다</li>
</ul>
</li>
<li>매우 간단하게 pytorch에서 구현 할 수 있다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parallel_model = torch.nn.DataParallel(model) <span class="comment"># 나머지는 일반 모델과 동일</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>Distributed DataParallel:<ul>
<li>GPU에서 모든 연산이 끝난뒤에 결과만을 공유하는 방식</li>
<li>Loss, Backward 계산 모두 각각의 GPU에서 이루어지며 연산이 완전히 끝나고 평균값이 결과로 출력된다</li>
<li>개별 GPU마다 CPU에서 프로세스를 생성해서 할당한다 (Multiprocessing)</li>
<li>DataParallel과는 다르게 <code>DataLoader</code>에서 Shuffle 대신에 DistributeSampler를 사용하고, pin_memory를 활성화 시킨다</li>
</ul>
</li>
</ol>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a><strong>reference</strong></h3><ul>
<li><a target="_blank" rel="noopener" href="https://boostcamp.connect.or.kr/program_ai.html">Naver Connect Boostcamp - ai tech</a></li>
<li><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">Alexnet 논문 - ImageNet Classification with Deep Convolutional Neural Networks</a>  </li>
<li><a target="_blank" rel="noopener" href="https://tutorials.pytorch.kr/intermediate/model_parallel_tutorial.html">Pytorch 공식 모델 병렬화 Tutorial</a></li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">Pytorch Distributed Data Parallel Tutorial</a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>부스트 캠프 ai tech 2주 4일차 Pytorch (6)</p><p><a href="https://kyubumshin.github.io/2022/01/27/boostcamp/week/week2/pytorch-8/">https://kyubumshin.github.io/2022/01/27/boostcamp/week/week2/pytorch-8/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>KyuBum Shin</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2022-01-27</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2022-01-28</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/week2/">week2</a><a class="link-muted mr-2" rel="tag" href="/tags/pytorch/">pytorch</a></div><div class="sharethis-inline-share-buttons"></div><script src="&lt;script type=&quot;text/javascript&quot; src=&quot;https://platform-api.sharethis.com/js/sharethis.js#property=612ce5f747069100121128ed&amp;product=inline-share-buttons&quot; async=&quot;async&quot;&gt;&lt;/script&gt;" defer></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2022/01/27/boostcamp/week/week2/pytorch-9/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">부스트 캠프 ai tech 2주 4일차 Pytorch (7)</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2022/01/27/boostcamp/Dairy/Week2-Day-4-Review/"><span class="level-item">Week2 - Day 4 Review</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">댓글</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://kyubumshin.github.io/2022/01/27/boostcamp/week/week2/pytorch-8/';
            this.page.identifier = '2022/01/27/boostcamp/week/week2/pytorch-8/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'kyubumshin-github-io' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><!--!--><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="KyuBum&#039;s Dev Blog" height="28"></a><p class="is-size-7"><span>&copy; 2022 KyuBum Shin</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ko");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="맨 위로" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "이 웹 사이트는 귀하의 경험을 향상시키기 위해 Cookie를 사용합니다.",
          dismiss: "무시",
          allow: "허용",
          deny: "거부",
          link: "더 알아보기",
          policy: "Cookie 정책",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="입력 하세요..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"입력 하세요...","untitled":"(제목 없음)","posts":"포스트","pages":"페이지","categories":"카테고리","tags":"태그"});
        });</script></body></html>