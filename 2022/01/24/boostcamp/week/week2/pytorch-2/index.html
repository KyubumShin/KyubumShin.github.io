<!doctype html>
<html lang="ko"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>부스트 캠프 ai tech 2주 1일차 Pytorch (2) - KyuBum&#039;s Dev Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="KyuBum Shin"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="KyuBum Shin"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="2. 유용한 torch 함수들 torch의 내장함수들 중 자주 쓰일만한 녀석들의 정리글이다pytorch 공식문서 - 링크   Tensors Creation Ops indexing, Slicing, Joining, Mutating   2.1 Tensors is_* 데이터 형태가 tensor인지 판단, tensor의 내부 데이터 등의 여러가지 판단을 하는 함"><meta property="og:type" content="blog"><meta property="og:title" content="부스트 캠프 ai tech 2주 1일차 Pytorch (2)"><meta property="og:url" content="https://kyubumshin.github.io/2022/01/24/boostcamp/week/week2/pytorch-2/"><meta property="og:site_name" content="KyuBum&#039;s Dev Blog"><meta property="og:description" content="2. 유용한 torch 함수들 torch의 내장함수들 중 자주 쓰일만한 녀석들의 정리글이다pytorch 공식문서 - 링크   Tensors Creation Ops indexing, Slicing, Joining, Mutating   2.1 Tensors is_* 데이터 형태가 tensor인지 판단, tensor의 내부 데이터 등의 여러가지 판단을 하는 함"><meta property="og:locale" content="ko_KR"><meta property="og:image" content="https://kyubumshin.github.io/img/og_image.png"><meta property="article:published_time" content="2022-01-24T01:42:40.000Z"><meta property="article:modified_time" content="2022-01-25T06:15:01.612Z"><meta property="article:author" content="KyuBum Shin"><meta property="article:tag" content="week2"><meta property="article:tag" content="pytorch"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://kyubumshin.github.io/2022/01/24/boostcamp/week/week2/pytorch-2/"},"headline":"부스트 캠프 ai tech 2주 1일차 Pytorch (2)","image":["https://kyubumshin.github.io/img/og_image.png"],"datePublished":"2022-01-24T01:42:40.000Z","dateModified":"2022-01-25T06:15:01.612Z","author":{"@type":"Person","name":"KyuBum Shin"},"publisher":{"@type":"Organization","name":"KyuBum's Dev Blog","logo":{"@type":"ImageObject","url":"https://kyubumshin.github.io/img/logo.svg"}},"description":"2. 유용한 torch 함수들 torch의 내장함수들 중 자주 쓰일만한 녀석들의 정리글이다pytorch 공식문서 - 링크   Tensors Creation Ops indexing, Slicing, Joining, Mutating   2.1 Tensors is_* 데이터 형태가 tensor인지 판단, tensor의 내부 데이터 등의 여러가지 판단을 하는 함"}</script><link rel="canonical" href="https://kyubumshin.github.io/2022/01/24/boostcamp/week/week2/pytorch-2/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-1-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="KyuBum&#039;s Dev Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="검색" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-12"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-01-24T01:42:40.000Z" title="2022. 1. 24. 오전 10:42:40">2022-01-24</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-01-25T06:15:01.612Z" title="2022. 1. 25. 오후 3:15:01">2022-01-25</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/boostcamp/">boostcamp</a><span> / </span><a class="link-muted" href="/categories/boostcamp/week/">week</a></span><span class="level-item">8분안에 읽기 (약 1256 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile">부스트 캠프 ai tech 2주 1일차 Pytorch (2)</h1><div class="content"><hr>
<h2 id="2-유용한-torch-함수들"><a href="#2-유용한-torch-함수들" class="headerlink" title="2. 유용한 torch 함수들"></a>2. <strong>유용한 torch 함수들</strong></h2><ul>
<li>torch의 내장함수들 중 자주 쓰일만한 녀석들의 정리글이다<br><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#tensors">pytorch 공식문서 - 링크</a></li>
</ul>
<ol>
<li>Tensors</li>
<li>Creation Ops</li>
<li>indexing, Slicing, Joining, Mutating</li>
<li></li>
</ol>
<h3 id="2-1-Tensors"><a href="#2-1-Tensors" class="headerlink" title="2.1 Tensors"></a>2.1 <strong>Tensors</strong></h3><ol>
<li>is_*<ul>
<li>데이터 형태가 tensor인지 판단, tensor의 내부 데이터 등의 여러가지 판단을 하는 함수<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">0</span>])</span><br><span class="line">is_tensor(x) <span class="comment"># True</span></span><br><span class="line">is_nonzero(x) <span class="comment"># False, input : single element tensor</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>torch.numel(x)<ul>
<li>전체 element가 몇개인지 출력하는 함수  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">torch.numel(a) <span class="comment"># 120</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h3 id="2-2-Creation-Ops"><a href="#2-2-Creation-Ops" class="headerlink" title="2.2 Creation Ops"></a>2.2 <strong>Creation Ops</strong></h3><ol>
<li>torch.from_numpy<ul>
<li>ndarray를 torch.Tensor로 바꾸는 함수</li>
</ul>
</li>
<li>torch.zeros(size), empty(size), ones(size)<ul>
<li>0, 빈, 1로 이루어진 tensor를 size 형태로 생성하는 함수</li>
<li>numpy와 같은 기능을 한다</li>
</ul>
</li>
<li>torch.zeros_like(tensor), empty_like(tensor), ones_like(tensor)<ul>
<li>tensor의 size를 가진 0, 빈, 1로 이루어진 tensor를 생성하는 함수</li>
<li>numpy와 같은 기능을 한다</li>
</ul>
</li>
<li>torch.arrange(start, end, step)<ul>
<li>numpy의 arrange와 같은 기능을 하는 함수</li>
<li>start 부터 end 까지 step마다의 수를 가진 1D-tensor를 생성한다</li>
</ul>
</li>
<li>torch.linspace(start, end, steps)<ul>
<li>start에서 end의 구간의 길이를 steps개로 균등하게 나누는 1D-tensor를 생성한다</li>
</ul>
</li>
<li>torch.full(size, fill_value), torch.full_like(tensor, fill_value)<ul>
<li>fill_value로 채워진 tensor를 생성한다</li>
</ul>
</li>
</ol>
<h3 id="2-3-indexing-Slicing-Joining-Mutating-함수"><a href="#2-3-indexing-Slicing-Joining-Mutating-함수" class="headerlink" title="2.3 indexing, Slicing, Joining, Mutating 함수"></a>2.3 <strong>indexing, Slicing, Joining, Mutating 함수</strong></h3><ol>
<li>torch.index_select(input, dim, index)<ul>
<li>특정한 index에 위치한 데이터를 모아서 return 해주는 함수<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">A = torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">torch.index_select(A, <span class="number">1</span>, torch.tensor([<span class="number">0</span>]))</span><br><span class="line">===========================================</span><br><span class="line">output:</span><br><span class="line">tensor([[<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">3.</span>]])</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>torch.gather(input, dim, index)<ul>
<li>특정한 index들에 위치한 데이터를 모아서 return 해주는 함수<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">torch.gather(t, <span class="number">1</span>, torch.tensor([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>]]))</span><br><span class="line">==================================================</span><br><span class="line">output:</span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">3</span>]])</span><br><span class="line">==================================================</span><br><span class="line">index calculate:</span><br><span class="line">out[i][j][k] = <span class="built_in">input</span>[index[i][j][k]][j][k]  <span class="comment"># if dim == 0</span></span><br><span class="line">out[i][j][k] = <span class="built_in">input</span>[i][index[i][j][k]][k]  <span class="comment"># if dim == 1</span></span><br><span class="line">out[i][j][k] = <span class="built_in">input</span>[i][j][index[i][j][k]]  <span class="comment"># if dim == 2</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>torch.cat(tensors, dim) == torch.concat <ul>
<li>tensors들을 합치는 함수</li>
<li>기준이 되는 dim을 제외하고 같은 shape를 가지고 있어야한다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">torch.cat((x,y), <span class="number">0</span>).size() <span class="comment"># torch.Size([3, 3])</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>torch.chunk(input, chunks, dim)<ul>
<li>tensor를 chunk의 갯수만큼으로 분리해주는 함수</li>
<li>chunks의 갯수가 넘어가지 않는 선에서 같은 size의 tensor로 분리해준다</li>
<li>나누어 떨어지지 않는경우 마지막 tensor의 사이즈의 크기가 다를 수도 있다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.arange(<span class="number">13</span>).chunk(<span class="number">6</span>)</span><br><span class="line">=========================</span><br><span class="line">output:</span><br><span class="line">(tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]),</span><br><span class="line"> tensor([<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]),</span><br><span class="line"> tensor([<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]),</span><br><span class="line"> tensor([ <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]),</span><br><span class="line"> tensor([<span class="number">12</span>]))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                  [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(torch.chunk(t, <span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">===========================</span><br><span class="line">output:</span><br><span class="line">(tensor([[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>]]), tensor([[<span class="number">3</span>],</span><br><span class="line">        [<span class="number">6</span>]]))</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>torch.Tensor.scatter_(dim, index, src, reduce=None)<ul>
<li>Tensor에 index에 맞춰서 src를 삽입하는 함수이다</li>
<li>reduce에 add, multiple을 넣어서 더하거나 곱하기도로 바꿀 수 있다</li>
<li>torch.gather와 반대로 작동한다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">src = torch.arange(<span class="number">1</span>, <span class="number">11</span>).reshape((<span class="number">2</span>, <span class="number">5</span>)) </span><br><span class="line"><span class="comment"># tensor([[ 1,  2,  3,  4,  5], [ 6,  7,  8,  9, 10]])</span></span><br><span class="line">index = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>]])</span><br><span class="line">torch.zeros(<span class="number">3</span>, <span class="number">5</span>, dtype=src.dtype).scatter_(<span class="number">0</span>, index, src)</span><br><span class="line">==========================================================</span><br><span class="line">output:</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">4</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line">==========================================================</span><br><span class="line">index calculate:</span><br><span class="line">self[index[i][j][k]][j][k] = src[i][j][k]  <span class="comment"># if dim == 0</span></span><br><span class="line">self[i][index[i][j][k]][k] = src[i][j][k]  <span class="comment"># if dim == 1</span></span><br><span class="line">self[i][j][index[i][j][k]] = src[i][j][k]  <span class="comment"># if dim == 2</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>torch.stack(tensors, dim)<ul>
<li>지정하는 차원으로 확장해서 tensor를 쌓아주는 함수이다</li>
<li>두 차원이 정확하게 일치해야 쌓기가 가능하다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>) <span class="comment"># 3, 1, 3</span></span><br><span class="line">y = torch.rand(<span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>) <span class="comment"># 3, 1, 3</span></span><br><span class="line">torch.stack((x,y), dim=<span class="number">2</span>).size() <span class="comment">#torch.Size([3, 1, 2, 3])</span></span><br></pre></td></tr></table></figure>
<h3 id="2-4-random-Sampling"><a href="#2-4-random-Sampling" class="headerlink" title="2.4 random Sampling"></a>2.4 <strong>random Sampling</strong></h3></li>
</ul>
</li>
</ol>
<ul>
<li>자주 쓰이지만 numpy와 비슷해서 문서를 참고하는편이 좋을듯 하다</li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#random-sampling">Random sampling - PyTorch 공식 문서</a></li>
<li>torch.seed(), torch.manual_seed(int)<ul>
<li>Seed값을 고정해서 랜덤한 변수를 고정시킬 수 있다</li>
<li>manual_seed는 직접 시드값을 입력할 수 있다</li>
</ul>
</li>
</ul>
<h3 id="2-5-Pointwise-Ops"><a href="#2-5-Pointwise-Ops" class="headerlink" title="2.5 Pointwise Ops"></a>2.5 <strong>Pointwise Ops</strong></h3><ul>
<li>수학 연산과 관련된 기능을 포함하는 함수군<ul>
<li>numpy와 비슷하다</li>
</ul>
</li>
</ul>
<ol>
<li>torch.sqrt(tensor)<ul>
<li>각 tensor의 element에 대한 제곱근을 구해주는 함수</li>
</ul>
</li>
<li>torch.exp(tensor)<ul>
<li>각 tensor의 element에 대한 $e^x$</li>
</ul>
</li>
<li>torch.pow(tensor)<ul>
<li>각 tensor의 element에 대한 $x^2$  </li>
</ul>
</li>
</ol>
<h3 id="2-6-Reduction-Ops"><a href="#2-6-Reduction-Ops" class="headerlink" title="2.6 Reduction Ops"></a>2.6 <strong>Reduction Ops</strong></h3><ul>
<li>조건에 따라 특정한 tensor의 값을 가져오는 함수군</li>
<li>대부분 numpy와 동일하게 작동한다  </li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#reduction-ops">Reduction Ops - PyTorch 공식 문서</a>  </li>
</ul>
<h3 id="2-7-Comparison-Ops"><a href="#2-7-Comparison-Ops" class="headerlink" title="2.7 Comparison Ops"></a>2.7 <strong>Comparison Ops</strong></h3><ul>
<li>비교와 관련된 기능을 포함하고 있는 함수군</li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#comparison-ops">Comparison Ops - PyTorch 공식 문서</a></li>
</ul>
<ol>
<li><p>torch.argsort(tensor)</p>
<ul>
<li>tensor를 sort하는 index를 return 해준다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randint(<span class="number">1</span>, <span class="number">10</span>, (<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">a</span><br><span class="line">torch.argsort(a)</span><br><span class="line">================</span><br><span class="line">output:</span><br><span class="line">tensor([[<span class="number">9</span>, <span class="number">5</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">4</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">8</span>, <span class="number">6</span>]])</span><br><span class="line">tensor([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>torch.eq, torch.gt, torch.ge</p>
<ul>
<li>tensor의 값들이 같은지, 더 큰지, 이상인지를 판단하는 함수들이다</li>
</ul>
</li>
<li><p>torch.allclose(input, other, trol, atol)</p>
<ul>
<li>input tensor와 other의 원소들의 차가 특정 범위인지를 판단하는 함수<br>$$<br>|\operatorname{input} - \operatorname{other}| \leq atol + rtol \times|other|<br>$$</li>
</ul>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.allclose(torch.tensor([<span class="number">10.1</span>, <span class="number">1e-9</span>]), torch.tensor([<span class="number">10.0</span>, <span class="number">1e-08</span>]))</span><br><span class="line"><span class="comment"># False</span></span><br></pre></td></tr></table></figure>

<h3 id="2-8-Other-Operations"><a href="#2-8-Other-Operations" class="headerlink" title="2.8 Other Operations"></a>2.8 <strong>Other Operations</strong></h3><ul>
<li>그 외 다양한 기능들이 모여있는 함수들</li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#other-operations">Other Operations - PyTorch 공식 문서</a></li>
</ul>
<ol>
<li>torch.einsum<ul>
<li>Einstein Notation에 따라 연산을 진행하는 함수</li>
<li>Einstein Notation은 특정 index의 집합에 대한 합연산을 간결하게 표시하는 방법이다<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">5</span>)</span><br><span class="line">y = torch.randn(<span class="number">4</span>)</span><br><span class="line">torch.einsum(<span class="string">&#x27;i,j-&gt;ij&#x27;</span>, x, y)</span><br><span class="line">============================</span><br><span class="line">output:</span><br><span class="line">tensor([[ <span class="number">0.1156</span>, -<span class="number">0.2897</span>, -<span class="number">0.3918</span>,  <span class="number">0.4963</span>],</span><br><span class="line">        [-<span class="number">0.3744</span>,  <span class="number">0.9381</span>,  <span class="number">1.2685</span>, -<span class="number">1.6070</span>],</span><br><span class="line">        [ <span class="number">0.7208</span>, -<span class="number">1.8058</span>, -<span class="number">2.4419</span>,  <span class="number">3.0936</span>],</span><br><span class="line">        [ <span class="number">0.1713</span>, -<span class="number">0.4291</span>, -<span class="number">0.5802</span>,  <span class="number">0.7350</span>],</span><br><span class="line">        [ <span class="number">0.5704</span>, -<span class="number">1.4290</span>, -<span class="number">1.9323</span>,  <span class="number">2.4480</span>]])</span><br><span class="line">==============================================</span><br><span class="line">As = torch.randn(<span class="number">3</span>,<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">Bs = torch.randn(<span class="number">3</span>,<span class="number">5</span>,<span class="number">4</span>)</span><br><span class="line">torch.einsum(<span class="string">&#x27;bij,bjk-&gt;bik&#x27;</span>, As, Bs)</span><br><span class="line">====================================</span><br><span class="line">output:</span><br><span class="line">tensor([[[-<span class="number">1.0564</span>, -<span class="number">1.5904</span>,  <span class="number">3.2023</span>,  <span class="number">3.1271</span>],</span><br><span class="line">        [-<span class="number">1.6706</span>, -<span class="number">0.8097</span>, -<span class="number">0.8025</span>, -<span class="number">2.1183</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">4.2239</span>,  <span class="number">0.3107</span>, -<span class="number">0.5756</span>, -<span class="number">0.2354</span>],</span><br><span class="line">        [-<span class="number">1.4558</span>, -<span class="number">0.3460</span>,  <span class="number">1.5087</span>, -<span class="number">0.8530</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">2.8153</span>,  <span class="number">1.8787</span>, -<span class="number">4.3839</span>, -<span class="number">1.2112</span>],</span><br><span class="line">        [ <span class="number">0.3728</span>, -<span class="number">2.1131</span>,  <span class="number">0.0921</span>,  <span class="number">0.8305</span>]]])</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h3 id="2-9-BLAS-amp-LAPACK-Ops"><a href="#2-9-BLAS-amp-LAPACK-Ops" class="headerlink" title="2.9 BLAS &amp; LAPACK Ops"></a>2.9 <strong>BLAS &amp; LAPACK Ops</strong></h3><ul>
<li>“BLAS” - Basic Linear Algebra Subprograms</li>
<li>“LAPACK” - Linear Algebra PACKage</li>
<li>선형대수에 관련된 함수군이다</li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html#blas-and-lapack-operations">BLAS &amp; LAPACK Ops - PyTorch 공식 문서</a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>부스트 캠프 ai tech 2주 1일차 Pytorch (2)</p><p><a href="https://kyubumshin.github.io/2022/01/24/boostcamp/week/week2/pytorch-2/">https://kyubumshin.github.io/2022/01/24/boostcamp/week/week2/pytorch-2/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>KyuBum Shin</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2022-01-24</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2022-01-25</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/week2/">week2</a><a class="link-muted mr-2" rel="tag" href="/tags/pytorch/">pytorch</a></div><div class="sharethis-inline-share-buttons"></div><script src="&lt;script type=&quot;text/javascript&quot; src=&quot;https://platform-api.sharethis.com/js/sharethis.js#property=612ce5f747069100121128ed&amp;product=inline-share-buttons&quot; async=&quot;async&quot;&gt;&lt;/script&gt;" defer></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2022/01/24/boostcamp/Dairy/week2-Day1/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Week2 - Day 1 Review</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2022/01/24/boostcamp/week/week2/pytorch-1/"><span class="level-item">부스트 캠프 ai tech 2주 1일차 Pytorch (1)</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">댓글</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://kyubumshin.github.io/2022/01/24/boostcamp/week/week2/pytorch-2/';
            this.page.identifier = '2022/01/24/boostcamp/week/week2/pytorch-2/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'kyubumshin-github-io' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><!--!--><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="KyuBum&#039;s Dev Blog" height="28"></a><p class="is-size-7"><span>&copy; 2022 KyuBum Shin</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ko");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="맨 위로" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "이 웹 사이트는 귀하의 경험을 향상시키기 위해 Cookie를 사용합니다.",
          dismiss: "무시",
          allow: "허용",
          deny: "거부",
          link: "더 알아보기",
          policy: "Cookie 정책",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="입력 하세요..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"입력 하세요...","untitled":"(제목 없음)","posts":"포스트","pages":"페이지","categories":"카테고리","tags":"태그"});
        });</script></body></html>