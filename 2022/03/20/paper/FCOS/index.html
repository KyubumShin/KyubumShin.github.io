<!doctype html>
<html lang="ko"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>FCOS: Fully Convolutional One-Stage Object Detection - KyuBum&#039;s Dev Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="KyuBum Shin"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="KyuBum Shin"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="논문 링크 : https:&amp;#x2F;&amp;#x2F;arxiv.org&amp;#x2F;pdf&amp;#x2F;1904.01355.pdf  첫 논문 리뷰로 YoloX 등의 고성능 Anchor Free 모델에 응용되는 OB 모델인 FCOS에 대해서 리뷰를 진행하겠습니다.       간단 요약 Anchor Free 기반의 Object Detection Model Semantic Segmentation과 비슷하게 P"><meta property="og:type" content="blog"><meta property="og:title" content="FCOS: Fully Convolutional One-Stage Object Detection"><meta property="og:url" content="https://kyubumshin.github.io/2022/03/20/paper/FCOS/"><meta property="og:site_name" content="KyuBum&#039;s Dev Blog"><meta property="og:description" content="논문 링크 : https:&amp;#x2F;&amp;#x2F;arxiv.org&amp;#x2F;pdf&amp;#x2F;1904.01355.pdf  첫 논문 리뷰로 YoloX 등의 고성능 Anchor Free 모델에 응용되는 OB 모델인 FCOS에 대해서 리뷰를 진행하겠습니다.       간단 요약 Anchor Free 기반의 Object Detection Model Semantic Segmentation과 비슷하게 P"><meta property="og:locale" content="ko_KR"><meta property="og:image" content="https://kyubumshin.github.io/img/FCOS2.PNG"><meta property="og:image" content="https://kyubumshin.github.io/img/FCOS1.PNG"><meta property="og:image" content="https://kyubumshin.github.io/img/FCOS3.PNG"><meta property="og:image" content="https://kyubumshin.github.io/img/FCOS4.PNG"><meta property="og:image" content="https://kyubumshin.github.io/img/FCOS5.PNG"><meta property="article:published_time" content="2022-03-20T03:45:11.000Z"><meta property="article:modified_time" content="2022-03-20T14:06:39.405Z"><meta property="article:author" content="KyuBum Shin"><meta property="article:tag" content="DeepLearning"><meta property="article:tag" content="CV"><meta property="article:tag" content="Object Detection"><meta property="article:tag" content="week10"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/FCOS2.PNG"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://kyubumshin.github.io/2022/03/20/paper/FCOS/"},"headline":"FCOS: Fully Convolutional One-Stage Object Detection","image":[],"datePublished":"2022-03-20T03:45:11.000Z","dateModified":"2022-03-20T14:06:39.405Z","author":{"@type":"Person","name":"KyuBum Shin"},"publisher":{"@type":"Organization","name":"KyuBum's Dev Blog","logo":{"@type":"ImageObject","url":"https://kyubumshin.github.io/img/logo.svg"}},"description":"논문 링크 : https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1904.01355.pdf  첫 논문 리뷰로 YoloX 등의 고성능 Anchor Free 모델에 응용되는 OB 모델인 FCOS에 대해서 리뷰를 진행하겠습니다.       간단 요약 Anchor Free 기반의 Object Detection Model Semantic Segmentation과 비슷하게 P"}</script><link rel="canonical" href="https://kyubumshin.github.io/2022/03/20/paper/FCOS/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.0.2"></head><body class="is-1-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="KyuBum&#039;s Dev Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="검색" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-12"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2022-03-20T03:45:11.000Z" title="2022. 3. 20. 오후 12:45:11">2022-03-20</time>&nbsp;게시 됨</span><span class="level-item"><time dateTime="2022-03-20T14:06:39.405Z" title="2022. 3. 20. 오후 11:06:39">2022-03-20</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Paper/">Paper</a><span> / </span><a class="link-muted" href="/categories/Paper/CV/">CV</a><span> / </span><a class="link-muted" href="/categories/Paper/CV/OB/">OB</a></span><span class="level-item">14분안에 읽기 (약 2033 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile">FCOS: Fully Convolutional One-Stage Object Detection</h1><div class="content"><hr>
<p>논문 링크 : <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.01355.pdf">https://arxiv.org/pdf/1904.01355.pdf</a></p>
<blockquote>
<p>첫 논문 리뷰로 YoloX 등의 고성능 Anchor Free 모델에 응용되는 OB 모델인 FCOS에 대해서 리뷰를 진행하겠습니다.     </p>
</blockquote>
<h2 id="간단-요약"><a href="#간단-요약" class="headerlink" title="간단 요약"></a>간단 요약</h2><ul>
<li><code>Anchor Free</code> 기반의 Object Detection Model</li>
<li>Semantic Segmentation과 비슷하게 Pixel 단위의 예측을 통하여 Object Detection(OB)을 진행</li>
<li>FPN + Multi head Branch를 이용하여 성능 UP</li>
<li>Center-ness을 이용하여 좀 더 정확성을 끌어올림</li>
</ul>
<h1 id="Anchor-Based-Model의-한계"><a href="#Anchor-Based-Model의-한계" class="headerlink" title="Anchor Based Model의 한계"></a>Anchor Based Model의 한계</h1><p>본 논문에서 지금 까지의 OB모델들은 Anchor Box Base로 좋은 성능을 내어왔지만 다음과 같은 단점이 존재한다고 서술한다</p>
<ol>
<li>Anchor Box 또한 Hyper Parameter로써 성능에 매우 큰 영향을 미치기 때문에 조심스러운 튜닝이 필요하다</li>
<li>조심스럽게 튜닝을 마쳐도 고정된 Anchor Box의 크기와 차이가 많이 나면 효과적으로 학습하지 못한다</li>
<li>높은 Recall 성능을 얻기 위해서는 촘촘한 Anchor Box가 필요하다. 수많은 Anchor Box들은 많은 Negetive Sample을 생성하며 Positive와의 균형이 깨어져서  Imbalance를 야기한다</li>
<li>Anchor Box와 Ground Truth(GT)와의 IoU 계산에서 많은 코스트가 필요하다</li>
</ol>
<p>이를 통하여 본 논문에서는 이런 단점을 해결하기 위해 Anchor Free Object Detection Model인 FCOS를 제안하였다</p>
<h1 id="논문에서-제안한-Point"><a href="#논문에서-제안한-Point" class="headerlink" title="논문에서 제안한 Point"></a>논문에서 제안한 Point</h1><h2 id="0-사전-설정"><a href="#0-사전-설정" class="headerlink" title="0. 사전 설정"></a>0. 사전 설정</h2><ul>
<li>먼저 Layer $i$ 의 Feature Map을 $F_{i}$ 라고 하고, input 이미지의 GT를 $B_i &#x3D; (x^{i}_{0}, y^{i}_{0}, x^{i}_{1}, y^{i}_{1}, c^{i})$ 라고 설정한다.  <ul>
<li>$(x^{i}_{0}, y^{i}_{0})$ : left-top</li>
<li>$(x^{i}_{1}, y^{i}_{1})$ : right-bottom</li>
<li>$c^{i}$ : class</li>
</ul>
</li>
<li>Feature Map에서의 위치 좌표 $(x, y)$ 는 실제 이미지에서 다음의 좌표와 대응된다.<ul>
<li>(xs, ys)로만 표현할 경우 오차의 범위가 너무 커지기 때문에 stride의 절반을 더해주어서 보상한다</li>
<li>$s$ : size of stride<br>$$<br>([\frac{s}{2}] + xs, [\frac{s}{2}] + ys)<br>$$</li>
</ul>
</li>
</ul>
<h2 id="1-Fully-Convolutional-One-Stage-Object-Detector"><a href="#1-Fully-Convolutional-One-Stage-Object-Detector" class="headerlink" title="1. Fully Convolutional One-Stage Object Detector"></a><strong>1. Fully Convolutional One-Stage Object Detector</strong></h2><p>이 부분에서는 OB를 Pixel 단위로 예측하는 방식이 어떻게 진행되는지에 대해서 알아본다</p>
<ul>
<li>기존의 Anchor Based Model은 기준점 $x, y$를 Box의 중심으로 가정하고 그 위치로 부터 Anchor Box를 생성하는 방식으로 물체를 Detection 한다. 하지만  FCOS에서는 $x, y$ 좌표의 픽셀말다 해당하는 Class와 GT Box의 Border를 추측한다.</li>
</ul>
<h3 id="FCOS-상세-계산-방법"><a href="#FCOS-상세-계산-방법" class="headerlink" title="FCOS 상세 계산 방법"></a><strong>FCOS 상세 계산 방법</strong></h3><ol>
<li>$x, y$ 좌표의 픽셀의 분류된 class가 GT Box 안에 속하면서 class값과 같을 경우 Positive Sample로 생각한다<ul>
<li>해당되지 않을 경우에는 negative Sample 간주하고, Background(class &#x3D; 0)로 계산된다</li>
</ul>
</li>
<li>$x, y$ 좌표의 class를 분류함과 동시에 4D vector $\mathbb{t}^* &#x3D; (l^*, t^*, r^*, b^*)$ 에 대하여 Regresstion을 진행한다</li>
</ol>
<center>

<img src="/img/FCOS2.PNG" alt="" width="600px"/>

</center>

<ul>
<li>여기서 $(l^*, t^*, r^*, b^*)$ 는 각각 $x, y$ 좌표에서부터 추측한 Bbox의 경계선 까지의 거리를 말하며 다음과 같이 나타낼 수 있다.</li>
</ul>
<p>$$<br>l^* &#x3D; x -x^{i}_{0} ,\quad t^* &#x3D; y - y^{i}_{0}\\<br>r^* &#x3D; x^{i}_{1} - x,\quad b^* &#x3D; y^{i}_{1} - y<br>$$</p>
<ol start="3">
<li>$x, y$에서 예측한 4차원 거리벡터와 GT box로 계산한 거리가 일치하도록 학습이 이루어진다</li>
</ol>
<h3 id="Network-Output"><a href="#Network-Output" class="headerlink" title="Network Output"></a><strong>Network Output</strong></h3><ul>
<li>FCOS 에서는 Output으로 80-D의 Classfication Vector와 4-D의 Bbox vector $(l^*, t^*, r^*, b^*)$, Center-ness를 추정하도록 구성된다</li>
</ul>
<center>

<img src="/img/FCOS1.PNG" alt="" width="600px"/>

</center>

<ul>
<li>실제로는 Network에서 출력되는 값 $(l, t, r, b)$ 들을 exp를 통하여 변환시킨 값이 $(l^*, t^*, r^*, b^*)$가 된다<ul>
<li>너무 큰 값을 출력으로 할 경우 학습에 문제가 생길 가능성이 존재하기 때문에 의도적으로 작은값을 출력하도록 설정하였다</li>
</ul>
</li>
<li>80-D의 Classfication Vector는 multi-class classifier 구별되는 것이 아닌 각 class에 대해서 binary classification으로 예측된다</li>
</ul>
<h3 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a><strong>Loss</strong></h3><ul>
<li>Classification Loss는 Focal Loss를 사용</li>
<li>Regression Loss는 IoUloss를 사용</li>
<li>그리고 각 Loss는 Positive sample 수만큼 나누어서 Normalization을 해주었다</li>
</ul>
<h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a><strong>Inference</strong></h3><ul>
<li>$p_{x, y}$ 가 threshold 이상일 경우에는 Positive Sample로 생각하여 Bbox를 Bbox vector를 통해서 예측하고 출력한다</li>
</ul>
<h2 id="2-Multi-level-Prediction-with-FPN-for-FCOS"><a href="#2-Multi-level-Prediction-with-FPN-for-FCOS" class="headerlink" title="2. Multi-level Prediction with FPN for FCOS"></a><strong>2. Multi-level Prediction with FPN for FCOS</strong></h2><ul>
<li><p>위의 항목에서 학습을 그대로 진행하면 Anchor Free Model의 고질적인 2가지의 문제점이 발생한다</p>
<ol>
<li>작은 물체들은 stride가 큰 Feature map에서 표현이 되지 않기 때문에 Recall이 낮아진다</li>
<li>GT Box가 겹쳐져 있을때 어느 GT에 맞춰서 Pixel이 학습을 해야하는지 모호함이 발생할 수 있다</li>
</ol>
</li>
<li><p>이러한 문제점을 해결하기 위해서 논문에서는 FPN을 통한 Multi level Prediection을 제안하였다</p>
</li>
</ul>
<h3 id="Stride-Problems"><a href="#Stride-Problems" class="headerlink" title="Stride Problems"></a><strong>Stride Problems</strong></h3><ul>
<li>FCOS에서는 FPN을 이용하여 다양한 stride를 누적을 통하여 표현</li>
<li>아래에 그림에서와 기본적인 FPN을 통해서 P3, P4, P5의 Layer를 생성한다<ul>
<li>P3, P4, P5는 각각 (8, 16, 32)의 누적된 stride를 가지고 있다</li>
</ul>
</li>
<li>P5에서 추가적으로 stride가 2로 설정된 CNN Layer를 2개 생성한다<ul>
<li>생성된 P6, P7는 각각 64, 128의 누적된 stride를 가지고 있다</li>
</ul>
</li>
<li>FPN을 통해서 다양한 stride를 가지고 탐색을 진행한다<ul>
<li>각 Level의 Layer에서 Box Regression을 진행할 때 $x, y$로 부터 예측되는 Bbox vector의 범위의 제한을 두고 제한을 넘어가면 Negative Sample로 취급한다</li>
<li>논문에서는 0, 64, 128, 256, 512, $\infty$ 로 설정하였다<br>ex) P3의 경우 0~64 제한</li>
</ul>
</li>
</ul>
<h3 id="GT-Box-Overlap"><a href="#GT-Box-Overlap" class="headerlink" title="GT Box Overlap"></a><strong>GT Box Overlap</strong></h3><ul>
<li>GT Box가 겹치는 경우 발생하는 모호함에 대해서 FPN구조로 어느정도 해결이 가능하다</li>
<li>Overlap된 구간에 존재하는 Pixel들에 대해서 겹쳐지는 GT Box들간에 크기 차이가 존재할 경우 다른 Level의 Layer에서 예측될 가능성이 높다</li>
<li>그럼에도 불구하고 한 위치에 Layer 이상의 Box들이 할당이 되면은 면적이 가장 작은 GT Box를 사용한다</li>
</ul>
<center>

<img src="/img/FCOS3.PNG" alt="" width="500px"/>

</center>

<h2 id="3-Center-ness"><a href="#3-Center-ness" class="headerlink" title="3. Center-ness"></a>3. Center-ness</h2><ul>
<li>물체의 중앙점에서 먼 Pixel에서 예측된 Box Vector의 스코어가 낮은 경향을 보이는 문제가 존재하였다. 이것을 해결하기 위해 Center-ness를 도입하였다<ul>
<li>Box 외각의 pixel에서의 예측값은 classification을 통한 확률은 높아서 Positive sample로 판단 되었지만, 실제 Box Vector값은 잘 예측하지 못하는 경우가 많이 발생하였다</li>
</ul>
</li>
</ul>
<h3 id="Center-ness"><a href="#Center-ness" class="headerlink" title="Center-ness"></a><strong>Center-ness</strong></h3><ul>
<li>예측한 Box vector로 부터 $x, y$ 좌표가 Box의 Center에 가까울 수록 높은 가중치를 가지게 된다</li>
</ul>
<p>$$<br>centerness &#x3D; \sqrt{\frac{min(l^*, r^*)}{max(l^*, r^*)} \times \frac{min(t^*, b^*)}{max(t^*, b^*)}}<br>$$</p>
<ul>
<li>classification score 출력에 center-ness를 곱해주면 마지막에 NMS를 진행할 때 낮은 score를 가지게 되기 때문에 걸러지게 만들 수 있다</li>
</ul>
<center>

<img src="/img/FCOS4.PNG" alt="" width="300px"/>

</center>

<ul>
<li>논문에서는 center-ness를 도입해서 classfication score는 높지만 IoU score는 낮은 Sample들을 걸러내는 효과를 얻었다고 한다</li>
</ul>
<center>

<img src="/img/FCOS5.PNG" alt="" width="700px"/>

</center>

<h2 id="결론"><a href="#결론" class="headerlink" title="결론"></a>결론</h2><ul>
<li>Anchor Free 모델로 Anchor와 관련된 HyperParameter를 제외했다</li>
<li>다른 One stage Anchor Based Model과 비교해서 tuning이 없이도 비슷한 성능을 보여주었다</li>
<li>Pixel Prediction(Semantic Segmentation) + Multi Label FPN + Center ness</li>
<li>전체 구조가 간단하면서도 좋은 성능을 보여주어서 응용성이 뛰어나다</li>
<li>Two Stage Detecto의 RPN으로도 응용이 가능하다</li>
</ul>
<h2 id="후기"><a href="#후기" class="headerlink" title="후기"></a>후기</h2><p>예전에 YoloX를 사용해 보면서 가볍게 보고 넘어갔던 논문이었는데 개념적으로만 일고 넘어가서 이번에 완전히 이해하는것을 목표로 리뷰를 해 보았다.<br>그동안 개념적으로 이런 논문이지를 알고 왜 그런것인지에 대해서 초점을 맞춰서 하나하나 읽고, 찾아가면서 공부를 했는데 퍼즐맞추는것처럼 나름 재미있었다!  그리고… 사실 다음주 P-Stage 대비용으로 하나두개 씩 읽어두는게 좋을거같아서 한것도 있다…ㅎ<br>마지막으로 영어공부를 하면서 해야겠다. 파파고가 너무 그동안 편했던것 같다</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>FCOS: Fully Convolutional One-Stage Object Detection</p><p><a href="https://kyubumshin.github.io/2022/03/20/paper/FCOS/">https://kyubumshin.github.io/2022/03/20/paper/FCOS/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>KyuBum Shin</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2022-03-20</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2022-03-20</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/DeepLearning/">DeepLearning</a><a class="link-muted mr-2" rel="tag" href="/tags/CV/">CV</a><a class="link-muted mr-2" rel="tag" href="/tags/Object-Detection/">Object Detection</a><a class="link-muted mr-2" rel="tag" href="/tags/week10/">week10</a></div><div class="sharethis-inline-share-buttons"></div><script src="&lt;script type=&quot;text/javascript&quot; src=&quot;https://platform-api.sharethis.com/js/sharethis.js#property=612ce5f747069100121128ed&amp;product=inline-share-buttons&quot; async=&quot;async&quot;&gt;&lt;/script&gt;" defer></script></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2022/03/23/boostcamp/Dairy/Week10-Day-1-2-Review/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Week10 - Day 1~2 Review</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2022/03/18/boostcamp/Dairy/Week9-Day-5-Review/"><span class="level-item">Week9 - Day 5 Review</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">댓글</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://kyubumshin.github.io/2022/03/20/paper/FCOS/';
            this.page.identifier = '2022/03/20/paper/FCOS/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'kyubumshin-github-io' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><!--!--><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="KyuBum&#039;s Dev Blog" height="28"></a><p class="is-size-7"><span>&copy; 2022 KyuBum Shin</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ko");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="맨 위로" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "이 웹 사이트는 귀하의 경험을 향상시키기 위해 Cookie를 사용합니다.",
          dismiss: "무시",
          allow: "허용",
          deny: "거부",
          link: "더 알아보기",
          policy: "Cookie 정책",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/mhchem.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="입력 하세요..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"입력 하세요...","untitled":"(제목 없음)","posts":"포스트","pages":"페이지","categories":"카테고리","tags":"태그"});
        });</script></body></html>